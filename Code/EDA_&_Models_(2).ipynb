{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyzHef-Oe91f"
      },
      "source": [
        "# In vehicle coupon recommendation \n",
        "The purpose of this project is to predict 1) if a customer will accept the coupon or not under machine learning algorithms with best tuned parameters, and 2) mining the circumstances under which people are more likely to accept the coupons throung explornatory data analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYLmD96ge91l"
      },
      "source": [
        "## Table of contents\n",
        "- [0 - Packages](#0) \n",
        "- [1 - Data Processing](#1)\n",
        "  - [1.1 - Know your data](#1-1)\n",
        "    - [1.1.1 - Show the first 5 records](#1-1-1)\n",
        "    - [1.1.2 - Dimension](#1-1-2)\n",
        "    - [1.1.3 - Fetures explanations](#1-1-3)\n",
        "    - [1.1.4 - Feature statistics](#1-1-4)\n",
        "    - [1.1.5 - Null or not](#1-1-5)\n",
        "  - [1.2 - Data cleaning and transformation](#1-2)\n",
        "    - [1.2.1 - Set data type to category](#1-2-1)\n",
        "    - [1.2.2 - Drop car columns](#1-2-2)\n",
        "    - [1.2.3 - Impute lack data](#1-2-3)\n",
        "  - [1.3 - Summary](1-3)\n",
        "- [2 - Exploratory Data Analysis](#2) \n",
        "  - [2.1 - Comparsion between coupon acception and rejection](#2.1)\n",
        "  - [2.2 - Which features are indepedent with the target?](#2.2)\n",
        "  - [2.3 - How do the rest of features influence the target?](#2.3)\n",
        "  - [2.4 - Summary](2-4)\n",
        "- [3 - Feature Enigineering](#3)\n",
        "  - [3.1 - Feature selection](#3-1)\n",
        "  - [3.2 - Feature encoding](#3-2)\n",
        "    - [3.2.1 - Ordinal encoding](#3-2-1)\n",
        "    - [3.2.2 - One hot encoding](#3-2-2)\n",
        "    - [3.2.3 - Target encoding](#3-2-3)\n",
        "    - [3.2.4 - Embedded encoding](#3-2-4)\n",
        "    - [3.2.5 - Our encoding choice](#3-2-5)\n",
        "  - [3.3 - Summary](#3-3)\n",
        "- [4 - Classification Models](#4)\n",
        "  - [4.1 - Logistic regression](#4-1)\n",
        "  - [4.2 - Neural Network](#4.2)\n",
        "    - [4.2.1 - Just one layer, baseline](#4-2-1)\n",
        "    - [4.2.2 - Add hidden layers](#4-2-2)\n",
        "    - [4.2.3 - Dropout](#4-2-3)\n",
        "    - [4.2.4 - Optimizers](#4-2-4)\n",
        "    - [4.2.5 - Number of epoches](#4-2-5)\n",
        "    - [4.2.6 - Number of batch size](#4-2-6)\n",
        "    - [4.2.7 - Learning rate](#4-2-7)\n",
        "    - [4.2.8 - Number of Hidden neurons](#4-2-8)\n",
        "    - [4.2.9 - Regularization](#4-2-9) \n",
        "  - [4.3 - Support Vector Machine(SVM)](#4.3)\n",
        "    - [4.3.1 - Hard-Margin SVM](#4-3-1)\n",
        "    - [4.3.2 - Soft-Margin SVM](#4-3-2)\n",
        "  - [4.4 - Summary](#4-4)\n",
        "- [5 - Clustering](#5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqa3BRySe91m"
      },
      "source": [
        "<a name='0'></a>\n",
        "## 0 - Load Package\n",
        "Here are all packages that will be used in this notebook. We are telling the story on the shoulder of gaints, resp. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4XDasiZBe91n"
      },
      "outputs": [],
      "source": [
        "# !pip install category_encoders\n",
        "# !pip install dmba\n",
        "\n",
        "# Utils \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os \n",
        "#from google.colab import files\n",
        "from pandas import read_csv\n",
        "\n",
        "# \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# EDA & Data processing\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import category_encoders as ce \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Neuron Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Measure metric\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from dmba import classificationSummary, gainsChart, liftChart\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTMUZPdAe91o"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Data Preprocessing\n",
        "This chapter preprocesses the [in-vehicle coupon recommendation Data Set](https://archive.ics.uci.edu/ml/datasets/in-vehicle+coupon+recommendation) from UCI Machine Learnign Repository. When it comes to a new data, knowing your data can help you with better decisions on how to clean or transform data, which makes the potential insights clear. What do we need to know? Here is the receipt, data dimension, exact meaning for columns, any null value inside, are there some outliers or inconsitent values. How can solve the problem? Would be some difference for numerical variables and categorical variables? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRduVJ_Pe91p"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "### 1.1 - Know your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVoK7Mkze91q"
      },
      "source": [
        "<a name=\"1-1-1\"></a>\n",
        "#### 1.1.1 - Show the first 5 records\n",
        "Just have a glimpse of your data to form the basic understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFjwWACuwH1f"
      },
      "source": [
        "- Access the data when using Google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "29CenDYzgONR",
        "outputId": "4f15f38c-2e9d-424f-85d4-33741447e857"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_&_Models_(2).ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000008?line=0'>1</a>\u001b[0m file \u001b[39m=\u001b[39m files\u001b[39m.\u001b[39mupload()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000008?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mVehicle_Coupon.csv\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# The data file can be accessed in Dat_Set folder\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000008?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "file = files.upload()\n",
        "df = pd.read_csv('Vehicle_Coupon.csv')  # The data file can be accessed in Dat_Set folder\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkMvyNEcwH1h"
      },
      "source": [
        "- Access the data when using the local data file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M496Q3qowH1i"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ5min</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "      <th>direction_same</th>\n",
              "      <th>direction_opp</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Restaurant(&lt;20)</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>never</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>2h</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>never</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>2h</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>never</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>2h</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>never</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>never</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination  passanger weather  temperature  time  \\\n",
              "0  No Urgent Place      Alone   Sunny           55   2PM   \n",
              "1  No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
              "2  No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
              "3  No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
              "4  No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
              "\n",
              "                  coupon expiration  gender age      maritalStatus  ...  \\\n",
              "0        Restaurant(<20)         1d  Female  21  Unmarried partner  ...   \n",
              "1           Coffee House         2h  Female  21  Unmarried partner  ...   \n",
              "2  Carry out & Take away         2h  Female  21  Unmarried partner  ...   \n",
              "3           Coffee House         2h  Female  21  Unmarried partner  ...   \n",
              "4           Coffee House         1d  Female  21  Unmarried partner  ...   \n",
              "\n",
              "   CoffeeHouse CarryAway RestaurantLessThan20 Restaurant20To50  \\\n",
              "0        never       NaN                  4~8              1~3   \n",
              "1        never       NaN                  4~8              1~3   \n",
              "2        never       NaN                  4~8              1~3   \n",
              "3        never       NaN                  4~8              1~3   \n",
              "4        never       NaN                  4~8              1~3   \n",
              "\n",
              "  toCoupon_GEQ5min toCoupon_GEQ15min toCoupon_GEQ25min direction_same  \\\n",
              "0                1                 0                 0              0   \n",
              "1                1                 0                 0              0   \n",
              "2                1                 1                 0              0   \n",
              "3                1                 1                 0              0   \n",
              "4                1                 1                 0              0   \n",
              "\n",
              "  direction_opp  Y  \n",
              "0             1  1  \n",
              "1             1  0  \n",
              "2             1  1  \n",
              "3             1  0  \n",
              "4             1  0  \n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df = pd.read_csv('/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Data_Set/Vehicle_Coupon.csv') \n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poE8nraqe91s"
      },
      "source": [
        "<a name='1-1-2'></a>\n",
        "#### 1.1.2 -  Dimension\n",
        "The original coupon recommendation dataset is composed with 12684 records and 26 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0v9RJyMe91t",
        "outputId": "4b527ed0-8137-46e2-a599-c0bbfd74721b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12684, 26)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUY_tn72e91u"
      },
      "source": [
        "<a name='1-1-3'></a>\n",
        "#### 1.1.3 - Features Explanations\n",
        "Among 26 columns, 25 of them are features which may affect the acception/rejection of the coupun that are represented as 1/0 in target column. All should be categorical variables as the physical meaning. The features can be roughly categorized into 3 parts, passenger info, coupon info, and driving info. For passenge info, it can be gender, age, maritalStatus, has_child, education, occupation, income (7). For coupon info, they are coupon, expiration, Bar, CoffeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50 (7). For driving info, destination, passanger, weather, time, car, toCoupon_GEQ5min, toCoupon_GEQ15min, toCoupon_GEQ25min, direction_same, direction_opp (10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEY0r6yd_26T",
        "outputId": "50989544-15d6-4770-9446-448c01d7ac1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "destination ['No Urgent Place' 'Home' 'Work'] \n",
            "\n",
            "passanger ['Alone' 'Friend(s)' 'Kid(s)' 'Partner'] \n",
            "\n",
            "weather ['Sunny' 'Rainy' 'Snowy'] \n",
            "\n",
            "temperature [55 80 30] \n",
            "\n",
            "time ['2PM' '10AM' '6PM' '7AM' '10PM'] \n",
            "\n",
            "coupon ['Restaurant(<20)' 'Coffee House' 'Carry out & Take away' 'Bar'\n",
            " 'Restaurant(20-50)'] \n",
            "\n",
            "expiration ['1d' '2h'] \n",
            "\n",
            "gender ['Female' 'Male'] \n",
            "\n",
            "age ['21' '46' '26' '31' '41' '50plus' '36' 'below21'] \n",
            "\n",
            "maritalStatus ['Unmarried partner' 'Single' 'Married partner' 'Divorced' 'Widowed'] \n",
            "\n",
            "has_children [1 0] \n",
            "\n",
            "education ['Some college - no degree' 'Bachelors degree' 'Associates degree'\n",
            " 'High School Graduate' 'Graduate degree (Masters or Doctorate)'\n",
            " 'Some High School'] \n",
            "\n",
            "occupation ['Unemployed' 'Architecture & Engineering' 'Student'\n",
            " 'Education&Training&Library' 'Healthcare Support'\n",
            " 'Healthcare Practitioners & Technical' 'Sales & Related' 'Management'\n",
            " 'Arts Design Entertainment Sports & Media' 'Computer & Mathematical'\n",
            " 'Life Physical Social Science' 'Personal Care & Service'\n",
            " 'Community & Social Services' 'Office & Administrative Support'\n",
            " 'Construction & Extraction' 'Legal' 'Retired'\n",
            " 'Installation Maintenance & Repair' 'Transportation & Material Moving'\n",
            " 'Business & Financial' 'Protective Service'\n",
            " 'Food Preparation & Serving Related' 'Production Occupations'\n",
            " 'Building & Grounds Cleaning & Maintenance' 'Farming Fishing & Forestry'] \n",
            "\n",
            "income ['$37500 - $49999' '$62500 - $74999' '$12500 - $24999' '$75000 - $87499'\n",
            " '$50000 - $62499' '$25000 - $37499' '$100000 or More' '$87500 - $99999'\n",
            " 'Less than $12500'] \n",
            "\n",
            "car [nan 'Scooter and motorcycle' 'crossover' 'Mazda5' 'do not drive'\n",
            " 'Car that is too old to install Onstar :D'] \n",
            "\n",
            "Bar ['never' 'less1' '1~3' 'gt8' nan '4~8'] \n",
            "\n",
            "CoffeeHouse ['never' 'less1' '4~8' '1~3' 'gt8' nan] \n",
            "\n",
            "CarryAway [nan '4~8' '1~3' 'gt8' 'less1' 'never'] \n",
            "\n",
            "RestaurantLessThan20 ['4~8' '1~3' 'less1' 'gt8' nan 'never'] \n",
            "\n",
            "Restaurant20To50 ['1~3' 'less1' 'never' 'gt8' '4~8' nan] \n",
            "\n",
            "toCoupon_GEQ5min [1] \n",
            "\n",
            "toCoupon_GEQ15min [0 1] \n",
            "\n",
            "toCoupon_GEQ25min [0 1] \n",
            "\n",
            "direction_same [0 1] \n",
            "\n",
            "direction_opp [1 0] \n",
            "\n",
            "Y [1 0] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for f in df.columns:\n",
        "  print(f, df[f].unique(), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5RFgd_Me91v"
      },
      "source": [
        "<a name=\"1-1-4\"></a>\n",
        "#### 1.1.4 -  Features Statistics\n",
        "From above, we understand that the meaning for all columns. In this part, we explore the their data type and quantity to see if whether it meet our expectation for all categorical variables or dectect the outliners. The insights are 1) some of them are int type, so we may need to tranform the data type to category type that can be seen in section 1.2.1. 2) no outliners are detected in the group statistics, thus ensuring the consientency of the data for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifL6hlM7e91w",
        "outputId": "4ca7e793-1020-4463-a9d1-fc5556d7bee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12684 entries, 0 to 12683\n",
            "Data columns (total 26 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   destination           12684 non-null  object\n",
            " 1   passanger             12684 non-null  object\n",
            " 2   weather               12684 non-null  object\n",
            " 3   temperature           12684 non-null  int64 \n",
            " 4   time                  12684 non-null  object\n",
            " 5   coupon                12684 non-null  object\n",
            " 6   expiration            12684 non-null  object\n",
            " 7   gender                12684 non-null  object\n",
            " 8   age                   12684 non-null  object\n",
            " 9   maritalStatus         12684 non-null  object\n",
            " 10  has_children          12684 non-null  int64 \n",
            " 11  education             12684 non-null  object\n",
            " 12  occupation            12684 non-null  object\n",
            " 13  income                12684 non-null  object\n",
            " 14  car                   108 non-null    object\n",
            " 15  Bar                   12577 non-null  object\n",
            " 16  CoffeeHouse           12467 non-null  object\n",
            " 17  CarryAway             12533 non-null  object\n",
            " 18  RestaurantLessThan20  12554 non-null  object\n",
            " 19  Restaurant20To50      12495 non-null  object\n",
            " 20  toCoupon_GEQ5min      12684 non-null  int64 \n",
            " 21  toCoupon_GEQ15min     12684 non-null  int64 \n",
            " 22  toCoupon_GEQ25min     12684 non-null  int64 \n",
            " 23  direction_same        12684 non-null  int64 \n",
            " 24  direction_opp         12684 non-null  int64 \n",
            " 25  Y                     12684 non-null  int64 \n",
            "dtypes: int64(8), object(18)\n",
            "memory usage: 2.5+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "J3bCYjZqe91w",
        "outputId": "23801dc4-0603-4934-9792-79dd86f08b73"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2fb815ea-15a8-485f-82bc-6ac12f2cf27e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temperature</th>\n",
              "      <th>has_children</th>\n",
              "      <th>toCoupon_GEQ5min</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "      <th>direction_same</th>\n",
              "      <th>direction_opp</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.0</td>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.000000</td>\n",
              "      <td>12684.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>63.301798</td>\n",
              "      <td>0.414144</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.561495</td>\n",
              "      <td>0.119126</td>\n",
              "      <td>0.214759</td>\n",
              "      <td>0.785241</td>\n",
              "      <td>0.568433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>19.154486</td>\n",
              "      <td>0.492593</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.496224</td>\n",
              "      <td>0.323950</td>\n",
              "      <td>0.410671</td>\n",
              "      <td>0.410671</td>\n",
              "      <td>0.495314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>55.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fb815ea-15a8-485f-82bc-6ac12f2cf27e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fb815ea-15a8-485f-82bc-6ac12f2cf27e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fb815ea-15a8-485f-82bc-6ac12f2cf27e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        temperature  has_children  toCoupon_GEQ5min  toCoupon_GEQ15min  \\\n",
              "count  12684.000000  12684.000000           12684.0       12684.000000   \n",
              "mean      63.301798      0.414144               1.0           0.561495   \n",
              "std       19.154486      0.492593               0.0           0.496224   \n",
              "min       30.000000      0.000000               1.0           0.000000   \n",
              "25%       55.000000      0.000000               1.0           0.000000   \n",
              "50%       80.000000      0.000000               1.0           1.000000   \n",
              "75%       80.000000      1.000000               1.0           1.000000   \n",
              "max       80.000000      1.000000               1.0           1.000000   \n",
              "\n",
              "       toCoupon_GEQ25min  direction_same  direction_opp             Y  \n",
              "count       12684.000000    12684.000000   12684.000000  12684.000000  \n",
              "mean            0.119126        0.214759       0.785241      0.568433  \n",
              "std             0.323950        0.410671       0.410671      0.495314  \n",
              "min             0.000000        0.000000       0.000000      0.000000  \n",
              "25%             0.000000        0.000000       1.000000      0.000000  \n",
              "50%             0.000000        0.000000       1.000000      1.000000  \n",
              "75%             0.000000        0.000000       1.000000      1.000000  \n",
              "max             1.000000        1.000000       1.000000      1.000000  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOELjWoDe91x"
      },
      "source": [
        "<a name='1-1-5'></a>\n",
        "#### 1.1.5 - Null or not\n",
        "99.15 percent of values in car column is null, and ~1 percent of values in Bar, CoffeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50 are missing. As a measure, we plans to drop the car column (section 1.2.2) and impute other nan value in the abovementioned columns, as is implementated in section 1.2.3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1_nOkt_e91x",
        "outputId": "fbb58439-d24e-4f8c-adfa-4951a0edc571"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "destination              0.000000\n",
              "passanger                0.000000\n",
              "weather                  0.000000\n",
              "temperature              0.000000\n",
              "time                     0.000000\n",
              "coupon                   0.000000\n",
              "expiration               0.000000\n",
              "gender                   0.000000\n",
              "age                      0.000000\n",
              "maritalStatus            0.000000\n",
              "has_children             0.000000\n",
              "education                0.000000\n",
              "occupation               0.000000\n",
              "income                   0.000000\n",
              "car                     99.148534\n",
              "Bar                      0.843582\n",
              "CoffeeHouse              1.710817\n",
              "CarryAway                1.190476\n",
              "RestaurantLessThan20     1.024913\n",
              "Restaurant20To50         1.490066\n",
              "toCoupon_GEQ5min         0.000000\n",
              "toCoupon_GEQ15min        0.000000\n",
              "toCoupon_GEQ25min        0.000000\n",
              "direction_same           0.000000\n",
              "direction_opp            0.000000\n",
              "Y                        0.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum() / df.shape[0] * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOT6365Ve91y"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - Data cleaning and transformation\n",
        "From the observations from section 1.1 know your data, we set data type to catergory to avoid some unexpected int encoding, drop the car column for its nearly 90% null value and impute the rest lack data using the most frequent category that happens in that feature. What I want to note is that this part can prepare for exploratory data analysis but not for the models. For latter purpose, we have a separate chapter, 3. Feature Engineering, to acheive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ipWj4ore91z"
      },
      "source": [
        "<a name='1-2-1'></a>\n",
        "#### 1.2.1 -  Set data type as category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26py3PYJe91z",
        "outputId": "448a2ea9-028d-4630-d32e-9c34b20e2adf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "destination             category\n",
              "passanger               category\n",
              "weather                 category\n",
              "temperature             category\n",
              "time                    category\n",
              "coupon                  category\n",
              "expiration              category\n",
              "gender                  category\n",
              "age                     category\n",
              "maritalStatus           category\n",
              "has_children            category\n",
              "education               category\n",
              "occupation              category\n",
              "income                  category\n",
              "car                     category\n",
              "Bar                     category\n",
              "CoffeeHouse             category\n",
              "CarryAway               category\n",
              "RestaurantLessThan20    category\n",
              "Restaurant20To50        category\n",
              "toCoupon_GEQ5min        category\n",
              "toCoupon_GEQ15min       category\n",
              "toCoupon_GEQ25min       category\n",
              "direction_same          category\n",
              "direction_opp           category\n",
              "Y                       category\n",
              "dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.astype('category')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yzvTmI7e91y"
      },
      "source": [
        "<a name='1-2-2'></a>\n",
        "#### 1.2.2 - Drop car column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yAc4_0Xe91y",
        "outputId": "26ac39ab-cc4b-402a-f0cb-b739e5ee88a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['destination', 'passanger', 'weather', 'temperature', 'time', 'coupon',\n",
              "       'expiration', 'gender', 'age', 'maritalStatus', 'has_children',\n",
              "       'education', 'occupation', 'income', 'Bar', 'CoffeeHouse', 'CarryAway',\n",
              "       'RestaurantLessThan20', 'Restaurant20To50', 'toCoupon_GEQ5min',\n",
              "       'toCoupon_GEQ15min', 'toCoupon_GEQ25min', 'direction_same',\n",
              "       'direction_opp', 'Y'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(columns='car', inplace=True)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uD1s5y2e910"
      },
      "source": [
        "<a name='1-2-3'></a>\n",
        "#### 1.2.3 - Impute lack data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEiD5fBe910"
      },
      "source": [
        "[Ref](https://jamesrledoux.com/code/imputation#:~:text=One%20approach%20to%20imputing%20categorical,given%20in%20Pandas'%20value_counts%20function.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WwLcg24e911",
        "outputId": "af066de3-318e-4a8b-fe37-348ced8afdcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unGjgh34iPnx"
      },
      "source": [
        "### 1.3 - Summary\n",
        "We start with how the data looks like, its dimension, features meaning, features statistics to see if there are abnormal values based on domain knowldege, and check null values for potential drop or imputation. Then, we switch to data cleaning and transformation with respect to the insights from last step. Because, our data are catergorical, so we check the its consistency by looking unique values for all features in feature statistics. If there is numerical data, scatter plot for all features and target would be a wise option to detect the outliners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQeo_AFJe911"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Exploratory Data Analysis\n",
        "This chapter mine the relationship between influential factors on coupons acception. At first, we compare the coupon acception and rejection rate for \n",
        "the acquaintance of current advertising situation and the significance of a recommendation system with a high accuracy. Then, we ask which features may influene the coupons acception? And how do they impact? After all explaration, we will know the answer of those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYZUL6CJe911"
      },
      "source": [
        "<a name='2-1'></a>\n",
        "### 2.1 - Comparison between coupon acception and rejection\n",
        "\n",
        "The accept number is 7210 and reject one is 5474. It can be roughly regarded as balance data set. Then we draw a bar chart and pie to represent the number and ratio. \n",
        "\n",
        "- Create a combination graph with left bar chart to show the number, and right pie chart to show the ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTdccBxHuoq_",
        "outputId": "c459509c-aa87-4242-ae62-3c74c81411c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accept number is 7210, and the reject number is 5474, with rates 0.57, 0.43 respectively \n"
          ]
        }
      ],
      "source": [
        "accept_num = (df.Y == 1).sum() \n",
        "reject_num = (df.Y == 0).sum() \n",
        "accept_rate = (df.Y == 1).sum() / df.shape[0]\n",
        "reject_rate = 1 - accept_rate\n",
        "print(\"The accept number is %i, and the reject number is %i, with rates %.2f, %.2f \\\n",
        "respectively \" %(accept_num, reject_num, accept_rate, reject_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Y3Y0xwO6e911",
        "outputId": "70240264-9355-4869-99b5-c5ae19298418"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def make_pie(sizes, text,colors,labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    col = [[i/255. for i in c] for c in colors]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axis('equal')\n",
        "    width = 0.35\n",
        "    kwargs = dict(colors=col, startangle=180, autopct='%1.1f%%')\n",
        "    outside, _, _ = ax.pie(sizes, radius=1.5, pctdistance=1-width/2,\n",
        "                       labels=labels,**kwargs)\n",
        "    plt.setp( outside, width=width, edgecolor='white')\n",
        "\n",
        "    kwargs = dict(size=20, fontweight='bold', va='center')\n",
        "    ax.text(0, 0, text, ha='center', **kwargs)\n",
        "    plt.show()\n",
        "\n",
        "c1 = (226,33,7)\n",
        "c2 = (60,121,189)\n",
        "\n",
        "make_pie([7210,5474], \"Acception\",[c1,c2],['Y','N'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43ayVBce912"
      },
      "source": [
        "<a name='2-2'></a>\n",
        "### 2.2 -  Which features will be indepedent of target?\n",
        "We conduct Chi-square test and plot p values using heatmap to verify the independence between features and target. From the bottom line of the heatmap, we can tell the p values between toCoupon_GET5min, destination_same, destination_opp and Y target are 1, 0.1, 0.1. If our alpha limit set as 0.05, that means if the p-value is below 0.05, we will consider the feature and target variable to be dependent whereas for a p-value above 0.05, the variables are independent of one another. This provides the reference for feature selection.\n",
        "[Ref](https://medium.com/analytics-vidhya/constructing-heat-map-for-chi-square-test-of-independence-6d78aa2b140f#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjFiZDY4NWY1ZThmYzYyZDc1ODcwNWMxZWIwZThhNzUyNGM0NzU5NzUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NTc1NzcwNTcsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjExNjk2NjQyODk4MDIxNTU1MjE0OSIsImVtYWlsIjoieWFubWluZ2xpdTIxQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJuYW1lIjoiWWFubWluZyBMaXUiLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUl0YnZtbW1DdnlEVzktaWF0dXQzYXhlQWFlZGpIWV9TY2laNWI4M3dfLTY9czk2LWMiLCJnaXZlbl9uYW1lIjoiWWFubWluZyIsImZhbWlseV9uYW1lIjoiTGl1IiwiaWF0IjoxNjU3NTc3MzU3LCJleHAiOjE2NTc1ODA5NTcsImp0aSI6IjhmMmJjM2MwOTk2NDZmMDdiODBhYTZmNWE5OGY5ZGU0YTk3YzAxMTQifQ.Tvz3S-XbT029APxcmsPETDBUj8S1ir0iYejfv9QbpWLtuCDYwxxbDuQv1FCL9Wz_D8hmxPutvuOgrF-dFT5oMYTq6rn0hV8Z0UNNn8rBR_KofiuSx10ChpZgJfmMAtqccsfW20Yk__v_xvZPNOLnjiHdXZAtoQiWUNo9VFAZ58NM98KjLfkkJ0r715iuIQrnyzWnUnabgwO96GT1OTaLynipdg6ugvClNENa3WZIncBtdqlP4jeTE7QOEPnTMRUntHGCTj5VRsnJRYt3OYpE6BqQgANptFUygI-K47IM5WSAicSFzmxyp9CgJxQ3U4d4S3LPKqiBy63v7gkxkLJPhQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "tJu2-EOEe914",
        "outputId": "a93cd679-156d-420d-8437-625185de67fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0a7c3a2fd0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = df.astype('object')\n",
        "column_names=df.columns\n",
        "\n",
        "chisqmatrix=pd.DataFrame(df,columns=column_names,index=column_names)\n",
        "\n",
        "\n",
        "outercnt=0\n",
        "innercnt=0\n",
        "for icol in column_names:\n",
        "    for jcol in column_names:\n",
        "       mycrosstab=pd.crosstab(df[icol],df[jcol])\n",
        "       stat,p,dof,expected=stats.chi2_contingency(mycrosstab)\n",
        "       chisqmatrix.iloc[outercnt,innercnt]=round(p,3)\n",
        "       cntexpected=expected[expected<5].size\n",
        "       perexpected=((expected.size-cntexpected)/expected.size)*100\n",
        "       if perexpected<20:\n",
        "            chisqmatrix.iloc[outercnt,innercnt]=2\n",
        "       if icol==jcol:\n",
        "           chisqmatrix.iloc[outercnt,innercnt]=0.00\n",
        "       innercnt=innercnt+1\n",
        "    outercnt=outercnt+1\n",
        "    innercnt=0\n",
        "    \n",
        "\n",
        "plt.figure(figsize = (16,12))\n",
        "sns.heatmap(chisqmatrix.astype(np.float64), annot=True,linewidths=0.1, \n",
        "            cmap='coolwarm')\n",
        "\n",
        "# Question: 2 perspectives here. If we see the bottom row of this heatmap, \n",
        "# we can easily remove those with bigger p values with alpha = .05 for they \n",
        "# are indepdent. How do we deal those variables among features for a relatie\n",
        "# small value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td2uLBJIe915"
      },
      "source": [
        "<a name='2-3'></a>\n",
        "### 2.3 - How do the rest of features will influence the target ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikDlJhiFe915"
      },
      "source": [
        "[Ref](https://stackoverflow.com/questions/63687789/how-do-i-create-a-pie-chart-using-categorical-data-in-matplotlib)\n",
        "\n",
        "From the bar charts below, we can observe that the acceptance of coupons increases when drivers are not actively pursuing a destination. They also prefer to purchase coupons when in casual group settings without the responsibility of kids or when on their own. According to the temperature and weather features, sunny and warm conditions are preferred for dining out with comparatively higher acceptation rates at breakfast (10AM), lunch (2PM) and dinner (6PM) hours. The types of discounts primarily sought include Carry out/ takeaway coupons and Restaurants for less than $20 with suffient time before expiration. The customers that opt for coupons typically comprise students and singles among the low income bracket who can gain considerable savings on their daily coffee or meals. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mz1mACCje915",
        "outputId": "09e0df48-5e53-47fd-e8b6-ea0a9d213024"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "count = 0\n",
        "target = df.columns[-1]\n",
        "for cat_val in df.columns:\n",
        "  if cat_val == target: \n",
        "    continue\n",
        "  temp_df = df.loc[:, [cat_val, target]]  \n",
        "  temp_df = temp_df.groupby([cat_val, target]).size().reset_index(name='count')\n",
        "  pivot_df = pd.pivot_table(temp_df, values='count', index=cat_val, columns=target)\n",
        "  pivot_df.plot(kind='bar', rot=45)\n",
        "  plt.legend(['Rejected','Accepted'])\n",
        "  count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyv8dW6Ue916"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Feature Engineering\n",
        "This chapter consists of feature selection and encoding for application to the models. It should answer the following quesitons, 1) what is the standard for feature selection? 2) which encoding style is the best for our coupon data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqyhqX45e916"
      },
      "source": [
        "<a name='3-1'></a>\n",
        "### 3.1 -  Feature selection\n",
        "We assume that the target and features are related only the confidence is above 95 percent. By observing the bottom row in the heatmap in section 2.2, those 3 columns, **toCoupon_GEQ5min, direction_same, direction_opp**, are removed as not meeting the confidence conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG9AqLbqe916",
        "outputId": "4593311b-ea47-44c2-db59-47c9b71cdf67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "22\n"
          ]
        }
      ],
      "source": [
        "df_droped = df.drop(columns = ['toCoupon_GEQ5min', 'direction_same', 'direction_opp'])\n",
        "print(len(df.columns))\n",
        "print(len(df_droped.columns))\n",
        "# df_droped.to_csv('Data_Dropped.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ses2qWye917"
      },
      "source": [
        "<a name=3-2></a>\n",
        "### 3.2 - Feature encoding\n",
        "For our catergorical variables, we attempted one hot encoding, oridinal encoding without mapping of orders, ordinal encoding with mapping of orders, target encoding and CatBoost encoder. Before testing the models, the dataset was cleaned and transformed by splitting the dataframe into train data, validation data, and test data with a ratio 6/2/2 as per the recommendations of Andrew Ngâ€™s online machine learning course.\n",
        "\n",
        "[Encoding categorical variables](https://kiwidamien.github.io/encoding-categorical-variables.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TouvrAoLe919",
        "outputId": "b1a39bb6-a07a-4ec8-df41-3cb8c7bae5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7610, 21) (2537, 21) (2537, 21) (7610, 1) (2537, 1) (2537, 1)\n"
          ]
        }
      ],
      "source": [
        "target = ['Y']\n",
        "XP = df_droped.drop(columns = target)\n",
        "yP = df_droped[target]\n",
        "XTrain, XTest, yTrain, yTest = train_test_split(XP, yP, test_size=0.2, random_state=0)\n",
        "XTrain, XValid, yTrain, yValid = train_test_split(XTrain, yTrain, test_size=0.25, random_state=0)\n",
        "print(XTrain.shape, XValid.shape, XTest.shape, yTrain.shape, yValid.shape, yTest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh2CfpNVe919"
      },
      "source": [
        "<a name='3-2-1'></a>\n",
        "#### 3.2.1 - Ordinal encoding\n",
        "Ordinal encoding converts each label into integer values and the encoded data represents the sequence of labels. Here, we tried two ways, package and manual implementation. The category_encoder package assigns values for categories in features randomly while our manual implementation maps the relationships between categories and integer values based on prior knowledge. Furthermore, we can apply oridinal encoding to all features or some features, like age, education, income, Bar, CoffeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdrRkCkKe91-"
      },
      "outputs": [],
      "source": [
        "# Ordinal encoding using category_encoder package with random integer asssignment\n",
        "def ordinal_encoder(train, valid, test, columns):\n",
        "    oe = ce.OrdinalEncoder(cols=columns, return_df=True)\n",
        "    train_transformed = oe.fit_transform(train)\n",
        "    valid_transformed = oe.transform(valid)\n",
        "    test_transformed = oe.transform(test)\n",
        "    return train_transformed, valid_transformed, test_transformed\n",
        "\n",
        "\n",
        "# Ordinal encoding manually with specific integer assigment for categories based\n",
        "# on domain knowledge\n",
        "def ordinal_encoding(df):\n",
        "    clean_df = df.copy()\n",
        "    frequency_map = {\n",
        "        'never': 0,\n",
        "        'less1': 1,\n",
        "        '1~3': 2,\n",
        "        '4~8': 3,\n",
        "        'gt8': 4\n",
        "    }\n",
        "    \n",
        "    age_map = {\n",
        "        'below21': 0,\n",
        "        '21': 1,\n",
        "        '26': 2,\n",
        "        '31': 3,\n",
        "        '36': 4,\n",
        "        '41': 5,\n",
        "        '46': 6,\n",
        "        '50plus': 7\n",
        "    }\n",
        "    \n",
        "    income_map = {\n",
        "        'Less than $12500': 0,\n",
        "        '$12500 - $24999': 1,\n",
        "        '$25000 - $37499': 2,\n",
        "        '$37500 - $49999': 3,\n",
        "        '$50000 - $62499': 4,\n",
        "        '$62500 - $74999': 5,\n",
        "        '$75000 - $87499': 6,\n",
        "        '$87500 - $99999': 7,\n",
        "        '$100000 or More': 8\n",
        "    }\n",
        "    \n",
        "    frequency_cols = ['Restaurant20To50', 'RestaurantLessThan20',\n",
        "                      'CarryAway', 'CoffeeHouse', 'Bar']\n",
        "\n",
        "    \n",
        "    education_map = {\n",
        "        'Some High School': 0, \n",
        "        'High School Graduate': 1,\n",
        "        'Some college - no degree': 2,\n",
        "        'Associates degree': 3,\n",
        "        'Bachelors degree': 4,\n",
        "        'Graduate degree (Masters or Doctorate)': 5\n",
        "    }\n",
        "        \n",
        "    \n",
        "    for col in frequency_cols:\n",
        "        clean_df[col] = clean_df[col].map(frequency_map)\n",
        "    clean_df.age = clean_df.age.map(age_map)\n",
        "    clean_df.income = clean_df.income.map(income_map)\n",
        "    clean_df.education = clean_df.education.map(education_map)\n",
        "\n",
        "    return clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DBguxqBWe91-",
        "outputId": "3e36a3fd-f030-44c3-ab56-3ad3668a5d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['destination', 'passanger', 'weather', 'temperature', 'time', 'coupon',\n",
            "       'expiration', 'gender', 'age', 'maritalStatus', 'has_children',\n",
            "       'education', 'occupation', 'income', 'Bar', 'CoffeeHouse', 'CarryAway',\n",
            "       'RestaurantLessThan20', 'Restaurant20To50', 'toCoupon_GEQ15min',\n",
            "       'toCoupon_GEQ25min', 'Y'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Rainy</td>\n",
              "      <td>55</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Office &amp; Administrative Support</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>Home</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>6PM</td>\n",
              "      <td>Restaurant(20-50)</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>2</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>Home</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>3</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>Production Occupations</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Restaurant(&lt;20)</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>4</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>Work</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>7AM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>5</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Office &amp; Administrative Support</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          destination passanger weather temperature  time  \\\n",
              "9863  No Urgent Place     Alone   Rainy          55  10AM   \n",
              "251              Home     Alone   Sunny          55   6PM   \n",
              "4949             Home     Alone   Sunny          55  10PM   \n",
              "9411  No Urgent Place     Alone   Sunny          80  10PM   \n",
              "6449             Work     Alone   Sunny          80   7AM   \n",
              "\n",
              "                     coupon expiration  gender  age maritalStatus  ...  \\\n",
              "9863           Coffee House         1d    Male    1        Single  ...   \n",
              "251       Restaurant(20-50)         1d  Female    2        Single  ...   \n",
              "4949  Carry out & Take away         1d    Male    3        Single  ...   \n",
              "9411        Restaurant(<20)         1d  Female    4      Divorced  ...   \n",
              "6449  Carry out & Take away         1d  Female    5        Single  ...   \n",
              "\n",
              "     education                       occupation income  Bar  CoffeeHouse  \\\n",
              "9863         1  Office & Administrative Support      1    1            1   \n",
              "251          2                       Unemployed      2    2            1   \n",
              "4949         2           Production Occupations      1    2            1   \n",
              "9411         1                       Unemployed      3    3            1   \n",
              "6449         1  Office & Administrative Support      3    2            1   \n",
              "\n",
              "      CarryAway  RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "9863          1                     1                 1                  1   \n",
              "251           2                     2                 1                  1   \n",
              "4949          2                     2                 1                  1   \n",
              "9411          2                     2                 1                  0   \n",
              "6449          3                     2                 2                  0   \n",
              "\n",
              "     toCoupon_GEQ25min  \n",
              "9863                 0  \n",
              "251                  0  \n",
              "4949                 1  \n",
              "9411                 0  \n",
              "6449                 0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6660</th>\n",
              "      <td>Home</td>\n",
              "      <td>Partner</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Bar</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Transportation &amp; Material Moving</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10645</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Snowy</td>\n",
              "      <td>30</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Bar</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>2</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Student</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>2h</td>\n",
              "      <td>Female</td>\n",
              "      <td>2</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Life Physical Social Science</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>Work</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>7AM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>6</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Student</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Partner</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>2h</td>\n",
              "      <td>Female</td>\n",
              "      <td>2</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>Student</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           destination  passanger weather temperature  time  \\\n",
              "6660              Home    Partner   Sunny          55  10PM   \n",
              "10645  No Urgent Place      Alone   Snowy          30   2PM   \n",
              "2127   No Urgent Place  Friend(s)   Sunny          55   2PM   \n",
              "6536              Work      Alone   Sunny          80   7AM   \n",
              "330    No Urgent Place    Partner   Sunny          80  10AM   \n",
              "\n",
              "                      coupon expiration  gender  age      maritalStatus  ...  \\\n",
              "6660                     Bar         1d    Male    5  Unmarried partner  ...   \n",
              "10645                    Bar         1d    Male    2  Unmarried partner  ...   \n",
              "2127            Coffee House         2h  Female    2             Single  ...   \n",
              "6536   Carry out & Take away         1d    Male    6             Single  ...   \n",
              "330             Coffee House         2h  Female    2  Unmarried partner  ...   \n",
              "\n",
              "      education                        occupation income  Bar  CoffeeHouse  \\\n",
              "6660          1  Transportation & Material Moving      7    2            3   \n",
              "10645         1                           Student      2    1            2   \n",
              "2127          1      Life Physical Social Science      2    1            2   \n",
              "6536          1                           Student      2    2            3   \n",
              "330           5                           Student      4    3            4   \n",
              "\n",
              "       CarryAway  RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "6660           3                     4                 1                  0   \n",
              "10645          2                     4                 2                  0   \n",
              "2127           2                     5                 1                  0   \n",
              "6536           3                     5                 3                  0   \n",
              "330            3                     2                 1                  0   \n",
              "\n",
              "      toCoupon_GEQ25min  \n",
              "6660                  0  \n",
              "10645                 0  \n",
              "2127                  0  \n",
              "6536                  0  \n",
              "330                   0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>6PM</td>\n",
              "      <td>Restaurant(&lt;20)</td>\n",
              "      <td>2h</td>\n",
              "      <td>Male</td>\n",
              "      <td>5</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8332</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Kid(s)</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>2PM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>7</td>\n",
              "      <td>Married partner</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Computer &amp; Mathematical</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8917</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Friend(s)</td>\n",
              "      <td>Rainy</td>\n",
              "      <td>55</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Restaurant(20-50)</td>\n",
              "      <td>2h</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6057</th>\n",
              "      <td>Home</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>5</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10644</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Rainy</td>\n",
              "      <td>55</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>2</td>\n",
              "      <td>Unmarried partner</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>Student</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           destination  passanger weather temperature  time  \\\n",
              "12370  No Urgent Place  Friend(s)   Sunny          80   6PM   \n",
              "8332   No Urgent Place     Kid(s)   Sunny          55   2PM   \n",
              "8917   No Urgent Place  Friend(s)   Rainy          55  10PM   \n",
              "6057              Home      Alone   Sunny          55  10PM   \n",
              "10644  No Urgent Place      Alone   Rainy          55  10AM   \n",
              "\n",
              "                      coupon expiration  gender  age      maritalStatus  ...  \\\n",
              "12370        Restaurant(<20)         2h    Male    5             Single  ...   \n",
              "8332            Coffee House         1d  Female    7    Married partner  ...   \n",
              "8917       Restaurant(20-50)         2h    Male    1             Single  ...   \n",
              "6057   Carry out & Take away         1d  Female    5             Single  ...   \n",
              "10644           Coffee House         1d    Male    2  Unmarried partner  ...   \n",
              "\n",
              "      education               occupation income  Bar  CoffeeHouse  CarryAway  \\\n",
              "12370         3               Unemployed      7    2            4          2   \n",
              "8332          4  Computer & Mathematical      4    1            3          4   \n",
              "8917          4               Unemployed      2    3            4          1   \n",
              "6057          2               Unemployed      2    1            4          2   \n",
              "10644         1                  Student      2    1            2          2   \n",
              "\n",
              "       RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "12370                     2                 1                  0   \n",
              "8332                      2                 1                  1   \n",
              "8917                      4                 1                  1   \n",
              "6057                      2                 4                  1   \n",
              "10644                     4                 2                  1   \n",
              "\n",
              "      toCoupon_GEQ25min  \n",
              "12370                 0  \n",
              "8332                  0  \n",
              "8917                  0  \n",
              "6057                  1  \n",
              "10644                 0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test purpose, you can skip this part\n",
        "print(df_droped.columns)\n",
        "ordial_columns = ['age', 'education', 'income', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
        "XTrain_some_ordinal, XValid_some_ordinal, XTest_some_ordinal = ordinal_encoder(XTrain, XValid, XTest, ordial_columns)\n",
        "display(XTrain_some_ordinal.head())\n",
        "display(XValid_some_ordinal.head())\n",
        "display(XTest_some_ordinal.head())\n",
        "# Question: Are time, temprature the ordinal variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob4MR0bje91_"
      },
      "source": [
        "<a name = '3-2-2'></a>\n",
        "#### 3.2.2 - One hot encoding\n",
        "A one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0) [Ref](https://en.wikipedia.org/wiki/One-hot). It usually used for transforming categorical variables into values, which can be suitable for machine learning models. Several ways can implement this encoding style, like get_dummies, OneHotEncoder from category_encoder and sklearn.preprocessing package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeTc2PHe92A"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(train, valid, test, columns):\n",
        "    oe = ce.OneHotEncoder(cols=columns, return_df=True)\n",
        "    train_transformed = oe.fit_transform(train)\n",
        "    valid_transfromed = oe.transform(valid)\n",
        "    test_transformed = oe.transform(test)\n",
        "    return train_transformed, valid_transfromed, test_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pg9fCIfJe92A",
        "outputId": "34272aa3-193f-48f3-ac75-b53e277dc9af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$25000 - $37499</td>\n",
              "      <td>less1</td>\n",
              "      <td>never</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$25000 - $37499</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$37500 - $49999</td>\n",
              "      <td>never</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$37500 - $49999</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1~3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "9863              1              0              0            1            0   \n",
              "251               0              1              0            1            0   \n",
              "4949              0              1              0            1            0   \n",
              "9411              1              0              0            1            0   \n",
              "6449              0              0              1            1            0   \n",
              "\n",
              "      passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  \\\n",
              "9863            0            0          1          0          0  ...   \n",
              "251             0            0          0          1          0  ...   \n",
              "4949            0            0          0          1          0  ...   \n",
              "9411            0            0          0          1          0  ...   \n",
              "6449            0            0          0          1          0  ...   \n",
              "\n",
              "                income    Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  \\\n",
              "9863   $25000 - $37499  less1        never      less1                 less1   \n",
              "251   Less than $12500    1~3        never        4~8                   1~3   \n",
              "4949   $25000 - $37499    1~3        never        4~8                   1~3   \n",
              "9411   $37500 - $49999  never        never        4~8                   1~3   \n",
              "6449   $37500 - $49999    1~3        never        1~3                   1~3   \n",
              "\n",
              "      Restaurant20To50  toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  \\\n",
              "9863             less1                    1                    0   \n",
              "251              less1                    1                    0   \n",
              "4949             less1                    1                    0   \n",
              "9411             less1                    0                    1   \n",
              "6449               1~3                    0                    1   \n",
              "\n",
              "      toCoupon_GEQ25min_1  toCoupon_GEQ25min_2  \n",
              "9863                    1                    0  \n",
              "251                     1                    0  \n",
              "4949                    0                    1  \n",
              "9411                    1                    0  \n",
              "6449                    1                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6660</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$12500 - $24999</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10645</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>less1</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>less1</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>gt8</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>gt8</td>\n",
              "      <td>4~8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$75000 - $87499</td>\n",
              "      <td>never</td>\n",
              "      <td>less1</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "6660               0              1              0            0            0   \n",
              "10645              1              0              0            1            0   \n",
              "2127               1              0              0            0            1   \n",
              "6536               0              0              1            1            0   \n",
              "330                1              0              0            0            0   \n",
              "\n",
              "       passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  \\\n",
              "6660             1            0          0          1          0  ...   \n",
              "10645            0            0          0          0          1  ...   \n",
              "2127             0            0          0          1          0  ...   \n",
              "6536             0            0          0          1          0  ...   \n",
              "330              1            0          0          1          0  ...   \n",
              "\n",
              "                 income    Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  \\\n",
              "6660    $12500 - $24999    1~3          4~8        1~3                   4~8   \n",
              "10645  Less than $12500  less1          1~3        4~8                   4~8   \n",
              "2127   Less than $12500  less1          1~3        4~8                   gt8   \n",
              "6536   Less than $12500    1~3          4~8        1~3                   gt8   \n",
              "330     $75000 - $87499  never        less1        1~3                   1~3   \n",
              "\n",
              "       Restaurant20To50  toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  \\\n",
              "6660              less1                    0                    1   \n",
              "10645               1~3                    0                    1   \n",
              "2127              less1                    0                    1   \n",
              "6536                4~8                    0                    1   \n",
              "330               less1                    0                    1   \n",
              "\n",
              "       toCoupon_GEQ25min_1  toCoupon_GEQ25min_2  \n",
              "6660                     1                    0  \n",
              "10645                    1                    0  \n",
              "2127                     1                    0  \n",
              "6536                     1                    0  \n",
              "330                      1                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$12500 - $24999</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8332</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>$75000 - $87499</td>\n",
              "      <td>less1</td>\n",
              "      <td>4~8</td>\n",
              "      <td>gt8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8917</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>never</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>4~8</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6057</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10644</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>less1</td>\n",
              "      <td>1~3</td>\n",
              "      <td>4~8</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "12370              1              0              0            0            1   \n",
              "8332               1              0              0            0            0   \n",
              "8917               1              0              0            0            1   \n",
              "6057               0              1              0            1            0   \n",
              "10644              1              0              0            1            0   \n",
              "\n",
              "       passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  \\\n",
              "12370            0            0          0          1          0  ...   \n",
              "8332             0            1          0          1          0  ...   \n",
              "8917             0            0          1          0          0  ...   \n",
              "6057             0            0          0          1          0  ...   \n",
              "10644            0            0          1          0          0  ...   \n",
              "\n",
              "                 income    Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  \\\n",
              "12370   $12500 - $24999    1~3        less1        4~8                   1~3   \n",
              "8332    $75000 - $87499  less1          4~8        gt8                   1~3   \n",
              "8917   Less than $12500  never        less1      less1                   4~8   \n",
              "6057   Less than $12500  less1        less1        4~8                   1~3   \n",
              "10644  Less than $12500  less1          1~3        4~8                   4~8   \n",
              "\n",
              "       Restaurant20To50  toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  \\\n",
              "12370             less1                    0                    1   \n",
              "8332              less1                    1                    0   \n",
              "8917              less1                    1                    0   \n",
              "6057              never                    1                    0   \n",
              "10644               1~3                    1                    0   \n",
              "\n",
              "       toCoupon_GEQ25min_1  toCoupon_GEQ25min_2  \n",
              "12370                    1                    0  \n",
              "8332                     1                    0  \n",
              "8917                     1                    0  \n",
              "6057                     0                    1  \n",
              "10644                    1                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test\n",
        "XTrain_some_one_hot, XValid_some_one_hot, XTest_some_one_hot = one_hot_encoding(XTrain, XValid, XTest, columns=[i for i in XTrain.columns if i not in ordial_columns])\n",
        "display(XTrain_some_one_hot.head())\n",
        "display(XValid_some_one_hot.head())\n",
        "display(XTest_some_one_hot.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSkRjjpzSWV7"
      },
      "source": [
        "<a name='3-2-3'></a>\n",
        "#### 3.2.3 - Mixture encoding \n",
        "Mixture encoding is our diy name for the combination of one hot encoding and ordinal encoding. We implement those encoding with ordinal encoding with random value assignments for categories and ordinal encoding with some order value assignment for categories. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nwch6Fc9TIIK",
        "outputId": "fc4f8b52-9a80-4e2a-b266-04d125a4f208"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "9863              1              0              0            1            0   \n",
              "251               0              1              0            1            0   \n",
              "4949              0              1              0            1            0   \n",
              "9411              1              0              0            1            0   \n",
              "6449              0              0              1            1            0   \n",
              "\n",
              "      passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  income  \\\n",
              "9863            0            0          1          0          0  ...       2   \n",
              "251             0            0          0          1          0  ...       0   \n",
              "4949            0            0          0          1          0  ...       2   \n",
              "9411            0            0          0          1          0  ...       3   \n",
              "6449            0            0          0          1          0  ...       3   \n",
              "\n",
              "      Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "9863    1            0          1                     1                 1   \n",
              "251     2            0          3                     2                 1   \n",
              "4949    2            0          3                     2                 1   \n",
              "9411    0            0          3                     2                 1   \n",
              "6449    2            0          2                     2                 2   \n",
              "\n",
              "      toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "9863                    1                    0                    1   \n",
              "251                     1                    0                    1   \n",
              "4949                    1                    0                    0   \n",
              "9411                    0                    1                    1   \n",
              "6449                    0                    1                    1   \n",
              "\n",
              "      toCoupon_GEQ25min_2  \n",
              "9863                    0  \n",
              "251                     0  \n",
              "4949                    1  \n",
              "9411                    0  \n",
              "6449                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6660</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10645</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "6660               0              1              0            0            0   \n",
              "10645              1              0              0            1            0   \n",
              "2127               1              0              0            0            1   \n",
              "6536               0              0              1            1            0   \n",
              "330                1              0              0            0            0   \n",
              "\n",
              "       passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  income  \\\n",
              "6660             1            0          0          1          0  ...       1   \n",
              "10645            0            0          0          0          1  ...       0   \n",
              "2127             0            0          0          1          0  ...       0   \n",
              "6536             0            0          0          1          0  ...       0   \n",
              "330              1            0          0          1          0  ...       6   \n",
              "\n",
              "       Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "6660     2            3          2                     3                 1   \n",
              "10645    1            2          3                     3                 2   \n",
              "2127     1            2          3                     4                 1   \n",
              "6536     2            3          2                     4                 3   \n",
              "330      0            1          2                     2                 1   \n",
              "\n",
              "       toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "6660                     0                    1                    1   \n",
              "10645                    0                    1                    1   \n",
              "2127                     0                    1                    1   \n",
              "6536                     0                    1                    1   \n",
              "330                      0                    1                    1   \n",
              "\n",
              "       toCoupon_GEQ25min_2  \n",
              "6660                     0  \n",
              "10645                    0  \n",
              "2127                     0  \n",
              "6536                     0  \n",
              "330                      0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8332</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8917</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6057</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10644</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "12370              1              0              0            0            1   \n",
              "8332               1              0              0            0            0   \n",
              "8917               1              0              0            0            1   \n",
              "6057               0              1              0            1            0   \n",
              "10644              1              0              0            1            0   \n",
              "\n",
              "       passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  income  \\\n",
              "12370            0            0          0          1          0  ...       1   \n",
              "8332             0            1          0          1          0  ...       6   \n",
              "8917             0            0          1          0          0  ...       0   \n",
              "6057             0            0          0          1          0  ...       0   \n",
              "10644            0            0          1          0          0  ...       0   \n",
              "\n",
              "       Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "12370    2            1          3                     2                 1   \n",
              "8332     1            3          4                     2                 1   \n",
              "8917     0            1          1                     3                 1   \n",
              "6057     1            1          3                     2                 0   \n",
              "10644    1            2          3                     3                 2   \n",
              "\n",
              "       toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "12370                    0                    1                    1   \n",
              "8332                     1                    0                    1   \n",
              "8917                     1                    0                    1   \n",
              "6057                     1                    0                    0   \n",
              "10644                    1                    0                    1   \n",
              "\n",
              "       toCoupon_GEQ25min_2  \n",
              "12370                    0  \n",
              "8332                     0  \n",
              "8917                     0  \n",
              "6057                     1  \n",
              "10644                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Rainy</td>\n",
              "      <td>55</td>\n",
              "      <td>10AM</td>\n",
              "      <td>Coffee House</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>36</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>Some college - no degree</td>\n",
              "      <td>Office &amp; Administrative Support</td>\n",
              "      <td>$25000 - $37499</td>\n",
              "      <td>less1</td>\n",
              "      <td>never</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>Home</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>6PM</td>\n",
              "      <td>Restaurant(20-50)</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>21</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>High School Graduate</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>Less than $12500</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>Home</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>55</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Male</td>\n",
              "      <td>31</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>High School Graduate</td>\n",
              "      <td>Production Occupations</td>\n",
              "      <td>$25000 - $37499</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>No Urgent Place</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>10PM</td>\n",
              "      <td>Restaurant(&lt;20)</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>50plus</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>...</td>\n",
              "      <td>Some college - no degree</td>\n",
              "      <td>Unemployed</td>\n",
              "      <td>$37500 - $49999</td>\n",
              "      <td>never</td>\n",
              "      <td>never</td>\n",
              "      <td>4~8</td>\n",
              "      <td>1~3</td>\n",
              "      <td>less1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>Work</td>\n",
              "      <td>Alone</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>80</td>\n",
              "      <td>7AM</td>\n",
              "      <td>Carry out &amp; Take away</td>\n",
              "      <td>1d</td>\n",
              "      <td>Female</td>\n",
              "      <td>26</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>Some college - no degree</td>\n",
              "      <td>Office &amp; Administrative Support</td>\n",
              "      <td>$37500 - $49999</td>\n",
              "      <td>1~3</td>\n",
              "      <td>never</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1~3</td>\n",
              "      <td>1~3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          destination passanger weather temperature  time  \\\n",
              "9863  No Urgent Place     Alone   Rainy          55  10AM   \n",
              "251              Home     Alone   Sunny          55   6PM   \n",
              "4949             Home     Alone   Sunny          55  10PM   \n",
              "9411  No Urgent Place     Alone   Sunny          80  10PM   \n",
              "6449             Work     Alone   Sunny          80   7AM   \n",
              "\n",
              "                     coupon expiration  gender     age maritalStatus  ...  \\\n",
              "9863           Coffee House         1d    Male      36        Single  ...   \n",
              "251       Restaurant(20-50)         1d  Female      21        Single  ...   \n",
              "4949  Carry out & Take away         1d    Male      31        Single  ...   \n",
              "9411        Restaurant(<20)         1d  Female  50plus      Divorced  ...   \n",
              "6449  Carry out & Take away         1d  Female      26        Single  ...   \n",
              "\n",
              "                     education                       occupation  \\\n",
              "9863  Some college - no degree  Office & Administrative Support   \n",
              "251       High School Graduate                       Unemployed   \n",
              "4949      High School Graduate           Production Occupations   \n",
              "9411  Some college - no degree                       Unemployed   \n",
              "6449  Some college - no degree  Office & Administrative Support   \n",
              "\n",
              "                income    Bar CoffeeHouse CarryAway RestaurantLessThan20  \\\n",
              "9863   $25000 - $37499  less1       never     less1                less1   \n",
              "251   Less than $12500    1~3       never       4~8                  1~3   \n",
              "4949   $25000 - $37499    1~3       never       4~8                  1~3   \n",
              "9411   $37500 - $49999  never       never       4~8                  1~3   \n",
              "6449   $37500 - $49999    1~3       never       1~3                  1~3   \n",
              "\n",
              "     Restaurant20To50 toCoupon_GEQ15min toCoupon_GEQ25min  \n",
              "9863            less1                 1                 0  \n",
              "251             less1                 1                 0  \n",
              "4949            less1                 1                 1  \n",
              "9411            less1                 0                 0  \n",
              "6449              1~3                 0                 0  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# One Hot Encoding + Ordianal Encoding with random value assignments for categories\n",
        "# yTrain, yTest, yValid= yTrain.astype('int'), yTest.astype('int'), yValid.asType('int')\n",
        "\n",
        "ordial_columns = ['age', 'education', 'income', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n",
        "XTrain_some_ordinal_random, XValid_some_ordinal_random, XTest_some_ordinal_random = ordinal_encoder(XTrain, XValid, XTest, ordial_columns)\n",
        "# display(XTrain_some_ordinal_random.head())\n",
        "# display(XValid_some_ordinal_random.head())\n",
        "# display(XTest_some_ordinal_random.head())\n",
        "\n",
        "\n",
        "XTrain_mixture_random, XValid_mixture_random, XTest_mixture_random = one_hot_encoding(XTrain_some_ordinal_random, XValid_some_ordinal_random, XTest_some_ordinal_random, columns=[i for i in XTrain.columns if i not in ordial_columns])\n",
        "# display(XTrain_mixture_random.head())\n",
        "# display(XValid_mixture_random.head())\n",
        "# display(XTest_mixture_random.head())\n",
        "\n",
        "## Ordinal encoding with some order value assignment for categories \n",
        "yTrain, yValid, yTest = yTrain.astype('int'), yValid.astype('int'), yTest.astype('int')\n",
        "XTrain_some_ordinal= ordinal_encoding(XTrain)\n",
        "XValid_some_ordinal = ordinal_encoding(XValid)\n",
        "XTest_some_ordinal = ordinal_encoding(XTest)\n",
        "\n",
        "XTrain_mixture, XValid_mixture, XTest_mixture = one_hot_encoding(XTrain_some_ordinal, XValid_some_ordinal, XTest_some_ordinal, columns=[i for i in XTrain.columns if i not in ordial_columns])\n",
        "display(XTrain_mixture.head())\n",
        "display(XValid_mixture.head())\n",
        "display(XTest_mixture.head())\n",
        "\n",
        "\n",
        "display(XTrain.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwfFj75je92A"
      },
      "source": [
        "<a name = '3-2-3'></a>\n",
        "#### 3.2.3 - Target encoding\n",
        "Target encoding replace the value for certain category of features with ratio of the number of coupon acception/all numbers of record for that category. In this way, we introduce the effect of target for each category, and all values will be float in the range of [0, 1].  [Target Encoding](https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01tt71XKe92A"
      },
      "outputs": [],
      "source": [
        "def target_encoding(train, ytrain, valid, test, columns):\n",
        "    ytrain = ytrain.astype(float)\n",
        "    oe = ce.TargetEncoder(return_df=True)\n",
        "\n",
        "    train_transformed = oe.fit_transform(train[columns], ytrain[ytrain.columns])\n",
        "    valid_transformed = oe.transform(valid)\n",
        "    test_transformed = oe.transform(test)\n",
        "    return train_transformed, valid_transformed, test_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "rC7KTfIYe92B",
        "outputId": "7d961f25-e63a-40c0-e56c-c4429d543f41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.476529</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.616469</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.553333</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.607235</td>\n",
              "      <td>0.602808</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.514905</td>\n",
              "      <td>0.535337</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.593316</td>\n",
              "      <td>0.455240</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.563180</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.602808</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.431770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.706515</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.506024</td>\n",
              "      <td>0.542373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.582868</td>\n",
              "      <td>0.538295</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.607235</td>\n",
              "      <td>0.582868</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.579038</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.604251</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination  passanger   weather  temperature      time    coupon  \\\n",
              "9863     0.638844   0.533318  0.476529     0.536713  0.616469  0.509167   \n",
              "251      0.514593   0.533318  0.598612     0.536713  0.593316  0.455240   \n",
              "4949     0.514593   0.533318  0.598612     0.536713  0.515860  0.740529   \n",
              "9411     0.638844   0.533318  0.598612     0.607422  0.515860  0.706515   \n",
              "6449     0.508077   0.533318  0.598612     0.607422  0.508077  0.740529   \n",
              "\n",
              "      expiration    gender       age  maritalStatus  ...  education  \\\n",
              "9863    0.637821  0.593439  0.553333       0.606734  ...   0.602874   \n",
              "251     0.637821  0.555384  0.599010       0.606734  ...   0.581006   \n",
              "4949    0.637821  0.593439  0.563180       0.606734  ...   0.581006   \n",
              "9411    0.637821  0.555384  0.506024       0.542373  ...   0.602874   \n",
              "6449    0.637821  0.555384  0.606805       0.606734  ...   0.602874   \n",
              "\n",
              "      occupation    income       Bar  CoffeeHouse  CarryAway  \\\n",
              "9863    0.607235  0.602808  0.566798     0.469115   0.514905   \n",
              "251     0.564516  0.606742  0.632667     0.469115   0.595435   \n",
              "4949    0.563636  0.602808  0.632667     0.469115   0.595435   \n",
              "9411    0.564516  0.582868  0.538295     0.469115   0.595435   \n",
              "6449    0.607235  0.582868  0.632667     0.469115   0.579038   \n",
              "\n",
              "      RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "9863              0.535337          0.566923           0.534649   \n",
              "251               0.568306          0.566923           0.534649   \n",
              "4949              0.568306          0.566923           0.534649   \n",
              "9411              0.568306          0.566923           0.623919   \n",
              "6449              0.568306          0.604251           0.623919   \n",
              "\n",
              "      toCoupon_GEQ25min  \n",
              "9863           0.593975  \n",
              "251            0.593975  \n",
              "4949           0.431770  \n",
              "9411           0.593975  \n",
              "6449           0.593975  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6660</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.585477</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.418167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>0.573496</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.637795</td>\n",
              "      <td>0.559415</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.629139</td>\n",
              "      <td>0.579038</td>\n",
              "      <td>0.595064</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10645</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.479484</td>\n",
              "      <td>0.539309</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.418167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.573496</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.615784</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.663758</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.595064</td>\n",
              "      <td>0.604251</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.675403</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.663758</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.619335</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.615784</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.629139</td>\n",
              "      <td>0.579038</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.642032</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.585477</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.616469</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.573496</td>\n",
              "      <td>...</td>\n",
              "      <td>0.561960</td>\n",
              "      <td>0.615784</td>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.538295</td>\n",
              "      <td>0.546009</td>\n",
              "      <td>0.579038</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination  passanger   weather  temperature      time    coupon  \\\n",
              "6660      0.514593   0.585477  0.598612     0.536713  0.515860  0.418167   \n",
              "10645     0.638844   0.533318  0.479484     0.539309  0.658333  0.418167   \n",
              "2127      0.638844   0.675403  0.598612     0.536713  0.658333  0.509167   \n",
              "6536      0.508077   0.533318  0.598612     0.607422  0.508077  0.740529   \n",
              "330       0.638844   0.585477  0.598612     0.607422  0.616469  0.509167   \n",
              "\n",
              "       expiration    gender       age  maritalStatus  ...  education  \\\n",
              "6660     0.637821  0.593439  0.606805       0.573496  ...   0.602874   \n",
              "10645    0.637821  0.593439  0.599010       0.573496  ...   0.602874   \n",
              "2127     0.493618  0.555384  0.599010       0.606734  ...   0.602874   \n",
              "6536     0.637821  0.593439  0.619335       0.606734  ...   0.602874   \n",
              "330      0.493618  0.555384  0.599010       0.573496  ...   0.561960   \n",
              "\n",
              "       occupation    income       Bar  CoffeeHouse  CarryAway  \\\n",
              "6660     0.637795  0.559415  0.632667     0.629139   0.579038   \n",
              "10645    0.615784  0.606742  0.566798     0.663758   0.595435   \n",
              "2127     0.608247  0.606742  0.566798     0.663758   0.595435   \n",
              "6536     0.615784  0.606742  0.632667     0.629139   0.579038   \n",
              "330      0.615784  0.490909  0.538295     0.546009   0.579038   \n",
              "\n",
              "       RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "6660               0.595064          0.566923           0.623919   \n",
              "10645              0.595064          0.604251           0.623919   \n",
              "2127               0.605263          0.566923           0.623919   \n",
              "6536               0.605263          0.642032           0.623919   \n",
              "330                0.568306          0.566923           0.623919   \n",
              "\n",
              "       toCoupon_GEQ25min  \n",
              "6660            0.593975  \n",
              "10645           0.593975  \n",
              "2127            0.593975  \n",
              "6536            0.593975  \n",
              "330             0.593975  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12370</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.675403</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.593316</td>\n",
              "      <td>0.706515</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.557220</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.559415</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.546009</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8332</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.550861</td>\n",
              "      <td>0.550987</td>\n",
              "      <td>...</td>\n",
              "      <td>0.541242</td>\n",
              "      <td>0.563035</td>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.629139</td>\n",
              "      <td>0.573235</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8917</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.675403</td>\n",
              "      <td>0.476529</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.455240</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.553333</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.541242</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.538295</td>\n",
              "      <td>0.546009</td>\n",
              "      <td>0.514905</td>\n",
              "      <td>0.595064</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6057</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.546009</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.512214</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.431770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10644</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.476529</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.616469</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.573496</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.615784</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.663758</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.595064</td>\n",
              "      <td>0.604251</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       destination  passanger   weather  temperature      time    coupon  \\\n",
              "12370     0.638844   0.675403  0.598612     0.607422  0.593316  0.706515   \n",
              "8332      0.638844   0.521739  0.598612     0.536713  0.658333  0.509167   \n",
              "8917      0.638844   0.675403  0.476529     0.536713  0.515860  0.455240   \n",
              "6057      0.514593   0.533318  0.598612     0.536713  0.515860  0.740529   \n",
              "10644     0.638844   0.533318  0.476529     0.536713  0.616469  0.509167   \n",
              "\n",
              "       expiration    gender       age  maritalStatus  ...  education  \\\n",
              "12370    0.493618  0.593439  0.606805       0.606734  ...   0.557220   \n",
              "8332     0.637821  0.555384  0.550861       0.550987  ...   0.541242   \n",
              "8917     0.493618  0.593439  0.553333       0.606734  ...   0.541242   \n",
              "6057     0.637821  0.555384  0.606805       0.606734  ...   0.581006   \n",
              "10644    0.637821  0.593439  0.599010       0.573496  ...   0.602874   \n",
              "\n",
              "       occupation    income       Bar  CoffeeHouse  CarryAway  \\\n",
              "12370    0.564516  0.559415  0.632667     0.546009   0.595435   \n",
              "8332     0.563035  0.490909  0.566798     0.629139   0.573235   \n",
              "8917     0.564516  0.606742  0.538295     0.546009   0.514905   \n",
              "6057     0.564516  0.606742  0.566798     0.546009   0.595435   \n",
              "10644    0.615784  0.606742  0.566798     0.663758   0.595435   \n",
              "\n",
              "       RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "12370              0.568306          0.566923           0.623919   \n",
              "8332               0.568306          0.566923           0.534649   \n",
              "8917               0.595064          0.566923           0.534649   \n",
              "6057               0.568306          0.512214           0.534649   \n",
              "10644              0.595064          0.604251           0.534649   \n",
              "\n",
              "       toCoupon_GEQ25min  \n",
              "12370           0.593975  \n",
              "8332            0.593975  \n",
              "8917            0.593975  \n",
              "6057            0.431770  \n",
              "10644           0.593975  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test\n",
        "XTrain_target, XValid_target, XTest_target = target_encoding(XTrain, yTrain, XValid, XTest, columns=list(XTrain.columns))\n",
        "display(XTrain_target.head())\n",
        "display(XValid_target.head())\n",
        "display(XTest_target.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISTb87-re92B"
      },
      "source": [
        "<a name = '3-2-4'></a>\n",
        "#### 3.2.4 - Embedded encoding\n",
        "Embedded encoding is a distributed representation for the categorical variables. Each feature will be mapped into a vector with certain size. The size is arbitary, which you may test by yourself. Here, we just fixed it as 10 for computational convenience. You can get an intitution for output dimension in later cell. Also, the input data are assumed as oridinal encoding, here we use the combinations of for-loop and label encoding to acheive the same performance.  \n",
        "[Ref](https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "wFXQmUF-e92C",
        "outputId": "78a7d73d-d146-4d18-9ce7-0fc302ffcf6a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2e808bea-3a84-455f-b8b1-fe3dc1ab9fbe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2e808bea-3a84-455f-b8b1-fe3dc1ab9fbe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Data_Dropped.csv to Data_Dropped.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([['No Urgent Place', 'Alone', 'Sunny', ..., '1~3', '0', '0'],\n",
              "       ['No Urgent Place', 'Friend(s)', 'Sunny', ..., '1~3', '0', '0'],\n",
              "       ['No Urgent Place', 'Friend(s)', 'Sunny', ..., '1~3', '1', '0'],\n",
              "       ...,\n",
              "       ['Work', 'Alone', 'Snowy', ..., '1~3', '0', '0'],\n",
              "       ['Work', 'Alone', 'Snowy', ..., '1~3', '1', '1'],\n",
              "       ['Work', 'Alone', 'Sunny', ..., '1~3', '0', '0']], dtype='<U41')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "555/555 - 12s - loss: 0.6215 - accuracy: 0.6530 - 12s/epoch - 22ms/step\n",
            "Epoch 2/20\n",
            "555/555 - 6s - loss: 0.5913 - accuracy: 0.6881 - 6s/epoch - 11ms/step\n",
            "Epoch 3/20\n",
            "555/555 - 6s - loss: 0.5855 - accuracy: 0.6878 - 6s/epoch - 11ms/step\n",
            "Epoch 4/20\n",
            "555/555 - 6s - loss: 0.5804 - accuracy: 0.6975 - 6s/epoch - 11ms/step\n",
            "Epoch 5/20\n",
            "555/555 - 6s - loss: 0.5744 - accuracy: 0.7007 - 6s/epoch - 11ms/step\n",
            "Epoch 6/20\n",
            "555/555 - 6s - loss: 0.5685 - accuracy: 0.7077 - 6s/epoch - 11ms/step\n",
            "Epoch 7/20\n",
            "555/555 - 6s - loss: 0.5602 - accuracy: 0.7181 - 6s/epoch - 11ms/step\n",
            "Epoch 8/20\n",
            "555/555 - 6s - loss: 0.5515 - accuracy: 0.7226 - 6s/epoch - 11ms/step\n",
            "Epoch 9/20\n",
            "555/555 - 7s - loss: 0.5411 - accuracy: 0.7325 - 7s/epoch - 12ms/step\n",
            "Epoch 10/20\n",
            "555/555 - 6s - loss: 0.5333 - accuracy: 0.7377 - 6s/epoch - 11ms/step\n",
            "Epoch 11/20\n",
            "555/555 - 6s - loss: 0.5270 - accuracy: 0.7433 - 6s/epoch - 11ms/step\n",
            "Epoch 12/20\n",
            "555/555 - 6s - loss: 0.5204 - accuracy: 0.7483 - 6s/epoch - 11ms/step\n",
            "Epoch 13/20\n",
            "555/555 - 6s - loss: 0.5167 - accuracy: 0.7504 - 6s/epoch - 11ms/step\n",
            "Epoch 14/20\n",
            "555/555 - 6s - loss: 0.5130 - accuracy: 0.7522 - 6s/epoch - 11ms/step\n",
            "Epoch 15/20\n",
            "555/555 - 6s - loss: 0.5100 - accuracy: 0.7538 - 6s/epoch - 11ms/step\n",
            "Epoch 16/20\n",
            "555/555 - 6s - loss: 0.5068 - accuracy: 0.7569 - 6s/epoch - 11ms/step\n",
            "Epoch 17/20\n",
            "555/555 - 6s - loss: 0.5031 - accuracy: 0.7575 - 6s/epoch - 12ms/step\n",
            "Epoch 18/20\n",
            "555/555 - 6s - loss: 0.5007 - accuracy: 0.7621 - 6s/epoch - 11ms/step\n",
            "Epoch 19/20\n",
            "555/555 - 6s - loss: 0.4976 - accuracy: 0.7639 - 6s/epoch - 11ms/step\n",
            "Epoch 20/20\n",
            "555/555 - 6s - loss: 0.4950 - accuracy: 0.7631 - 6s/epoch - 11ms/step\n",
            "Accuracy: 70.97\n"
          ]
        }
      ],
      "source": [
        "from numpy import unique\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import concatenate\n",
        "from tensorflow.keras.utils import plot_model\n",
        " \n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "\t# load the dataset as a pandas DataFrame\n",
        "\tdata = read_csv(filename, header=0).iloc[:,1:]\n",
        "\t# retrieve numpy array\n",
        "\tdataset = data.values\n",
        "\t# split into input (X) and output (y) variables\n",
        "\tX = dataset[:, :-1]\n",
        "\ty = dataset[:,-1]\n",
        "\t# format all fields as string\n",
        "\tX = X.astype(str)\n",
        "\t# reshape target to be a 2d array\n",
        "\ty = y.reshape((len(y), 1))\n",
        "\treturn X, y\n",
        " \n",
        "# prepare input data\n",
        "def prepare_inputs(X_train, X_test):\n",
        "\tX_train_enc, X_test_enc = list(), list()\n",
        "\t# label encode each column\n",
        "\tfor i in range(X_train.shape[1]):\n",
        "\t\tle = LabelEncoder()\n",
        "\t\tle.fit(X_train[:, i])\n",
        "\t\t# encode\n",
        "\t\ttrain_enc = le.transform(X_train[:, i])\n",
        "\t\ttest_enc = le.transform(X_test[:, i])\n",
        "\t\t# store\n",
        "\t\tX_train_enc.append(train_enc)\n",
        "\t\tX_test_enc.append(test_enc)\n",
        "\treturn X_train_enc, X_test_enc\n",
        " \n",
        "# prepare target\n",
        "def prepare_targets(y_train, y_test):\n",
        "\tle = LabelEncoder()\n",
        "\tle.fit(y_train)\n",
        "\ty_train_enc = le.transform(y_train)\n",
        "\ty_test_enc = le.transform(y_test)\n",
        "\treturn y_train_enc, y_test_enc\n",
        " \n",
        "# load the dataset\n",
        "files.upload()\n",
        "X_e, y_e = load_dataset('Data_Dropped.csv')\n",
        "\n",
        "# X_e, y_e = load_dataset('D:\\Data-Science\\Course\\IE7374\\Projects\\ml_project\\Data_Set\\Data_Dropped.csv')\n",
        "display(X_e, y_e)\n",
        "# split into train and test sets\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X_e, y_e, test_size=0.3, random_state=0)\n",
        "# prepare input data\n",
        "X_train_enc, X_test_enc = prepare_inputs(X_Train, X_Test)\n",
        "# prepare output data\n",
        "y_train_enc, y_test_enc = prepare_targets(y_Train, y_Test)\n",
        "# make output 3d\n",
        "y_train_enc = y_train_enc.reshape((len(y_train_enc), 1, 1))\n",
        "y_test_enc = y_test_enc.reshape((len(y_test_enc), 1, 1))\n",
        "\n",
        "# prepare each input head\n",
        "in_layers = list()\n",
        "em_layers = list()\n",
        "for i in range(len(X_train_enc)):\n",
        "\t# calculate the number of unique inputs\n",
        "\tn_labels = len(unique(X_train_enc[i]))\n",
        "\t# define input layer\n",
        "\tin_layer = Input(shape=(1,))\n",
        "\t# define embedding layer\n",
        "\tem_layer = Embedding(n_labels, 10)(in_layer)\n",
        "\t# store layers\n",
        "\tin_layers.append(in_layer)\n",
        "\tem_layers.append(em_layer)\n",
        "# concat all embeddings\n",
        "merge = concatenate(em_layers)\n",
        "dense = Dense(10, activation='relu', kernel_initializer='he_normal')(merge)\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "model = Model(inputs=in_layers, outputs=output)\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# plot graph\n",
        "plot_model(model, show_shapes=True, to_file='embeddings.png')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train_enc, y_train_enc, epochs=20, batch_size=16, verbose=2)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "upFnUaMte92C",
        "outputId": "f87c31f5-4761-4cdf-d57a-c62798323f07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1, 210) dtype=float32 (created by layer 'concatenate')>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(merge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD0N-8c1AEws"
      },
      "source": [
        "Here, the problem is that the dimension of embbed data will shrink so that it can not match the length of y label. As a consequence, we cannot apply to logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFZE2TBQTuA"
      },
      "source": [
        "<a name='3-2-5'></a>\n",
        "#### 3.2.5 - Our encoding choice\n",
        "Classical logistic regression is applied on the abovemetioned encoding styles for picking the proper one considering the accuracy and further neural networks application. Those encoding style are one hot encoding, ordinal encoding, mixture encoding, target encoding and embedded encoding. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSos6pBI0aTn"
      },
      "outputs": [],
      "source": [
        "# Initialize a logistic regression classifier \n",
        "lr = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "2wN3sYEodfSp",
        "outputId": "53c95a96-c51b-4e30-8665-d62face81153"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.476529</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.616469</td>\n",
              "      <td>0.509167</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.553333</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.607235</td>\n",
              "      <td>0.602808</td>\n",
              "      <td>0.566798</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.514905</td>\n",
              "      <td>0.535337</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.593316</td>\n",
              "      <td>0.455240</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.599010</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.606742</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.536713</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.593439</td>\n",
              "      <td>0.563180</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.602808</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.534649</td>\n",
              "      <td>0.431770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>0.638844</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.515860</td>\n",
              "      <td>0.706515</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.506024</td>\n",
              "      <td>0.542373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.564516</td>\n",
              "      <td>0.582868</td>\n",
              "      <td>0.538295</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.595435</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.566923</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.533318</td>\n",
              "      <td>0.598612</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>0.508077</td>\n",
              "      <td>0.740529</td>\n",
              "      <td>0.637821</td>\n",
              "      <td>0.555384</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>0.606734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602874</td>\n",
              "      <td>0.607235</td>\n",
              "      <td>0.582868</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>0.469115</td>\n",
              "      <td>0.579038</td>\n",
              "      <td>0.568306</td>\n",
              "      <td>0.604251</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.593975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination  passanger   weather  temperature      time    coupon  \\\n",
              "9863     0.638844   0.533318  0.476529     0.536713  0.616469  0.509167   \n",
              "251      0.514593   0.533318  0.598612     0.536713  0.593316  0.455240   \n",
              "4949     0.514593   0.533318  0.598612     0.536713  0.515860  0.740529   \n",
              "9411     0.638844   0.533318  0.598612     0.607422  0.515860  0.706515   \n",
              "6449     0.508077   0.533318  0.598612     0.607422  0.508077  0.740529   \n",
              "\n",
              "      expiration    gender       age  maritalStatus  ...  education  \\\n",
              "9863    0.637821  0.593439  0.553333       0.606734  ...   0.602874   \n",
              "251     0.637821  0.555384  0.599010       0.606734  ...   0.581006   \n",
              "4949    0.637821  0.593439  0.563180       0.606734  ...   0.581006   \n",
              "9411    0.637821  0.555384  0.506024       0.542373  ...   0.602874   \n",
              "6449    0.637821  0.555384  0.606805       0.606734  ...   0.602874   \n",
              "\n",
              "      occupation    income       Bar  CoffeeHouse  CarryAway  \\\n",
              "9863    0.607235  0.602808  0.566798     0.469115   0.514905   \n",
              "251     0.564516  0.606742  0.632667     0.469115   0.595435   \n",
              "4949    0.563636  0.602808  0.632667     0.469115   0.595435   \n",
              "9411    0.564516  0.582868  0.538295     0.469115   0.595435   \n",
              "6449    0.607235  0.582868  0.632667     0.469115   0.579038   \n",
              "\n",
              "      RestaurantLessThan20  Restaurant20To50  toCoupon_GEQ15min  \\\n",
              "9863              0.535337          0.566923           0.534649   \n",
              "251               0.568306          0.566923           0.534649   \n",
              "4949              0.568306          0.566923           0.534649   \n",
              "9411              0.568306          0.566923           0.623919   \n",
              "6449              0.568306          0.604251           0.623919   \n",
              "\n",
              "      toCoupon_GEQ25min  \n",
              "9863           0.593975  \n",
              "251            0.593975  \n",
              "4949           0.431770  \n",
              "9411           0.593975  \n",
              "6449           0.593975  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6897910918407568"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6696886085928262"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Target Encoding\n",
        "display(XTrain_target.head())\n",
        "\n",
        "lr.fit(XTrain_target, yTrain)\n",
        "target_valid_acc = accuracy_score(yValid, lr.predict(XValid_target))\n",
        "target_test_acc = accuracy_score(yTest, lr.predict(XTest_target))\n",
        "display(target_valid_acc, target_test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "--kx39ig2rc7",
        "outputId": "693b455f-405e-438b-888e-df8aa8a43386"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>RestaurantLessThan20_5</th>\n",
              "      <th>Restaurant20To50_1</th>\n",
              "      <th>Restaurant20To50_2</th>\n",
              "      <th>Restaurant20To50_3</th>\n",
              "      <th>Restaurant20To50_4</th>\n",
              "      <th>Restaurant20To50_5</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "9863              1              0              0            1            0   \n",
              "251               0              1              0            1            0   \n",
              "4949              0              1              0            1            0   \n",
              "9411              1              0              0            1            0   \n",
              "6449              0              0              1            1            0   \n",
              "\n",
              "      passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  \\\n",
              "9863            0            0          1          0          0  ...   \n",
              "251             0            0          0          1          0  ...   \n",
              "4949            0            0          0          1          0  ...   \n",
              "9411            0            0          0          1          0  ...   \n",
              "6449            0            0          0          1          0  ...   \n",
              "\n",
              "      RestaurantLessThan20_5  Restaurant20To50_1  Restaurant20To50_2  \\\n",
              "9863                       0                   1                   0   \n",
              "251                        0                   1                   0   \n",
              "4949                       0                   1                   0   \n",
              "9411                       0                   1                   0   \n",
              "6449                       0                   0                   1   \n",
              "\n",
              "      Restaurant20To50_3  Restaurant20To50_4  Restaurant20To50_5  \\\n",
              "9863                   0                   0                   0   \n",
              "251                    0                   0                   0   \n",
              "4949                   0                   0                   0   \n",
              "9411                   0                   0                   0   \n",
              "6449                   0                   0                   0   \n",
              "\n",
              "      toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "9863                    1                    0                    1   \n",
              "251                     1                    0                    1   \n",
              "4949                    1                    0                    0   \n",
              "9411                    0                    1                    1   \n",
              "6449                    0                    1                    1   \n",
              "\n",
              "      toCoupon_GEQ25min_2  \n",
              "9863                    0  \n",
              "251                     0  \n",
              "4949                    1  \n",
              "9411                    0  \n",
              "6449                    0  \n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.681119432400473"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6728419392983839"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# One Hot Encoding \n",
        "XTrain_only_one_hot, XValid_only_one_hot, XTest_only_one_hot = one_hot_encoding(XTrain, XValid, XTest, columns=XTrain.columns)\n",
        "display(XTrain_only_one_hot.head())\n",
        "lr.fit(XTrain_only_one_hot, yTrain)\n",
        "onehot_valid_acc = accuracy_score(yValid, lr.predict(XValid_only_one_hot))\n",
        "onehot_test_acc = accuracy_score(yTest, lr.predict(XTest_only_one_hot))\n",
        "display(onehot_valid_acc, onehot_test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "foKNMHlq3Hid",
        "outputId": "b30c9422-c865-4991-b723-48882f74588a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination</th>\n",
              "      <th>passanger</th>\n",
              "      <th>weather</th>\n",
              "      <th>temperature</th>\n",
              "      <th>time</th>\n",
              "      <th>coupon</th>\n",
              "      <th>expiration</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>education</th>\n",
              "      <th>occupation</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min</th>\n",
              "      <th>toCoupon_GEQ25min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination  passanger  weather  temperature  time  coupon  expiration  \\\n",
              "9863            1          1        1            1     1       1           1   \n",
              "251             2          1        2            1     2       2           1   \n",
              "4949            2          1        2            1     3       3           1   \n",
              "9411            1          1        2            2     3       4           1   \n",
              "6449            3          1        2            2     4       3           1   \n",
              "\n",
              "      gender  age  maritalStatus  ...  education  occupation  income  Bar  \\\n",
              "9863       1    1              1  ...          1           1       1    1   \n",
              "251        2    2              1  ...          2           2       2    2   \n",
              "4949       1    3              1  ...          2           3       1    2   \n",
              "9411       2    4              2  ...          1           2       3    3   \n",
              "6449       2    5              1  ...          1           1       3    2   \n",
              "\n",
              "      CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "9863            1          1                     1                 1   \n",
              "251             1          2                     2                 1   \n",
              "4949            1          2                     2                 1   \n",
              "9411            1          2                     2                 1   \n",
              "6449            1          3                     2                 2   \n",
              "\n",
              "      toCoupon_GEQ15min  toCoupon_GEQ25min  \n",
              "9863                  1                  1  \n",
              "251                   1                  1  \n",
              "4949                  1                  2  \n",
              "9411                  2                  1  \n",
              "6449                  2                  1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6038628301143082"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5959795033504138"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ordinal Encoding\n",
        "XTrain_ordinal, XValid_ordinal, XTest_ordinal = ordinal_encoder(XTrain, XValid, XTest, columns=XTrain.columns)\n",
        "display(XTrain_ordinal.head())\n",
        "lr.fit(XTrain_ordinal, yTrain)\n",
        "ordinal_valid_acc = accuracy_score(yValid, lr.predict(XValid_ordinal))\n",
        "ordinal_test_acc = accuracy_score(yTest, lr.predict(XTest_ordinal))\n",
        "display(ordinal_valid_acc, ordinal_test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "3I12uIwn5IxN",
        "outputId": "8e5d2355-ba5b-4e46-9a33-bf3ac4e520e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "9863              1              0              0            1            0   \n",
              "251               0              1              0            1            0   \n",
              "4949              0              1              0            1            0   \n",
              "9411              1              0              0            1            0   \n",
              "6449              0              0              1            1            0   \n",
              "\n",
              "      passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  income  \\\n",
              "9863            0            0          1          0          0  ...       1   \n",
              "251             0            0          0          1          0  ...       2   \n",
              "4949            0            0          0          1          0  ...       1   \n",
              "9411            0            0          0          1          0  ...       3   \n",
              "6449            0            0          0          1          0  ...       3   \n",
              "\n",
              "      Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "9863    1            1          1                     1                 1   \n",
              "251     2            1          2                     2                 1   \n",
              "4949    2            1          2                     2                 1   \n",
              "9411    3            1          2                     2                 1   \n",
              "6449    2            1          3                     2                 2   \n",
              "\n",
              "      toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "9863                    1                    0                    1   \n",
              "251                     1                    0                    1   \n",
              "4949                    1                    0                    0   \n",
              "9411                    0                    1                    1   \n",
              "6449                    0                    1                    1   \n",
              "\n",
              "      toCoupon_GEQ25min_2  \n",
              "9863                    0  \n",
              "251                     0  \n",
              "4949                    1  \n",
              "9411                    0  \n",
              "6449                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6795427670476941"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6606227828143476"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>destination_1</th>\n",
              "      <th>destination_2</th>\n",
              "      <th>destination_3</th>\n",
              "      <th>passanger_1</th>\n",
              "      <th>passanger_2</th>\n",
              "      <th>passanger_3</th>\n",
              "      <th>passanger_4</th>\n",
              "      <th>weather_1</th>\n",
              "      <th>weather_2</th>\n",
              "      <th>weather_3</th>\n",
              "      <th>...</th>\n",
              "      <th>income</th>\n",
              "      <th>Bar</th>\n",
              "      <th>CoffeeHouse</th>\n",
              "      <th>CarryAway</th>\n",
              "      <th>RestaurantLessThan20</th>\n",
              "      <th>Restaurant20To50</th>\n",
              "      <th>toCoupon_GEQ15min_1</th>\n",
              "      <th>toCoupon_GEQ15min_2</th>\n",
              "      <th>toCoupon_GEQ25min_1</th>\n",
              "      <th>toCoupon_GEQ25min_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9863</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4949</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 71 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      destination_1  destination_2  destination_3  passanger_1  passanger_2  \\\n",
              "9863              1              0              0            1            0   \n",
              "251               0              1              0            1            0   \n",
              "4949              0              1              0            1            0   \n",
              "9411              1              0              0            1            0   \n",
              "6449              0              0              1            1            0   \n",
              "\n",
              "      passanger_3  passanger_4  weather_1  weather_2  weather_3  ...  income  \\\n",
              "9863            0            0          1          0          0  ...       2   \n",
              "251             0            0          0          1          0  ...       0   \n",
              "4949            0            0          0          1          0  ...       2   \n",
              "9411            0            0          0          1          0  ...       3   \n",
              "6449            0            0          0          1          0  ...       3   \n",
              "\n",
              "      Bar  CoffeeHouse  CarryAway  RestaurantLessThan20  Restaurant20To50  \\\n",
              "9863    1            0          1                     1                 1   \n",
              "251     2            0          3                     2                 1   \n",
              "4949    2            0          3                     2                 1   \n",
              "9411    0            0          3                     2                 1   \n",
              "6449    2            0          2                     2                 2   \n",
              "\n",
              "      toCoupon_GEQ15min_1  toCoupon_GEQ15min_2  toCoupon_GEQ25min_1  \\\n",
              "9863                    1                    0                    1   \n",
              "251                     1                    0                    1   \n",
              "4949                    1                    0                    0   \n",
              "9411                    0                    1                    1   \n",
              "6449                    0                    1                    1   \n",
              "\n",
              "      toCoupon_GEQ25min_2  \n",
              "9863                    0  \n",
              "251                     0  \n",
              "4949                    1  \n",
              "9411                    0  \n",
              "6449                    0  \n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6826960977532519"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6767836026803311"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Mixture Encoding \n",
        "# 1) One Hot Encoding + Ordianal Encoding with random value assignments for categories\n",
        "display(XTrain_mixture_random.head())\n",
        "lr.fit(XTrain_mixture_random, yTrain)\n",
        "mixture_random_valid_acc = accuracy_score(yValid, lr.predict(XValid_mixture_random))\n",
        "mixture_random_test_acc = accuracy_score(yTest, lr.predict(XTest_mixture_random))\n",
        "display(mixture_random_valid_acc, mixture_random_test_acc)\n",
        "\n",
        "# 2) Ordinal encoding with some order value assignment for categories \n",
        "display(XTrain_mixture.head())\n",
        "lr.fit(XTrain_mixture, yTrain)\n",
        "mixture_valid_acc = accuracy_score(yValid, lr.predict(XValid_mixture))\n",
        "mixture_test_acc = accuracy_score(yTest, lr.predict(XTest_mixture))\n",
        "display(mixture_valid_acc, mixture_test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A9lAUd3uyKL_",
        "outputId": "d655414f-11ce-4ddb-9b9b-47d6b5f4d622"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Encodings</th>\n",
              "      <th>Test Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OneHot</td>\n",
              "      <td>0.672842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ordinal</td>\n",
              "      <td>0.595980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Target</td>\n",
              "      <td>0.689791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mixture1</td>\n",
              "      <td>0.679543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mixture2</td>\n",
              "      <td>0.682696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Encodings  Test Accuracy\n",
              "0    OneHot       0.672842\n",
              "1   Ordinal       0.595980\n",
              "2    Target       0.689791\n",
              "3  Mixture1       0.679543\n",
              "4  Mixture2       0.682696"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get accuracy summary dataframe for visualization\n",
        "summary_encoding = [['OneHot', onehot_test_acc], ['Ordinal', ordinal_test_acc], \n",
        "                    ['Target', target_valid_acc], ['Mixture1', mixture_random_valid_acc], \n",
        "                    ['Mixture2', mixture_valid_acc]]\n",
        "test_acc_encoding_df = pd.DataFrame(summary_encoding, columns=['Encodings', 'Test Accuracy'])\n",
        "display(test_acc_encoding_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlOKiLmkXIHR"
      },
      "source": [
        "<a name='3-3'></a>\n",
        "### 3.3 - Summary\n",
        "In this chapter, we select the features that are related to the coupon acception based on Chi-Square test. Then, we test the classification accuracy based on logistic regression to pick the best encoding style among one-hot encoding, ordinal encoding, mixture encoding. For mixture encoding, we also test the prior knowledge effect on accuracy. That means, we can assign values for catergories specifically or randomly. Though we got the highest accuracy on target encoding, it has been known to bave issues with target leakage and performs poorly in neural networks. Ordinal encoding has the worst performance among all the encodings. As for one hot encoding and mixture encoding, they both works well. The prior knowledge on value assigment in mixture encoding works as the increase accuracy. We also explore the embedded encoding and apply it to neural network, however, we can not apply it into other classical algorithms such as the logistic regression, naive bayes or SVM as the mismatch of shrinked records and y labels. Considering roubstness, applicalibity, accuracy, we will choose the mixture encoding with assign values for categories as our general encoding style for testing the performance of another models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s19k1Frxe92C"
      },
      "source": [
        "<a name = '4'></a>\n",
        "# 4 - Classifiaction Models\n",
        "This chapter will answer two quesitons: 1) which model is the best for coupon acceptance classification? Performance metrics:  accuracy, space complexity and time complexity. 2) How to tune parameters for models to acheive the best accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpGs6Ole92D"
      },
      "source": [
        "<a name = '4-1'></a>\n",
        "### 4.1 - Logistic regression\n",
        "We manually implement the logistic regression with gradient descent in mixture encoding, then obtain the train, validation, test accurary as a baseline using some fixed parameters. Then, improve them on classification accuracy with different learning rates, tolerance, and maxIteration. Next, we validate the effectiveness of our manual code with tuned parameters compared to the accuray of LogisticRegression package. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm0yzeWPwH2A"
      },
      "source": [
        "<a name = '4-1-1'></a>\n",
        "#### 4.1.1 - Manual implementation with Gradient Descent - baseline\n",
        "With fix parameters, learningRate = 0.00001, tolerance = 0.0001, maxIteration = 100, tolerance and maxIteration are same as the default values that sklearn.linear_model.LogisticRegression package provide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZJSwrX21w-y"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self,data, learningRate = 0.00001, tolerance = 0.0001, maxIteration = 100, batchSize = 32):\n",
        "        self.data = data\n",
        "        self.tolerance = tolerance\n",
        "        self.maxIteration = maxIteration\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "        \n",
        "    def splitData(self):\n",
        "\n",
        "        [X_train, y_train, X_valid, y_valid, X_test, y_test ] = self.data\n",
        "\n",
        "        return X_train, y_train, X_valid, y_valid, X_test, y_test \n",
        "\n",
        "    def add_x0(self, X):\n",
        "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
        "        \n",
        "    def sigmoid(self,z):\n",
        "        sig = 1/(1+np.exp(-z))\n",
        "        return sig\n",
        "    \n",
        "    def costFunction(self, X, y):\n",
        "        pred_ =np.log(np.ones(X.shape[0])+np.exp(X.dot(self.w))) - X.dot(self.w).dot(y)\n",
        "        cost = pred_.sum( )\n",
        "        return cost\n",
        "    \n",
        "    def gradient(self,X,y):\n",
        "        sigmoid = self.sigmoid(X.dot(self.w))\n",
        "        grad = (sigmoid -y ).dot(X)\n",
        "        return grad\n",
        "    \n",
        "    def gradientDescent(self, X, y):\n",
        "        errors = []\n",
        "        last = float('inf')\n",
        "        \n",
        "        for i in range(self.maxIteration):\n",
        "            self.w = self.w - self.learningRate*self.gradient(X,y)\n",
        "            curr = self.costFunction(X,y)\n",
        "            \n",
        "            diff = last - curr\n",
        "            last - curr\n",
        "            \n",
        "            errors.append(curr)\n",
        "            \n",
        "            if diff < self.tolerance:\n",
        "                print(\"The model stopped Learning\")\n",
        "                break\n",
        "        # self.plot_cost(errors)\n",
        "\n",
        "    def stochasticGD(self, X, y):\n",
        "        X, y = np.array(X, dtype=np.float64), np.array(y, dtype=np.float64)\n",
        "        XY = np.c_[X.reshape(X.shape[0], X.shape[1]), y.reshape(X.shape[0], 1)]\n",
        "        \n",
        "        \n",
        "        # Set seed\n",
        "        np.random.seed(2022)\n",
        "        errors = []\n",
        "\n",
        "        for i in tqdm(range(self.maxIteration)):\n",
        "        # Shuffle x and y\n",
        "          np.random.shuffle(XY)\n",
        "\n",
        "          start = 0\n",
        "          stop = start + self.batchSize\n",
        "          X_batch, y_batch = XY[start:stop, :-1], XY[start:stop, -1]\n",
        "\n",
        "          \n",
        "          last_error = float('inf')\n",
        "\n",
        "          # Recalculating the difference\n",
        "          self.w = self.w - self.learningRate * self.gradient(X_batch, y_batch)\n",
        "          current_error = self.costFunction(X, y)\n",
        "         \n",
        "          diff = last_error - current_error\n",
        "          last_error = current_error\n",
        "\n",
        "          errors.append(current_error)\n",
        "          if np.abs(diff) < self.tolerance:\n",
        "              print('Model stopped learning')\n",
        "              break\n",
        "        print(self.w)\n",
        "        \n",
        "    def predict(self,X):\n",
        "        pred = self.sigmoid(X.dot(self.w))\n",
        "        return np.around(pred)\n",
        "        \n",
        "    def evaluate(self, y, y_hat):\n",
        "        \n",
        "        y = (y == 1)\n",
        "        y_hat = (y_hat == 1)\n",
        "        \n",
        "        accuracy = (y == y_hat).sum() / y.size\n",
        "        precision = (y & y_hat).sum() / y_hat.sum()\n",
        "        recall = (y & y_hat).sum() / y.sum()\n",
        "\n",
        "        # print(\"Accuracy is\", accuracy)\n",
        "        # print('Recall is', recall)\n",
        "        # print('precision is ', precision)\n",
        "        \n",
        "        return recall, precision, accuracy\n",
        "    \n",
        "    def fit(self):\n",
        "\n",
        "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
        "        self.w = np.ones(X_train.shape[1], dtype = np.float64)*0\n",
        "        self.stochasticGD(X_train, y_train)\n",
        "        \n",
        "        #print(self.w)\n",
        "        \n",
        "        y_hat_train = self.predict(X_train)\n",
        "        recall, precision, accuracy = self.evaluate(y_train,y_hat_train)\n",
        "\n",
        "    def validation(self):\n",
        "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
        "        y_hat_valid = self.predict(X_valid)\n",
        "        recall, precision, accuracy  = self.evaluate(y_valid, y_hat_valid)\n",
        "        return accuracy\n",
        "\n",
        "    def test(self):\n",
        "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
        "        y_hat_test = self.predict(X_test)\n",
        "        recall, precision, accuracy  = self.evaluate(y_test, y_hat_test)\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "OYKUSfhpFvE3",
        "outputId": "8a6080a3-192b-46a0-cea5-23f8bf93f0f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.46346911957950065"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.47851793456838787"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.47102877414268823"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = [XTrain_mixture, yTrain['Y'], XValid_mixture, yValid['Y'], XTest_mixture, yTest['Y']]\n",
        "# data = [XTrain_only_one_hot, yTrain['Y'], XValid_only_one_hot, yValid['Y'], XTest_only_one_hot, yTest['Y']]\n",
        "\n",
        "lr = LogisticRegression(data, learningRate = 0.0001, tolerance = 0.0001, maxIteration = 100, batchSize=32)\n",
        "log_base_train_acc = lr.fit()\n",
        "log_base_valid_acc = lr.validation()\n",
        "log_base_test_acc = lr.test()\n",
        "display(log_base_train_acc, log_base_valid_acc, log_base_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzTM3mvbwH2B"
      },
      "source": [
        "#### 4.1.2 - Learning rate\n",
        "\n",
        "For loop learning_rate = [0.001, 0.005, 0.01, 0.1, 0.5], get the best one on validation accuracy. \n",
        "Do similar operation for other parameters trail. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "4O7K3hdYFvt6",
        "outputId": "73daea45-8bf6-4754-e131-840e40bbc202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for learnig rate 0.001000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5745072273324573"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5581395348837209"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5628695309420575"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for learnig rate 0.005000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5420499342969777"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5545920378399685"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5471028774142688"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for learnig rate 0.010000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.47700394218134035"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.4903429247142294"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.4844304296413086"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for learnig rate 0.100000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for learnig rate 0.500000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5795006570302234"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5640520299566417"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5668111943240047"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = [0.001, 0.005, 0.01, 0.1, 0.5]\n",
        "log_base_valid_acc_dict = {}\n",
        "for l_rate in learning_rate:\n",
        "    lr = LogisticRegression(data, learningRate = l_rate, tolerance = 0.0001, maxIteration = 100, batchSize=32)\n",
        "    print('Accuracy for learnig rate %f' % l_rate)\n",
        "    log_base_train_acc = lr.fit()\n",
        "    log_base_valid_acc = lr.validation()\n",
        "    log_base_test_acc = lr.test()\n",
        "    # display(log_base_train_acc, log_base_valid_acc, log_base_test_acc)\n",
        "    log_base_valid_acc_dict[l_rate] = log_base_valid_acc\n",
        "    # print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(log_base_valid_acc_dict)\n",
        "l_rate = max(log_base_valid_acc_dict, key=log_base_valid_acc_dict.get)\n",
        "l_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHNxu69F3mk"
      },
      "source": [
        "<a name='4-1-3'></a>\n",
        "#### 4.1.3 - Tolerance\n",
        "tol_list = [0.001, 0.005, 0.01, 0.1, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "t0ew6HvZwH2C",
        "outputId": "5a7ad96f-21ce-4d7b-a47c-fe18c02e60d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for tolerance 0.001000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for tolerance 0.005000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for tolerance 0.010000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for tolerance 0.100000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for tolerance 0.500000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# l_rate = 0.1\n",
        "tol_list = [0.001, 0.005, 0.01, 0.1, 0.5]\n",
        "log_base_valid_acc_dict = {}\n",
        "for tol in tol_list:\n",
        "    lr = LogisticRegression(data, learningRate = l_rate, tolerance = tol, maxIteration = 100, batchSize=32)\n",
        "    print('Accuracy for tolerance %f' % tol)\n",
        "    log_base_train_acc = lr.fit()\n",
        "    log_base_valid_acc = lr.validation()\n",
        "    log_base_test_acc = lr.test()\n",
        "    # display(log_base_train_acc, log_base_valid_acc, log_base_test_acc)\n",
        "    # print('\\n')\n",
        "    log_base_valid_acc_dict[tol] = log_base_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(log_base_valid_acc_dict)\n",
        "tol = max(log_base_valid_acc_dict, key=log_base_valid_acc_dict.get)\n",
        "tol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZafsojAwH2C"
      },
      "source": [
        "<a name='4-1-4'></a>\n",
        "#### 4.1.4 - Max Iteration\n",
        "max_Iterations = [100, 200, 500, 1000, 5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "DDw6_SijwH2C",
        "outputId": "58c9f7db-fced-4112-b9ae-8939e1ade0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for iter 100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5859395532194481"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.570752857705952"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5715411903823413"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for iter 200\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.593035479632063"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5758770201024832"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5758770201024832"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for iter 500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6038107752956636"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.584548679542767"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5849428458809618"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for iter 1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6076215505913272"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5908553409538826"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5884903429247142"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy for iter 5000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6084099868593955"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5908553409538826"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5892786756011037"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# tol = 0.001\n",
        "max_Iterations = [100, 200, 500, 1000, 5000]\n",
        "log_base_valid_acc_dict = {}\n",
        "for iter in max_Iterations:\n",
        "    lr = LogisticRegression(data, learningRate = l_rate, tolerance = tol, maxIteration = iter, batchSize=32)\n",
        "    print('Accuracy for iter %i' % iter)\n",
        "    log_base_train_acc = lr.fit()\n",
        "    log_base_valid_acc = lr.validation()\n",
        "    log_base_test_acc = lr.test()\n",
        "    # display(log_base_train_acc, log_base_valid_acc, log_base_test_acc)\n",
        "    # print('\\n')\n",
        "    log_base_valid_acc_dict[iter] = log_base_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(log_base_valid_acc_dict)\n",
        "iter = max(log_base_valid_acc_dict, key=log_base_valid_acc_dict.get)\n",
        "iter "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZafsojAwH2C"
      },
      "source": [
        "<a name='4-1-5'></a>\n",
        "#### 4.1.4 - Batch Size\n",
        "batch_sizes = [32, 64, 128, 512, 1028]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tol = 0.001\n",
        "batch_sizes = [32, 64, 128, 512, 1028]\n",
        "log_base_valid_acc_dict = {}\n",
        "for bat in batch_sizes:\n",
        "    lr = LogisticRegression(data, learningRate = l_rate, tolerance = tol, maxIteration = iter, batchSize=bat)\n",
        "    print('Accuracy for iter %i' % iter)\n",
        "    log_base_train_acc = lr.fit()\n",
        "    log_base_valid_acc = lr.validation()\n",
        "    log_base_test_acc = lr.test()\n",
        "    # display(log_base_train_acc, log_base_valid_acc, log_base_test_acc)\n",
        "    # print('\\n')\n",
        "    log_base_valid_acc_dict[iter] = log_base_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(log_base_valid_acc_dict)\n",
        "bat = max(log_base_valid_acc_dict, key=log_base_valid_acc_dict.get)\n",
        "bat "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqaNxiduF70t"
      },
      "source": [
        "#### 4.1.6 - Accuracy for Logistic Regression with tuned parameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Z6h8FRpNwH2D",
        "outputId": "d60ddc86-0bfe-46ea-c22c-95172e0ddbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for best parameters, learning rate 0.100000, tolerance 0.001000, max iteration 1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6076215505913272"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5908553409538826"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5884903429247142"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train, validation, test accuracy using manual implementation with tuned parameter values\n",
        "def logRegression():\n",
        "  # iter = 1000\n",
        "  lr = LogisticRegression(data, learningRate = l_rate, tolerance = tol, maxIteration = iter, batchSize=bat)\n",
        "  print('Accuracy for best parameters, learning rate %f, tolerance %f, max iteration %i, batchSize %i' % (l_rate, tol, iter, bat))\n",
        "  log_train_acc = lr.fit()\n",
        "  log_valid_acc = lr.validation()\n",
        "  log_test_acc = lr.test()\n",
        "  display(log_train_acc, log_valid_acc, log_test_acc)\n",
        "  print('\\n')\n",
        "  return log_train_acc, log_valid_acc, log_test_acc\n",
        "log_train_acc, log_valid_acc, log_test_acc = logRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Y_JahL8CwH2D",
        "outputId": "86474427-0a4c-4a4b-e246-fe336376fd0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6826960977532519"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.6767836026803311"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# You can also compare your implementation with the package\n",
        "# Validation, test accurary using LogistRegression Package with defalut parameter values\n",
        "# display(mixture_valid_acc, mixture_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouki7sKJe92H"
      },
      "source": [
        "### 4.2 - Deep Neural Network\n",
        "This section tunes the parameters on validation accuracy for deep neural network. What is the baseline of validation accuracy of deep neural network? What is the hyper-parameters? How to tune them? What is the test accuracy with tuned parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8mmfvIze92H"
      },
      "source": [
        "#### 4.2.1 - Just one layer -  Baseline\n",
        "This is the base line for the deep neural networks. We just has one softmax layer to map the data with the classification result with fixed parameter values. Here are paramters we consider in the following improvement, number of layers, number of hidden neurons, droupout percent, optimizers, number of epochs, batch size, learning rate and regularization type. Their original value for the validation accuracy can be found as follows. The value can be chose arbitarily here, just serving the function to know what does the deep neural network performs. In later improvement, for each time, we will tune one parameter for the best validation accuracy, and use that parameter for anothe parameter tuning. Like accuray increment, finally, we will get a set of tuned paramters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6xAPomce92H",
        "outputId": "8e3037c2-da37-404e-c58a-1ddeb61e872e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 6ms/step - loss: 0.8579 - accuracy: 0.5028 - val_loss: 0.7670 - val_accuracy: 0.5315\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.7354 - accuracy: 0.5450 - val_loss: 0.7275 - val_accuracy: 0.5440\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.7047 - accuracy: 0.5618 - val_loss: 0.7041 - val_accuracy: 0.5657\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5856 - val_loss: 0.6908 - val_accuracy: 0.5749\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.5964 - val_loss: 0.6754 - val_accuracy: 0.5979\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6627 - accuracy: 0.6043 - val_loss: 0.6676 - val_accuracy: 0.6097\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.6124 - val_loss: 0.6606 - val_accuracy: 0.6117\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6482 - accuracy: 0.6230 - val_loss: 0.6555 - val_accuracy: 0.6170\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6429 - accuracy: 0.6326 - val_loss: 0.6537 - val_accuracy: 0.6137\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6383 - val_loss: 0.6489 - val_accuracy: 0.6235\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.6460 - val_loss: 0.6454 - val_accuracy: 0.6327\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6318 - accuracy: 0.6511 - val_loss: 0.6468 - val_accuracy: 0.6288\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6292 - accuracy: 0.6536 - val_loss: 0.6404 - val_accuracy: 0.6406\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.6605 - val_loss: 0.6393 - val_accuracy: 0.6386\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6243 - accuracy: 0.6615 - val_loss: 0.6369 - val_accuracy: 0.6419\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.6621 - val_loss: 0.6354 - val_accuracy: 0.6426\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.6656 - val_loss: 0.6348 - val_accuracy: 0.6413\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.6652 - val_loss: 0.6335 - val_accuracy: 0.6432\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6177 - accuracy: 0.6692 - val_loss: 0.6321 - val_accuracy: 0.6478\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6166 - accuracy: 0.6708 - val_loss: 0.6326 - val_accuracy: 0.6439\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.6692 - val_loss: 0.6299 - val_accuracy: 0.6491\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6142 - accuracy: 0.6716 - val_loss: 0.6292 - val_accuracy: 0.6491\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6133 - accuracy: 0.6685 - val_loss: 0.6290 - val_accuracy: 0.6498\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6128 - accuracy: 0.6702 - val_loss: 0.6303 - val_accuracy: 0.6426\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6115 - accuracy: 0.6712 - val_loss: 0.6326 - val_accuracy: 0.6505\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6720 - val_loss: 0.6275 - val_accuracy: 0.6452\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6101 - accuracy: 0.6712 - val_loss: 0.6256 - val_accuracy: 0.6505\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6093 - accuracy: 0.6708 - val_loss: 0.6247 - val_accuracy: 0.6524\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.6748 - val_loss: 0.6244 - val_accuracy: 0.6491\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.6756 - val_loss: 0.6237 - val_accuracy: 0.6544\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.6759 - val_loss: 0.6235 - val_accuracy: 0.6511\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6754 - val_loss: 0.6298 - val_accuracy: 0.6485\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6063 - accuracy: 0.6779 - val_loss: 0.6273 - val_accuracy: 0.6531\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6062 - accuracy: 0.6744 - val_loss: 0.6223 - val_accuracy: 0.6564\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.6762 - val_loss: 0.6240 - val_accuracy: 0.6544\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6054 - accuracy: 0.6743 - val_loss: 0.6214 - val_accuracy: 0.6564\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6044 - accuracy: 0.6764 - val_loss: 0.6277 - val_accuracy: 0.6485\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.6771 - val_loss: 0.6226 - val_accuracy: 0.6537\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.6754 - val_loss: 0.6208 - val_accuracy: 0.6537\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.6774 - val_loss: 0.6204 - val_accuracy: 0.6570\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.6795 - val_loss: 0.6209 - val_accuracy: 0.6570\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.6738 - val_loss: 0.6242 - val_accuracy: 0.6518\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6033 - accuracy: 0.6762 - val_loss: 0.6191 - val_accuracy: 0.6557\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6020 - accuracy: 0.6764 - val_loss: 0.6204 - val_accuracy: 0.6531\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6026 - accuracy: 0.6759 - val_loss: 0.6199 - val_accuracy: 0.6557\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6762 - val_loss: 0.6201 - val_accuracy: 0.6511\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.6767 - val_loss: 0.6230 - val_accuracy: 0.6498\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6764 - val_loss: 0.6194 - val_accuracy: 0.6537\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6789 - val_loss: 0.6184 - val_accuracy: 0.6537\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.6792 - val_loss: 0.6199 - val_accuracy: 0.6531\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6014 - accuracy: 0.6785 - val_loss: 0.6180 - val_accuracy: 0.6518\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6013 - accuracy: 0.6781 - val_loss: 0.6175 - val_accuracy: 0.6564\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6012 - accuracy: 0.6789 - val_loss: 0.6193 - val_accuracy: 0.6537\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6013 - accuracy: 0.6766 - val_loss: 0.6225 - val_accuracy: 0.6518\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6008 - accuracy: 0.6797 - val_loss: 0.6224 - val_accuracy: 0.6518\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6009 - accuracy: 0.6784 - val_loss: 0.6167 - val_accuracy: 0.6564\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.6790 - val_loss: 0.6232 - val_accuracy: 0.6531\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6836 - val_loss: 0.6208 - val_accuracy: 0.6537\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6792 - val_loss: 0.6182 - val_accuracy: 0.6537\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6820 - val_loss: 0.6171 - val_accuracy: 0.6551\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6802 - val_loss: 0.6165 - val_accuracy: 0.6544\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5999 - accuracy: 0.6808 - val_loss: 0.6181 - val_accuracy: 0.6524\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.6825 - val_loss: 0.6202 - val_accuracy: 0.6551\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5999 - accuracy: 0.6800 - val_loss: 0.6157 - val_accuracy: 0.6590\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.6789 - val_loss: 0.6157 - val_accuracy: 0.6590\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6000 - accuracy: 0.6802 - val_loss: 0.6155 - val_accuracy: 0.6597\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.6828 - val_loss: 0.6211 - val_accuracy: 0.6518\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6795 - val_loss: 0.6209 - val_accuracy: 0.6524\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.6815 - val_loss: 0.6172 - val_accuracy: 0.6557\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5989 - accuracy: 0.6810 - val_loss: 0.6193 - val_accuracy: 0.6551\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6805 - val_loss: 0.6152 - val_accuracy: 0.6597\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.6792 - val_loss: 0.6184 - val_accuracy: 0.6564\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.6822 - val_loss: 0.6177 - val_accuracy: 0.6557\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.6802 - val_loss: 0.6153 - val_accuracy: 0.6590\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6817 - val_loss: 0.6160 - val_accuracy: 0.6557\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.6782 - val_loss: 0.6159 - val_accuracy: 0.6557\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6781 - val_loss: 0.6153 - val_accuracy: 0.6557\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.6831 - val_loss: 0.6200 - val_accuracy: 0.6544\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.6813 - val_loss: 0.6158 - val_accuracy: 0.6551\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.6812 - val_loss: 0.6156 - val_accuracy: 0.6551\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.6818 - val_loss: 0.6147 - val_accuracy: 0.6610\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.6817 - val_loss: 0.6143 - val_accuracy: 0.6603\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.6825 - val_loss: 0.6195 - val_accuracy: 0.6577\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6840 - val_loss: 0.6155 - val_accuracy: 0.6557\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6845 - val_loss: 0.6143 - val_accuracy: 0.6577\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.6823 - val_loss: 0.6180 - val_accuracy: 0.6590\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6820 - val_loss: 0.6167 - val_accuracy: 0.6590\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.6818 - val_loss: 0.6140 - val_accuracy: 0.6623\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5978 - accuracy: 0.6823 - val_loss: 0.6153 - val_accuracy: 0.6577\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5978 - accuracy: 0.6831 - val_loss: 0.6139 - val_accuracy: 0.6649\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.6804 - val_loss: 0.6141 - val_accuracy: 0.6616\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.6825 - val_loss: 0.6141 - val_accuracy: 0.6610\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.6818 - val_loss: 0.6154 - val_accuracy: 0.6570\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.6835 - val_loss: 0.6140 - val_accuracy: 0.6577\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.6815 - val_loss: 0.6139 - val_accuracy: 0.6577\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.6823 - val_loss: 0.6166 - val_accuracy: 0.6597\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6853 - val_loss: 0.6171 - val_accuracy: 0.6603\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.6840 - val_loss: 0.6140 - val_accuracy: 0.6656\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.6846 - val_loss: 0.6179 - val_accuracy: 0.6616\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6835 - val_loss: 0.6136 - val_accuracy: 0.6610\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.6810 - val_loss: 0.6145 - val_accuracy: 0.6577\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.6813 - val_loss: 0.6164 - val_accuracy: 0.6597\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.6830 - val_loss: 0.6191 - val_accuracy: 0.6557\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5974 - accuracy: 0.6835 - val_loss: 0.6134 - val_accuracy: 0.6623\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6838 - val_loss: 0.6155 - val_accuracy: 0.6590\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.6841 - val_loss: 0.6166 - val_accuracy: 0.6610\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6815 - val_loss: 0.6133 - val_accuracy: 0.6623\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6833 - val_loss: 0.6139 - val_accuracy: 0.6564\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6835 - val_loss: 0.6159 - val_accuracy: 0.6597\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6848 - val_loss: 0.6146 - val_accuracy: 0.6610\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6853 - val_loss: 0.6134 - val_accuracy: 0.6675\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6840 - val_loss: 0.6132 - val_accuracy: 0.6616\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6838 - val_loss: 0.6144 - val_accuracy: 0.6590\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6841 - val_loss: 0.6129 - val_accuracy: 0.6623\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6817 - val_loss: 0.6132 - val_accuracy: 0.6629\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5972 - accuracy: 0.6827 - val_loss: 0.6136 - val_accuracy: 0.6597\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6825 - val_loss: 0.6132 - val_accuracy: 0.6616\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6831 - val_loss: 0.6142 - val_accuracy: 0.6570\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.6815 - val_loss: 0.6166 - val_accuracy: 0.6616\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.6861 - val_loss: 0.6173 - val_accuracy: 0.6597\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5971 - accuracy: 0.6848 - val_loss: 0.6158 - val_accuracy: 0.6603\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.6813 - val_loss: 0.6132 - val_accuracy: 0.6603\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6822 - val_loss: 0.6152 - val_accuracy: 0.6583\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.6825 - val_loss: 0.6128 - val_accuracy: 0.6636\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6851 - val_loss: 0.6230 - val_accuracy: 0.6544\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6827 - val_loss: 0.6170 - val_accuracy: 0.6597\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6846 - val_loss: 0.6134 - val_accuracy: 0.6610\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6836 - val_loss: 0.6226 - val_accuracy: 0.6557\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6828 - val_loss: 0.6145 - val_accuracy: 0.6616\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6835 - val_loss: 0.6129 - val_accuracy: 0.6643\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5964 - accuracy: 0.6843 - val_loss: 0.6140 - val_accuracy: 0.6583\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6854 - val_loss: 0.6160 - val_accuracy: 0.6570\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6861 - val_loss: 0.6242 - val_accuracy: 0.6524\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.6823 - val_loss: 0.6196 - val_accuracy: 0.6583\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6848 - val_loss: 0.6174 - val_accuracy: 0.6597\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6840 - val_loss: 0.6127 - val_accuracy: 0.6669\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6858 - val_loss: 0.6140 - val_accuracy: 0.6597\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6823 - val_loss: 0.6139 - val_accuracy: 0.6597\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6833 - val_loss: 0.6133 - val_accuracy: 0.6656\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.6836 - val_loss: 0.6123 - val_accuracy: 0.6669\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6868 - val_loss: 0.6129 - val_accuracy: 0.6636\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6841 - val_loss: 0.6147 - val_accuracy: 0.6616\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.6830 - val_loss: 0.6208 - val_accuracy: 0.6570\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5966 - accuracy: 0.6856 - val_loss: 0.6127 - val_accuracy: 0.6629\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5962 - accuracy: 0.6843 - val_loss: 0.6135 - val_accuracy: 0.6603\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5963 - accuracy: 0.6848 - val_loss: 0.6139 - val_accuracy: 0.6623\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5962 - accuracy: 0.6868 - val_loss: 0.6184 - val_accuracy: 0.6623\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6848 - val_loss: 0.6144 - val_accuracy: 0.6610\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6848 - val_loss: 0.6155 - val_accuracy: 0.6597\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6850 - val_loss: 0.6123 - val_accuracy: 0.6649\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5962 - accuracy: 0.6853 - val_loss: 0.6119 - val_accuracy: 0.6675\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6856 - val_loss: 0.6120 - val_accuracy: 0.6689\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6848 - val_loss: 0.6168 - val_accuracy: 0.6583\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6866 - val_loss: 0.6221 - val_accuracy: 0.6557\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6831 - val_loss: 0.6119 - val_accuracy: 0.6675\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6838 - val_loss: 0.6144 - val_accuracy: 0.6597\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5960 - accuracy: 0.6879 - val_loss: 0.6168 - val_accuracy: 0.6590\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6840 - val_loss: 0.6125 - val_accuracy: 0.6643\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6874 - val_loss: 0.6151 - val_accuracy: 0.6590\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6812 - val_loss: 0.6122 - val_accuracy: 0.6643\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6863 - val_loss: 0.6141 - val_accuracy: 0.6623\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6861 - val_loss: 0.6161 - val_accuracy: 0.6610\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6830 - val_loss: 0.6121 - val_accuracy: 0.6649\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6828 - val_loss: 0.6156 - val_accuracy: 0.6603\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6838 - val_loss: 0.6125 - val_accuracy: 0.6629\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6858 - val_loss: 0.6122 - val_accuracy: 0.6662\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5962 - accuracy: 0.6846 - val_loss: 0.6245 - val_accuracy: 0.6511\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6828 - val_loss: 0.6120 - val_accuracy: 0.6669\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.6836 - val_loss: 0.6143 - val_accuracy: 0.6597\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6836 - val_loss: 0.6172 - val_accuracy: 0.6610\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6871 - val_loss: 0.6125 - val_accuracy: 0.6649\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6868 - val_loss: 0.6150 - val_accuracy: 0.6636\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6838 - val_loss: 0.6137 - val_accuracy: 0.6623\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.6854 - val_loss: 0.6175 - val_accuracy: 0.6610\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6854 - val_loss: 0.6134 - val_accuracy: 0.6623\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6853 - val_loss: 0.6119 - val_accuracy: 0.6675\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5952 - accuracy: 0.6850 - val_loss: 0.6119 - val_accuracy: 0.6675\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6868 - val_loss: 0.6124 - val_accuracy: 0.6649\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6873 - val_loss: 0.6117 - val_accuracy: 0.6682\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5956 - accuracy: 0.6840 - val_loss: 0.6132 - val_accuracy: 0.6636\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6851 - val_loss: 0.6123 - val_accuracy: 0.6656\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5958 - accuracy: 0.6884 - val_loss: 0.6116 - val_accuracy: 0.6675\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6859 - val_loss: 0.6170 - val_accuracy: 0.6597\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6840 - val_loss: 0.6115 - val_accuracy: 0.6689\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6886 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6879 - val_loss: 0.6156 - val_accuracy: 0.6623\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6854 - val_loss: 0.6127 - val_accuracy: 0.6629\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5957 - accuracy: 0.6833 - val_loss: 0.6126 - val_accuracy: 0.6662\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6856 - val_loss: 0.6121 - val_accuracy: 0.6662\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5956 - accuracy: 0.6863 - val_loss: 0.6129 - val_accuracy: 0.6636\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6871 - val_loss: 0.6136 - val_accuracy: 0.6629\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6863 - val_loss: 0.6129 - val_accuracy: 0.6629\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5952 - accuracy: 0.6861 - val_loss: 0.6139 - val_accuracy: 0.6616\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5956 - accuracy: 0.6859 - val_loss: 0.6126 - val_accuracy: 0.6649\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5956 - accuracy: 0.6843 - val_loss: 0.6152 - val_accuracy: 0.6616\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.6841 - val_loss: 0.6127 - val_accuracy: 0.6649\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6856 - val_loss: 0.6157 - val_accuracy: 0.6616\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.6831 - val_loss: 0.6138 - val_accuracy: 0.6616\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6850 - val_loss: 0.6154 - val_accuracy: 0.6643\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6846 - val_loss: 0.6114 - val_accuracy: 0.6649\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.5993 - accuracy: 0.6811\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6082 - accuracy: 0.6744\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 0.6764\n",
            "Test accuracy:  0.6763894557952881 Train accurcy:  0.6810775399208069 Valid accuracy:  0.6744186282157898\n"
          ]
        }
      ],
      "source": [
        "#  parameters\n",
        "n_hiddens = 128\n",
        "dropout = 0\n",
        "default_optimizer = 'SGD'\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "regtype = ''\n",
        "verbose = 0\n",
        "n_classes = 2\n",
        "validation_split = 0.2\n",
        "\n",
        "\n",
        "# No need to change the data types, or normalizations and one hot encoding \n",
        "# for the target class\n",
        "\n",
        "# model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(n_classes, input_shape=(XTrain_mixture.shape[1],),\n",
        "            name = 'dense_layer', activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(XTrain_mixture, yTrain, batch_size=batch_size, epochs=num_epochs,\n",
        "            verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "# Evaluate the model\n",
        "NN_base_train_loss, NN_base_train_acc = model.evaluate(XTrain_mixture, yTrain)\n",
        "NN_base_valid_loss, NN_base_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "NN_base_test_loss, NN_base_test_acc = model.evaluate(XTest_mixture, yTest)\n",
        "print('Test accuracy: ', NN_base_test_acc, 'Train accurcy: ', NN_base_train_acc, 'Valid accuracy: ', NN_base_valid_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPRejrrwe92I"
      },
      "source": [
        "#### 4.2.2 - Add Hidden layers\n",
        "Hidden layers can be signicant for improving classification accuracy.Here we choose 2 layers. After add one hidden relu layer with fixed number of hidden neurons, the validation accuracy improves. We are not going to add more layers, since we just want to know if the hidden layer existence will work here or not. The answer is yes from the expermentation below. If we add more layers, the validation accuracy usually improve. That will be likely to introduce a problem for over-fitting, which may be mediated by drop out some percents of data or apply the regularization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYXqASTYe92I",
        "outputId": "ba51d2fa-8e23-487c-fe9e-ea60b81e1186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6951 - accuracy: 0.5554 - val_loss: 0.6613 - val_accuracy: 0.6097\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.6035 - val_loss: 0.6582 - val_accuracy: 0.6150\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.6275 - val_loss: 0.6390 - val_accuracy: 0.6399\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6392 - accuracy: 0.6421 - val_loss: 0.6330 - val_accuracy: 0.6380\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.6549 - val_loss: 0.6278 - val_accuracy: 0.6505\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6249 - accuracy: 0.6592 - val_loss: 0.6264 - val_accuracy: 0.6590\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.6675 - val_loss: 0.6234 - val_accuracy: 0.6505\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6146 - accuracy: 0.6705 - val_loss: 0.6200 - val_accuracy: 0.6610\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6110 - accuracy: 0.6733 - val_loss: 0.6179 - val_accuracy: 0.6656\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6712 - val_loss: 0.6209 - val_accuracy: 0.6557\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6754 - val_loss: 0.6183 - val_accuracy: 0.6708\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6026 - accuracy: 0.6767 - val_loss: 0.6161 - val_accuracy: 0.6616\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6008 - accuracy: 0.6802 - val_loss: 0.6131 - val_accuracy: 0.6741\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6767 - val_loss: 0.6161 - val_accuracy: 0.6629\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6825 - val_loss: 0.6101 - val_accuracy: 0.6787\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.6841 - val_loss: 0.6295 - val_accuracy: 0.6505\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6884 - val_loss: 0.6066 - val_accuracy: 0.6761\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5914 - accuracy: 0.6874 - val_loss: 0.6212 - val_accuracy: 0.6610\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5895 - accuracy: 0.6946 - val_loss: 0.6080 - val_accuracy: 0.6800\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5886 - accuracy: 0.6904 - val_loss: 0.6045 - val_accuracy: 0.6807\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.6927 - val_loss: 0.6035 - val_accuracy: 0.6846\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.6966 - val_loss: 0.6028 - val_accuracy: 0.6820\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5864 - accuracy: 0.6919 - val_loss: 0.6022 - val_accuracy: 0.6866\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5843 - accuracy: 0.6976 - val_loss: 0.6033 - val_accuracy: 0.6695\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.6960 - val_loss: 0.6022 - val_accuracy: 0.6820\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5820 - accuracy: 0.6955 - val_loss: 0.6179 - val_accuracy: 0.6643\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5818 - accuracy: 0.7022 - val_loss: 0.6004 - val_accuracy: 0.6827\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5805 - accuracy: 0.7042 - val_loss: 0.6133 - val_accuracy: 0.6689\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5779 - accuracy: 0.7040 - val_loss: 0.5986 - val_accuracy: 0.6905\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.6999 - val_loss: 0.5995 - val_accuracy: 0.6781\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5777 - accuracy: 0.7061 - val_loss: 0.6023 - val_accuracy: 0.6820\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.7047 - val_loss: 0.5965 - val_accuracy: 0.6827\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7040 - val_loss: 0.6006 - val_accuracy: 0.6879\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5747 - accuracy: 0.7053 - val_loss: 0.6050 - val_accuracy: 0.6735\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.7027 - val_loss: 0.6001 - val_accuracy: 0.6820\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5721 - accuracy: 0.7106 - val_loss: 0.5938 - val_accuracy: 0.6892\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5718 - accuracy: 0.7104 - val_loss: 0.6117 - val_accuracy: 0.6748\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.7050 - val_loss: 0.6034 - val_accuracy: 0.6813\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5705 - accuracy: 0.7091 - val_loss: 0.5949 - val_accuracy: 0.6886\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5721 - accuracy: 0.7076 - val_loss: 0.5927 - val_accuracy: 0.6879\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5711 - accuracy: 0.7096 - val_loss: 0.5978 - val_accuracy: 0.6807\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5686 - accuracy: 0.7117 - val_loss: 0.6036 - val_accuracy: 0.6800\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5678 - accuracy: 0.7111 - val_loss: 0.5908 - val_accuracy: 0.6866\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5656 - accuracy: 0.7114 - val_loss: 0.5923 - val_accuracy: 0.6853\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5653 - accuracy: 0.7167 - val_loss: 0.5903 - val_accuracy: 0.6938\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5641 - accuracy: 0.7129 - val_loss: 0.5896 - val_accuracy: 0.6925\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5636 - accuracy: 0.7144 - val_loss: 0.5975 - val_accuracy: 0.6866\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5642 - accuracy: 0.7137 - val_loss: 0.5938 - val_accuracy: 0.6892\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5646 - accuracy: 0.7129 - val_loss: 0.6421 - val_accuracy: 0.6498\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7211 - val_loss: 0.6042 - val_accuracy: 0.6787\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5610 - accuracy: 0.7152 - val_loss: 0.5871 - val_accuracy: 0.6984\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5587 - accuracy: 0.7229 - val_loss: 0.5874 - val_accuracy: 0.6938\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5620 - accuracy: 0.7158 - val_loss: 0.6230 - val_accuracy: 0.6636\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5588 - accuracy: 0.7208 - val_loss: 0.5933 - val_accuracy: 0.6912\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7160 - val_loss: 0.5989 - val_accuracy: 0.6813\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.7190 - val_loss: 0.6024 - val_accuracy: 0.6807\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5571 - accuracy: 0.7196 - val_loss: 0.5849 - val_accuracy: 0.7011\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5565 - accuracy: 0.7249 - val_loss: 0.6065 - val_accuracy: 0.6787\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7293 - val_loss: 0.6057 - val_accuracy: 0.6794\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5541 - accuracy: 0.7290 - val_loss: 0.6020 - val_accuracy: 0.6807\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5543 - accuracy: 0.7263 - val_loss: 0.5847 - val_accuracy: 0.6984\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7239 - val_loss: 0.5846 - val_accuracy: 0.6971\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7249 - val_loss: 0.5923 - val_accuracy: 0.6912\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.7194 - val_loss: 0.5837 - val_accuracy: 0.6919\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5521 - accuracy: 0.7263 - val_loss: 0.6026 - val_accuracy: 0.6866\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5503 - accuracy: 0.7267 - val_loss: 0.5837 - val_accuracy: 0.7024\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5491 - accuracy: 0.7300 - val_loss: 0.5816 - val_accuracy: 0.7037\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5501 - accuracy: 0.7270 - val_loss: 0.5816 - val_accuracy: 0.7057\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7244 - val_loss: 0.6018 - val_accuracy: 0.6813\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7247 - val_loss: 0.5926 - val_accuracy: 0.6971\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5475 - accuracy: 0.7300 - val_loss: 0.5819 - val_accuracy: 0.6991\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.7323 - val_loss: 0.5813 - val_accuracy: 0.7017\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5449 - accuracy: 0.7321 - val_loss: 0.5784 - val_accuracy: 0.7109\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.7301 - val_loss: 0.5811 - val_accuracy: 0.7030\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.7306 - val_loss: 0.5826 - val_accuracy: 0.6991\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5426 - accuracy: 0.7332 - val_loss: 0.5866 - val_accuracy: 0.6958\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.7319 - val_loss: 0.5959 - val_accuracy: 0.6892\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7334 - val_loss: 0.5777 - val_accuracy: 0.7057\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7336 - val_loss: 0.5832 - val_accuracy: 0.6938\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5404 - accuracy: 0.7332 - val_loss: 0.5773 - val_accuracy: 0.7135\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5419 - accuracy: 0.7324 - val_loss: 0.5770 - val_accuracy: 0.7129\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5401 - accuracy: 0.7354 - val_loss: 0.5999 - val_accuracy: 0.6886\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.7329 - val_loss: 0.5775 - val_accuracy: 0.7017\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7354 - val_loss: 0.5787 - val_accuracy: 0.7162\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5361 - accuracy: 0.7392 - val_loss: 0.5779 - val_accuracy: 0.7050\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.7370 - val_loss: 0.5787 - val_accuracy: 0.6997\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7398 - val_loss: 0.6132 - val_accuracy: 0.6741\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5384 - accuracy: 0.7311 - val_loss: 0.6154 - val_accuracy: 0.6728\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5378 - accuracy: 0.7364 - val_loss: 0.5947 - val_accuracy: 0.6866\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7406 - val_loss: 0.6060 - val_accuracy: 0.6721\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.7329 - val_loss: 0.5811 - val_accuracy: 0.6971\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7387 - val_loss: 0.5776 - val_accuracy: 0.7050\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5307 - accuracy: 0.7410 - val_loss: 0.5751 - val_accuracy: 0.7122\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7375 - val_loss: 0.5801 - val_accuracy: 0.6965\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7383 - val_loss: 0.6014 - val_accuracy: 0.6813\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7406 - val_loss: 0.5958 - val_accuracy: 0.6919\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5272 - accuracy: 0.7441 - val_loss: 0.6245 - val_accuracy: 0.6629\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5305 - accuracy: 0.7354 - val_loss: 0.5857 - val_accuracy: 0.6945\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7415 - val_loss: 0.5781 - val_accuracy: 0.7148\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5265 - accuracy: 0.7436 - val_loss: 0.5836 - val_accuracy: 0.6971\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5277 - accuracy: 0.7433 - val_loss: 0.5731 - val_accuracy: 0.7142\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5242 - accuracy: 0.7457 - val_loss: 0.6234 - val_accuracy: 0.6820\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7408 - val_loss: 0.5754 - val_accuracy: 0.7162\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5285 - accuracy: 0.7431 - val_loss: 0.5718 - val_accuracy: 0.7148\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7454 - val_loss: 0.5734 - val_accuracy: 0.7122\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5283 - accuracy: 0.7439 - val_loss: 0.5731 - val_accuracy: 0.7188\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5240 - accuracy: 0.7454 - val_loss: 0.5955 - val_accuracy: 0.6873\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7447 - val_loss: 0.6088 - val_accuracy: 0.6715\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5280 - accuracy: 0.7441 - val_loss: 0.5738 - val_accuracy: 0.7208\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7431 - val_loss: 0.6065 - val_accuracy: 0.6741\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5202 - accuracy: 0.7489 - val_loss: 0.6174 - val_accuracy: 0.6708\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7467 - val_loss: 0.5913 - val_accuracy: 0.6951\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5237 - accuracy: 0.7467 - val_loss: 0.5788 - val_accuracy: 0.7148\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5175 - accuracy: 0.7480 - val_loss: 0.5894 - val_accuracy: 0.7135\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.7470 - val_loss: 0.5735 - val_accuracy: 0.7201\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5210 - accuracy: 0.7467 - val_loss: 0.5703 - val_accuracy: 0.7155\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5159 - accuracy: 0.7479 - val_loss: 0.5709 - val_accuracy: 0.7188\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7470 - val_loss: 0.5735 - val_accuracy: 0.7234\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.7502 - val_loss: 0.5709 - val_accuracy: 0.7260\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7503 - val_loss: 0.5690 - val_accuracy: 0.7214\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5150 - accuracy: 0.7492 - val_loss: 0.6215 - val_accuracy: 0.6649\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.7500 - val_loss: 0.5685 - val_accuracy: 0.7181\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5165 - accuracy: 0.7500 - val_loss: 0.5740 - val_accuracy: 0.7181\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5146 - accuracy: 0.7464 - val_loss: 0.5745 - val_accuracy: 0.7201\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5115 - accuracy: 0.7538 - val_loss: 0.5717 - val_accuracy: 0.7122\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5129 - accuracy: 0.7559 - val_loss: 0.5665 - val_accuracy: 0.7175\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7457 - val_loss: 0.5783 - val_accuracy: 0.6997\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5089 - accuracy: 0.7553 - val_loss: 0.5680 - val_accuracy: 0.7116\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7585 - val_loss: 0.6328 - val_accuracy: 0.6577\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5104 - accuracy: 0.7528 - val_loss: 0.5683 - val_accuracy: 0.7227\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7562 - val_loss: 0.6008 - val_accuracy: 0.6787\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5094 - accuracy: 0.7553 - val_loss: 0.5655 - val_accuracy: 0.7194\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5073 - accuracy: 0.7530 - val_loss: 0.5844 - val_accuracy: 0.6971\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7557 - val_loss: 0.5670 - val_accuracy: 0.7188\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7607 - val_loss: 0.5999 - val_accuracy: 0.6827\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5064 - accuracy: 0.7556 - val_loss: 0.6133 - val_accuracy: 0.6932\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7574 - val_loss: 0.5772 - val_accuracy: 0.7234\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5136 - accuracy: 0.7525 - val_loss: 0.5709 - val_accuracy: 0.7063\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5025 - accuracy: 0.7608 - val_loss: 0.6073 - val_accuracy: 0.6984\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5055 - accuracy: 0.7597 - val_loss: 0.6718 - val_accuracy: 0.6386\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5016 - accuracy: 0.7595 - val_loss: 0.5709 - val_accuracy: 0.7175\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.7549 - val_loss: 0.5664 - val_accuracy: 0.7254\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.7554 - val_loss: 0.5666 - val_accuracy: 0.7280\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5084 - accuracy: 0.7571 - val_loss: 0.5661 - val_accuracy: 0.7214\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5014 - accuracy: 0.7577 - val_loss: 0.5919 - val_accuracy: 0.6853\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7635 - val_loss: 0.5683 - val_accuracy: 0.7083\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7602 - val_loss: 0.5640 - val_accuracy: 0.7286\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4957 - accuracy: 0.7641 - val_loss: 0.5680 - val_accuracy: 0.7332\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7508 - val_loss: 0.5715 - val_accuracy: 0.7306\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.7590 - val_loss: 0.5867 - val_accuracy: 0.6886\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4973 - accuracy: 0.7676 - val_loss: 0.5656 - val_accuracy: 0.7260\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5020 - accuracy: 0.7554 - val_loss: 0.5655 - val_accuracy: 0.7240\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4960 - accuracy: 0.7603 - val_loss: 0.5633 - val_accuracy: 0.7267\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7576 - val_loss: 0.5696 - val_accuracy: 0.7129\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4960 - accuracy: 0.7623 - val_loss: 0.5889 - val_accuracy: 0.7089\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4903 - accuracy: 0.7661 - val_loss: 0.5857 - val_accuracy: 0.6951\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4957 - accuracy: 0.7661 - val_loss: 0.5653 - val_accuracy: 0.7201\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4989 - accuracy: 0.7571 - val_loss: 0.5854 - val_accuracy: 0.7175\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.7649 - val_loss: 0.5631 - val_accuracy: 0.7267\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.7661 - val_loss: 0.5725 - val_accuracy: 0.7083\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4955 - accuracy: 0.7653 - val_loss: 0.6223 - val_accuracy: 0.6925\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4947 - accuracy: 0.7620 - val_loss: 0.6163 - val_accuracy: 0.6735\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4890 - accuracy: 0.7643 - val_loss: 0.6645 - val_accuracy: 0.6465\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4935 - accuracy: 0.7622 - val_loss: 0.5647 - val_accuracy: 0.7293\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4857 - accuracy: 0.7694 - val_loss: 0.5890 - val_accuracy: 0.6886\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7643 - val_loss: 0.5930 - val_accuracy: 0.6853\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7605 - val_loss: 0.5853 - val_accuracy: 0.7227\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7654 - val_loss: 0.5635 - val_accuracy: 0.7280\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7623 - val_loss: 0.5653 - val_accuracy: 0.7155\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7702 - val_loss: 0.5679 - val_accuracy: 0.7083\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.7679 - val_loss: 0.6788 - val_accuracy: 0.6340\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.7648 - val_loss: 0.5685 - val_accuracy: 0.7083\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7695 - val_loss: 0.5759 - val_accuracy: 0.7280\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4892 - accuracy: 0.7669 - val_loss: 0.6250 - val_accuracy: 0.6689\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4820 - accuracy: 0.7704 - val_loss: 0.5635 - val_accuracy: 0.7194\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4887 - accuracy: 0.7640 - val_loss: 0.5721 - val_accuracy: 0.7037\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4843 - accuracy: 0.7700 - val_loss: 0.5746 - val_accuracy: 0.7116\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4823 - accuracy: 0.7714 - val_loss: 0.5646 - val_accuracy: 0.7260\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4865 - accuracy: 0.7691 - val_loss: 0.5846 - val_accuracy: 0.6899\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4844 - accuracy: 0.7750 - val_loss: 0.5748 - val_accuracy: 0.7050\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4737 - accuracy: 0.7833 - val_loss: 0.5642 - val_accuracy: 0.7286\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4836 - accuracy: 0.7730 - val_loss: 0.5645 - val_accuracy: 0.7260\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4801 - accuracy: 0.7756 - val_loss: 0.5672 - val_accuracy: 0.7102\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4797 - accuracy: 0.7763 - val_loss: 0.5652 - val_accuracy: 0.7188\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4744 - accuracy: 0.7784 - val_loss: 0.5669 - val_accuracy: 0.7142\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4785 - accuracy: 0.7743 - val_loss: 0.5745 - val_accuracy: 0.7004\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4897 - accuracy: 0.7649 - val_loss: 0.5667 - val_accuracy: 0.7378\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4698 - accuracy: 0.7825 - val_loss: 0.5737 - val_accuracy: 0.7221\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4871 - accuracy: 0.7668 - val_loss: 0.5629 - val_accuracy: 0.7280\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4881 - accuracy: 0.7664 - val_loss: 0.5612 - val_accuracy: 0.7254\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4877 - accuracy: 0.7735 - val_loss: 0.5659 - val_accuracy: 0.7240\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 0.4689 - accuracy: 0.7789 - val_loss: 0.5660 - val_accuracy: 0.7267\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4742 - accuracy: 0.7774 - val_loss: 0.5662 - val_accuracy: 0.7306\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.4734 - accuracy: 0.7796 - val_loss: 0.5745 - val_accuracy: 0.7063\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4702 - accuracy: 0.7753 - val_loss: 0.5644 - val_accuracy: 0.7188\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4714 - accuracy: 0.7822 - val_loss: 0.5623 - val_accuracy: 0.7247\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4733 - accuracy: 0.7791 - val_loss: 0.5657 - val_accuracy: 0.7260\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4728 - accuracy: 0.7810 - val_loss: 0.5856 - val_accuracy: 0.7260\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4683 - accuracy: 0.7815 - val_loss: 0.5633 - val_accuracy: 0.7254\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.4722 - accuracy: 0.7822 - val_loss: 0.5803 - val_accuracy: 0.7247\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.5883 - accuracy: 0.7099\n",
            "Valid accuracy: 0.7098935842514038\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "            name='dense_layer1', activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(n_hiddens,\n",
        "name='dense_layer_2', activation='relu'))\n",
        "\n",
        "model.add(keras.layers.Dense(n_classes,\n",
        "name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "# Summary of the model.\n",
        "model.summary()\n",
        "\n",
        "# Compiling the model.\n",
        "model.compile(optimizer='SGD',\n",
        "loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "\n",
        "# Training the model.\n",
        "model.fit(XTrain_mixture, yTrain, batch_size=batch_size, epochs=num_epochs,\n",
        "verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "# Evaluating the model.\n",
        "NN_layers_valid_loss, NN_layers_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "print('Valid accuracy:', NN_layers_valid_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1MP8dfe92I"
      },
      "source": [
        "#### 4.2.3 - Dropout to avoid overfitting\n",
        "To avoid the overfitting, dropout values are tested for robustness of the model. We find the best dropout value by looping its value [0.1, 0.2, 0.3, 0.4, 0.5], and pick the best value that corresponds to the highest validation accuracy. Since the validation accurary will vary a bit for each run, we should choose the parameter corresponding to the highest validation accuracy each time with code instead of manual selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqlHlPVlCX6b",
        "outputId": "78025c5b-ad7c-4183-9ed8-5d3f6e9ae7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 2s 18ms/step - loss: 0.7076 - accuracy: 0.5350 - val_loss: 0.6769 - val_accuracy: 0.5749\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5660 - val_loss: 0.6657 - val_accuracy: 0.6038\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.6773 - accuracy: 0.5813 - val_loss: 0.6602 - val_accuracy: 0.6183\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.6704 - accuracy: 0.5910 - val_loss: 0.6568 - val_accuracy: 0.6183\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.6061 - val_loss: 0.6538 - val_accuracy: 0.6275\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6610 - accuracy: 0.5989 - val_loss: 0.6546 - val_accuracy: 0.6255\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6053 - val_loss: 0.6500 - val_accuracy: 0.6275\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.6176 - val_loss: 0.6471 - val_accuracy: 0.6386\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.6207 - val_loss: 0.6454 - val_accuracy: 0.6340\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6443 - accuracy: 0.6316 - val_loss: 0.6436 - val_accuracy: 0.6393\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6504 - accuracy: 0.6284 - val_loss: 0.6424 - val_accuracy: 0.6334\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6469 - accuracy: 0.6314 - val_loss: 0.6491 - val_accuracy: 0.6340\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6362 - val_loss: 0.6403 - val_accuracy: 0.6465\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6427 - accuracy: 0.6383 - val_loss: 0.6378 - val_accuracy: 0.6472\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6370 - accuracy: 0.6403 - val_loss: 0.6369 - val_accuracy: 0.6498\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 0.6396 - val_loss: 0.6355 - val_accuracy: 0.6537\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6403 - val_loss: 0.6370 - val_accuracy: 0.6459\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6487 - val_loss: 0.6351 - val_accuracy: 0.6505\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6330 - accuracy: 0.6372 - val_loss: 0.6344 - val_accuracy: 0.6518\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6299 - accuracy: 0.6506 - val_loss: 0.6314 - val_accuracy: 0.6583\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6281 - accuracy: 0.6516 - val_loss: 0.6313 - val_accuracy: 0.6577\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6269 - accuracy: 0.6564 - val_loss: 0.6297 - val_accuracy: 0.6636\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6276 - accuracy: 0.6529 - val_loss: 0.6285 - val_accuracy: 0.6636\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6260 - accuracy: 0.6470 - val_loss: 0.6281 - val_accuracy: 0.6623\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6229 - accuracy: 0.6529 - val_loss: 0.6300 - val_accuracy: 0.6570\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6215 - accuracy: 0.6587 - val_loss: 0.6272 - val_accuracy: 0.6623\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6169 - accuracy: 0.6661 - val_loss: 0.6259 - val_accuracy: 0.6682\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6625 - val_loss: 0.6248 - val_accuracy: 0.6675\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6539 - val_loss: 0.6244 - val_accuracy: 0.6669\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6192 - accuracy: 0.6649 - val_loss: 0.6230 - val_accuracy: 0.6669\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6176 - accuracy: 0.6603 - val_loss: 0.6268 - val_accuracy: 0.6577\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6139 - accuracy: 0.6675 - val_loss: 0.6226 - val_accuracy: 0.6675\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.6716 - val_loss: 0.6245 - val_accuracy: 0.6603\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6123 - accuracy: 0.6677 - val_loss: 0.6212 - val_accuracy: 0.6675\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6143 - accuracy: 0.6679 - val_loss: 0.6210 - val_accuracy: 0.6675\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6121 - accuracy: 0.6725 - val_loss: 0.6204 - val_accuracy: 0.6656\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6104 - accuracy: 0.6684 - val_loss: 0.6199 - val_accuracy: 0.6662\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6087 - accuracy: 0.6720 - val_loss: 0.6218 - val_accuracy: 0.6597\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.6759 - val_loss: 0.6215 - val_accuracy: 0.6590\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6040 - accuracy: 0.6789 - val_loss: 0.6195 - val_accuracy: 0.6649\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.6766 - val_loss: 0.6205 - val_accuracy: 0.6564\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.6772 - val_loss: 0.6178 - val_accuracy: 0.6708\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6052 - accuracy: 0.6766 - val_loss: 0.6219 - val_accuracy: 0.6577\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6728 - val_loss: 0.6233 - val_accuracy: 0.6537\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.6777 - val_loss: 0.6214 - val_accuracy: 0.6544\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6037 - accuracy: 0.6799 - val_loss: 0.6144 - val_accuracy: 0.6675\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.6818 - val_loss: 0.6139 - val_accuracy: 0.6643\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.6823 - val_loss: 0.6149 - val_accuracy: 0.6656\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.6850 - val_loss: 0.6133 - val_accuracy: 0.6748\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.6912 - val_loss: 0.6141 - val_accuracy: 0.6675\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5935 - accuracy: 0.6822 - val_loss: 0.6143 - val_accuracy: 0.6636\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5980 - accuracy: 0.6794 - val_loss: 0.6138 - val_accuracy: 0.6643\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5953 - accuracy: 0.6914 - val_loss: 0.6118 - val_accuracy: 0.6741\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5960 - accuracy: 0.6850 - val_loss: 0.6113 - val_accuracy: 0.6689\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6799 - val_loss: 0.6121 - val_accuracy: 0.6669\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.6864 - val_loss: 0.6102 - val_accuracy: 0.6682\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.6894 - val_loss: 0.6097 - val_accuracy: 0.6761\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5921 - accuracy: 0.6874 - val_loss: 0.6104 - val_accuracy: 0.6695\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.6876 - val_loss: 0.6087 - val_accuracy: 0.6708\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.6937 - val_loss: 0.6077 - val_accuracy: 0.6761\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.6925 - val_loss: 0.6071 - val_accuracy: 0.6735\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5899 - accuracy: 0.6909 - val_loss: 0.6136 - val_accuracy: 0.6597\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5842 - accuracy: 0.6955 - val_loss: 0.6076 - val_accuracy: 0.6721\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5832 - accuracy: 0.6976 - val_loss: 0.6063 - val_accuracy: 0.6708\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5849 - accuracy: 0.7001 - val_loss: 0.6085 - val_accuracy: 0.6675\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5870 - accuracy: 0.6984 - val_loss: 0.6054 - val_accuracy: 0.6787\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5867 - accuracy: 0.6943 - val_loss: 0.6065 - val_accuracy: 0.6675\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5854 - accuracy: 0.6925 - val_loss: 0.6046 - val_accuracy: 0.6748\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5816 - accuracy: 0.6978 - val_loss: 0.6063 - val_accuracy: 0.6675\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5838 - accuracy: 0.6897 - val_loss: 0.6035 - val_accuracy: 0.6774\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7024 - val_loss: 0.6047 - val_accuracy: 0.6754\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5824 - accuracy: 0.6923 - val_loss: 0.6044 - val_accuracy: 0.6741\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5798 - accuracy: 0.6978 - val_loss: 0.6032 - val_accuracy: 0.6781\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5822 - accuracy: 0.7024 - val_loss: 0.6019 - val_accuracy: 0.6787\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7022 - val_loss: 0.6021 - val_accuracy: 0.6794\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7055 - val_loss: 0.6051 - val_accuracy: 0.6708\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7009 - val_loss: 0.6014 - val_accuracy: 0.6807\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5751 - accuracy: 0.7032 - val_loss: 0.6033 - val_accuracy: 0.6787\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5786 - accuracy: 0.6965 - val_loss: 0.6029 - val_accuracy: 0.6781\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5786 - accuracy: 0.7020 - val_loss: 0.6012 - val_accuracy: 0.6794\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7081 - val_loss: 0.6075 - val_accuracy: 0.6695\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5731 - accuracy: 0.7058 - val_loss: 0.5997 - val_accuracy: 0.6827\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.7038 - val_loss: 0.5996 - val_accuracy: 0.6833\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7038 - val_loss: 0.5989 - val_accuracy: 0.6827\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5731 - accuracy: 0.7070 - val_loss: 0.5988 - val_accuracy: 0.6833\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5752 - accuracy: 0.6983 - val_loss: 0.5977 - val_accuracy: 0.6873\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5708 - accuracy: 0.7148 - val_loss: 0.5984 - val_accuracy: 0.6820\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5695 - accuracy: 0.7086 - val_loss: 0.5975 - val_accuracy: 0.6866\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5719 - accuracy: 0.7116 - val_loss: 0.5969 - val_accuracy: 0.6866\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5728 - accuracy: 0.7045 - val_loss: 0.5965 - val_accuracy: 0.6866\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5676 - accuracy: 0.7101 - val_loss: 0.5972 - val_accuracy: 0.6846\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5690 - accuracy: 0.7099 - val_loss: 0.5965 - val_accuracy: 0.6859\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5676 - accuracy: 0.7106 - val_loss: 0.5959 - val_accuracy: 0.6866\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5671 - accuracy: 0.7098 - val_loss: 0.5951 - val_accuracy: 0.6859\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5677 - accuracy: 0.7099 - val_loss: 0.5952 - val_accuracy: 0.6932\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5695 - accuracy: 0.7111 - val_loss: 0.5955 - val_accuracy: 0.6859\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7093 - val_loss: 0.5948 - val_accuracy: 0.6840\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5672 - accuracy: 0.7117 - val_loss: 0.5983 - val_accuracy: 0.6846\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5629 - accuracy: 0.7124 - val_loss: 0.5978 - val_accuracy: 0.6859\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5653 - accuracy: 0.7117 - val_loss: 0.5945 - val_accuracy: 0.6866\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5650 - accuracy: 0.7147 - val_loss: 0.5958 - val_accuracy: 0.6853\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5623 - accuracy: 0.7065 - val_loss: 0.5933 - val_accuracy: 0.6892\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.7157 - val_loss: 0.5934 - val_accuracy: 0.6899\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7196 - val_loss: 0.5918 - val_accuracy: 0.6892\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5574 - accuracy: 0.7199 - val_loss: 0.5925 - val_accuracy: 0.6905\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5582 - accuracy: 0.7180 - val_loss: 0.5920 - val_accuracy: 0.6912\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5587 - accuracy: 0.7181 - val_loss: 0.5949 - val_accuracy: 0.6859\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5588 - accuracy: 0.7196 - val_loss: 0.5909 - val_accuracy: 0.6879\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5579 - accuracy: 0.7171 - val_loss: 0.5933 - val_accuracy: 0.6873\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5540 - accuracy: 0.7213 - val_loss: 0.5905 - val_accuracy: 0.6938\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5583 - accuracy: 0.7168 - val_loss: 0.5926 - val_accuracy: 0.6886\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5558 - accuracy: 0.7199 - val_loss: 0.5912 - val_accuracy: 0.6899\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7226 - val_loss: 0.5949 - val_accuracy: 0.6873\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5564 - accuracy: 0.7162 - val_loss: 0.5906 - val_accuracy: 0.6925\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5542 - accuracy: 0.7262 - val_loss: 0.5920 - val_accuracy: 0.6846\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5559 - accuracy: 0.7208 - val_loss: 0.5904 - val_accuracy: 0.6938\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5568 - accuracy: 0.7153 - val_loss: 0.5885 - val_accuracy: 0.6925\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5535 - accuracy: 0.7224 - val_loss: 0.5881 - val_accuracy: 0.6951\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5544 - accuracy: 0.7191 - val_loss: 0.5885 - val_accuracy: 0.6965\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.7171 - val_loss: 0.5876 - val_accuracy: 0.6938\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.7254 - val_loss: 0.5907 - val_accuracy: 0.6899\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5496 - accuracy: 0.7198 - val_loss: 0.5891 - val_accuracy: 0.6899\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5519 - accuracy: 0.7245 - val_loss: 0.5915 - val_accuracy: 0.6912\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7231 - val_loss: 0.5873 - val_accuracy: 0.6925\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5531 - accuracy: 0.7214 - val_loss: 0.5869 - val_accuracy: 0.6984\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5512 - accuracy: 0.7244 - val_loss: 0.5852 - val_accuracy: 0.6945\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5524 - accuracy: 0.7227 - val_loss: 0.5883 - val_accuracy: 0.6951\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5463 - accuracy: 0.7291 - val_loss: 0.5862 - val_accuracy: 0.6912\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5471 - accuracy: 0.7221 - val_loss: 0.5850 - val_accuracy: 0.6945\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5485 - accuracy: 0.7203 - val_loss: 0.5853 - val_accuracy: 0.6925\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5485 - accuracy: 0.7272 - val_loss: 0.5860 - val_accuracy: 0.6938\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.7293 - val_loss: 0.5854 - val_accuracy: 0.6951\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5479 - accuracy: 0.7303 - val_loss: 0.5868 - val_accuracy: 0.6951\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5493 - accuracy: 0.7222 - val_loss: 0.5905 - val_accuracy: 0.6932\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5481 - accuracy: 0.7265 - val_loss: 0.5845 - val_accuracy: 0.6991\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.7242 - val_loss: 0.5845 - val_accuracy: 0.6991\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5427 - accuracy: 0.7301 - val_loss: 0.5841 - val_accuracy: 0.6965\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5442 - accuracy: 0.7305 - val_loss: 0.5846 - val_accuracy: 0.6945\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7306 - val_loss: 0.5829 - val_accuracy: 0.7004\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5440 - accuracy: 0.7273 - val_loss: 0.5830 - val_accuracy: 0.7004\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5459 - accuracy: 0.7311 - val_loss: 0.5831 - val_accuracy: 0.6984\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5416 - accuracy: 0.7316 - val_loss: 0.5827 - val_accuracy: 0.7004\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5428 - accuracy: 0.7288 - val_loss: 0.5828 - val_accuracy: 0.6997\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5398 - accuracy: 0.7331 - val_loss: 0.5819 - val_accuracy: 0.6997\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.7311 - val_loss: 0.5819 - val_accuracy: 0.7017\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5408 - accuracy: 0.7324 - val_loss: 0.5816 - val_accuracy: 0.6991\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5384 - accuracy: 0.7313 - val_loss: 0.5828 - val_accuracy: 0.6951\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5379 - accuracy: 0.7347 - val_loss: 0.5817 - val_accuracy: 0.6984\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.7352 - val_loss: 0.5815 - val_accuracy: 0.6965\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5378 - accuracy: 0.7378 - val_loss: 0.5809 - val_accuracy: 0.7004\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5384 - accuracy: 0.7285 - val_loss: 0.5806 - val_accuracy: 0.6984\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5401 - accuracy: 0.7293 - val_loss: 0.5797 - val_accuracy: 0.7024\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.7313 - val_loss: 0.5807 - val_accuracy: 0.6965\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5356 - accuracy: 0.7359 - val_loss: 0.5794 - val_accuracy: 0.6997\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5376 - accuracy: 0.7347 - val_loss: 0.5792 - val_accuracy: 0.7024\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7360 - val_loss: 0.5793 - val_accuracy: 0.7063\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5352 - accuracy: 0.7390 - val_loss: 0.5790 - val_accuracy: 0.6991\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5334 - accuracy: 0.7388 - val_loss: 0.5798 - val_accuracy: 0.7011\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5319 - accuracy: 0.7313 - val_loss: 0.5797 - val_accuracy: 0.7011\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7374 - val_loss: 0.5799 - val_accuracy: 0.7011\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7349 - val_loss: 0.5787 - val_accuracy: 0.7017\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5347 - accuracy: 0.7374 - val_loss: 0.5799 - val_accuracy: 0.7017\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7369 - val_loss: 0.5790 - val_accuracy: 0.7043\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7300 - val_loss: 0.5777 - val_accuracy: 0.7030\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5310 - accuracy: 0.7438 - val_loss: 0.5818 - val_accuracy: 0.7050\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7424 - val_loss: 0.5780 - val_accuracy: 0.6991\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7385 - val_loss: 0.5767 - val_accuracy: 0.7030\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5305 - accuracy: 0.7397 - val_loss: 0.5782 - val_accuracy: 0.7070\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5303 - accuracy: 0.7418 - val_loss: 0.5774 - val_accuracy: 0.6984\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7359 - val_loss: 0.5773 - val_accuracy: 0.7017\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7416 - val_loss: 0.5779 - val_accuracy: 0.6997\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5280 - accuracy: 0.7390 - val_loss: 0.5782 - val_accuracy: 0.7017\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.7413 - val_loss: 0.5808 - val_accuracy: 0.7043\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5234 - accuracy: 0.7475 - val_loss: 0.5754 - val_accuracy: 0.7050\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5236 - accuracy: 0.7438 - val_loss: 0.5789 - val_accuracy: 0.7057\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5277 - accuracy: 0.7411 - val_loss: 0.5778 - val_accuracy: 0.7070\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5283 - accuracy: 0.7416 - val_loss: 0.5769 - val_accuracy: 0.7057\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.7408 - val_loss: 0.5768 - val_accuracy: 0.7070\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5250 - accuracy: 0.7401 - val_loss: 0.5752 - val_accuracy: 0.7037\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5228 - accuracy: 0.7424 - val_loss: 0.5763 - val_accuracy: 0.7070\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.7385 - val_loss: 0.5783 - val_accuracy: 0.7057\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7433 - val_loss: 0.5760 - val_accuracy: 0.7057\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5243 - accuracy: 0.7436 - val_loss: 0.5762 - val_accuracy: 0.7011\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5206 - accuracy: 0.7477 - val_loss: 0.5744 - val_accuracy: 0.7089\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5214 - accuracy: 0.7467 - val_loss: 0.5744 - val_accuracy: 0.7050\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5198 - accuracy: 0.7474 - val_loss: 0.5734 - val_accuracy: 0.7070\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7426 - val_loss: 0.5735 - val_accuracy: 0.7070\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5199 - accuracy: 0.7475 - val_loss: 0.5749 - val_accuracy: 0.7057\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7484 - val_loss: 0.5736 - val_accuracy: 0.7076\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5187 - accuracy: 0.7416 - val_loss: 0.5734 - val_accuracy: 0.7037\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5194 - accuracy: 0.7474 - val_loss: 0.5749 - val_accuracy: 0.7109\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5150 - accuracy: 0.7503 - val_loss: 0.5757 - val_accuracy: 0.7050\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5167 - accuracy: 0.7505 - val_loss: 0.5745 - val_accuracy: 0.7050\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5150 - accuracy: 0.7534 - val_loss: 0.5730 - val_accuracy: 0.7076\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5170 - accuracy: 0.7538 - val_loss: 0.5730 - val_accuracy: 0.7076\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5206 - accuracy: 0.7464 - val_loss: 0.5741 - val_accuracy: 0.7109\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5187 - accuracy: 0.7487 - val_loss: 0.5728 - val_accuracy: 0.7076\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5120 - accuracy: 0.7508 - val_loss: 0.5729 - val_accuracy: 0.7076\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 0.7528 - val_loss: 0.5712 - val_accuracy: 0.7089\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5138 - accuracy: 0.7510 - val_loss: 0.5727 - val_accuracy: 0.7076\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7233\n",
            "This is for dropout with a value of 0.10\n",
            "Valid accuracy: 0.7232952117919922\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.7330 - accuracy: 0.5315 - val_loss: 0.6608 - val_accuracy: 0.6150\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6998 - accuracy: 0.5540 - val_loss: 0.6566 - val_accuracy: 0.6261\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5672 - val_loss: 0.6475 - val_accuracy: 0.6406\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.5828 - val_loss: 0.6482 - val_accuracy: 0.6472\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6716 - accuracy: 0.5889 - val_loss: 0.6421 - val_accuracy: 0.6459\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.6033 - val_loss: 0.6411 - val_accuracy: 0.6478\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.6027 - val_loss: 0.6416 - val_accuracy: 0.6524\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6059 - val_loss: 0.6372 - val_accuracy: 0.6505\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6537 - accuracy: 0.6174 - val_loss: 0.6365 - val_accuracy: 0.6537\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6505 - accuracy: 0.6229 - val_loss: 0.6345 - val_accuracy: 0.6577\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6217 - val_loss: 0.6391 - val_accuracy: 0.6531\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.6173 - val_loss: 0.6344 - val_accuracy: 0.6603\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6459 - accuracy: 0.6303 - val_loss: 0.6344 - val_accuracy: 0.6577\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6409 - accuracy: 0.6309 - val_loss: 0.6327 - val_accuracy: 0.6557\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6447 - accuracy: 0.6322 - val_loss: 0.6314 - val_accuracy: 0.6603\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6420 - accuracy: 0.6257 - val_loss: 0.6304 - val_accuracy: 0.6583\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6414 - accuracy: 0.6344 - val_loss: 0.6291 - val_accuracy: 0.6564\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6400 - accuracy: 0.6353 - val_loss: 0.6284 - val_accuracy: 0.6557\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.6401 - val_loss: 0.6268 - val_accuracy: 0.6544\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6377 - accuracy: 0.6393 - val_loss: 0.6273 - val_accuracy: 0.6557\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6343 - accuracy: 0.6445 - val_loss: 0.6254 - val_accuracy: 0.6597\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6313 - accuracy: 0.6467 - val_loss: 0.6235 - val_accuracy: 0.6616\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 0.6480 - val_loss: 0.6240 - val_accuracy: 0.6610\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6275 - accuracy: 0.6480 - val_loss: 0.6223 - val_accuracy: 0.6636\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6292 - accuracy: 0.6485 - val_loss: 0.6259 - val_accuracy: 0.6564\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6318 - accuracy: 0.6475 - val_loss: 0.6221 - val_accuracy: 0.6616\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6289 - accuracy: 0.6449 - val_loss: 0.6231 - val_accuracy: 0.6557\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6300 - accuracy: 0.6488 - val_loss: 0.6208 - val_accuracy: 0.6597\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.6537 - val_loss: 0.6198 - val_accuracy: 0.6643\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6211 - accuracy: 0.6618 - val_loss: 0.6223 - val_accuracy: 0.6583\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6232 - accuracy: 0.6560 - val_loss: 0.6211 - val_accuracy: 0.6583\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6194 - accuracy: 0.6574 - val_loss: 0.6185 - val_accuracy: 0.6603\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6185 - accuracy: 0.6672 - val_loss: 0.6169 - val_accuracy: 0.6662\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6164 - accuracy: 0.6646 - val_loss: 0.6156 - val_accuracy: 0.6636\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6201 - accuracy: 0.6556 - val_loss: 0.6158 - val_accuracy: 0.6610\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6144 - accuracy: 0.6697 - val_loss: 0.6145 - val_accuracy: 0.6656\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6188 - accuracy: 0.6611 - val_loss: 0.6147 - val_accuracy: 0.6636\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6132 - accuracy: 0.6651 - val_loss: 0.6143 - val_accuracy: 0.6636\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6171 - accuracy: 0.6689 - val_loss: 0.6147 - val_accuracy: 0.6623\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.6631 - val_loss: 0.6143 - val_accuracy: 0.6649\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6120 - accuracy: 0.6662 - val_loss: 0.6134 - val_accuracy: 0.6656\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6111 - accuracy: 0.6730 - val_loss: 0.6123 - val_accuracy: 0.6689\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6094 - accuracy: 0.6736 - val_loss: 0.6137 - val_accuracy: 0.6656\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6101 - accuracy: 0.6703 - val_loss: 0.6144 - val_accuracy: 0.6623\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6091 - accuracy: 0.6735 - val_loss: 0.6117 - val_accuracy: 0.6721\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6070 - accuracy: 0.6785 - val_loss: 0.6125 - val_accuracy: 0.6649\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6109 - accuracy: 0.6725 - val_loss: 0.6103 - val_accuracy: 0.6689\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6013 - accuracy: 0.6739 - val_loss: 0.6109 - val_accuracy: 0.6669\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6105 - accuracy: 0.6689 - val_loss: 0.6108 - val_accuracy: 0.6702\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6066 - accuracy: 0.6744 - val_loss: 0.6104 - val_accuracy: 0.6649\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6088 - accuracy: 0.6677 - val_loss: 0.6089 - val_accuracy: 0.6702\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6091 - accuracy: 0.6677 - val_loss: 0.6089 - val_accuracy: 0.6715\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6014 - accuracy: 0.6736 - val_loss: 0.6077 - val_accuracy: 0.6689\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6066 - accuracy: 0.6695 - val_loss: 0.6071 - val_accuracy: 0.6682\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6023 - accuracy: 0.6807 - val_loss: 0.6079 - val_accuracy: 0.6728\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6754 - val_loss: 0.6067 - val_accuracy: 0.6689\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6021 - accuracy: 0.6825 - val_loss: 0.6070 - val_accuracy: 0.6715\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6817 - val_loss: 0.6084 - val_accuracy: 0.6656\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5995 - accuracy: 0.6771 - val_loss: 0.6072 - val_accuracy: 0.6702\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5989 - accuracy: 0.6784 - val_loss: 0.6053 - val_accuracy: 0.6695\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5983 - accuracy: 0.6840 - val_loss: 0.6059 - val_accuracy: 0.6721\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.6777 - val_loss: 0.6049 - val_accuracy: 0.6702\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.6884 - val_loss: 0.6066 - val_accuracy: 0.6662\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6864 - val_loss: 0.6043 - val_accuracy: 0.6669\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5967 - accuracy: 0.6836 - val_loss: 0.6040 - val_accuracy: 0.6702\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5951 - accuracy: 0.6836 - val_loss: 0.6040 - val_accuracy: 0.6702\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5947 - accuracy: 0.6846 - val_loss: 0.6052 - val_accuracy: 0.6695\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5941 - accuracy: 0.6887 - val_loss: 0.6040 - val_accuracy: 0.6669\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.6927 - val_loss: 0.6045 - val_accuracy: 0.6669\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5941 - accuracy: 0.6808 - val_loss: 0.6022 - val_accuracy: 0.6708\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6902 - val_loss: 0.6018 - val_accuracy: 0.6669\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6897 - val_loss: 0.6029 - val_accuracy: 0.6695\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6869 - val_loss: 0.6005 - val_accuracy: 0.6728\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5899 - accuracy: 0.6940 - val_loss: 0.6021 - val_accuracy: 0.6695\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5877 - accuracy: 0.6937 - val_loss: 0.6005 - val_accuracy: 0.6721\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5894 - accuracy: 0.6943 - val_loss: 0.6004 - val_accuracy: 0.6669\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.5922 - accuracy: 0.6894 - val_loss: 0.5999 - val_accuracy: 0.6721\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5903 - accuracy: 0.6912 - val_loss: 0.5995 - val_accuracy: 0.6721\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5863 - accuracy: 0.6933 - val_loss: 0.5986 - val_accuracy: 0.6721\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.5890 - accuracy: 0.6937 - val_loss: 0.6003 - val_accuracy: 0.6761\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5880 - accuracy: 0.6896 - val_loss: 0.5977 - val_accuracy: 0.6728\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5901 - accuracy: 0.6915 - val_loss: 0.5981 - val_accuracy: 0.6735\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.5836 - accuracy: 0.6938 - val_loss: 0.5974 - val_accuracy: 0.6741\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5868 - accuracy: 0.6956 - val_loss: 0.5972 - val_accuracy: 0.6767\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5859 - accuracy: 0.6922 - val_loss: 0.5974 - val_accuracy: 0.6807\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5872 - accuracy: 0.6869 - val_loss: 0.5969 - val_accuracy: 0.6774\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.5850 - accuracy: 0.6945 - val_loss: 0.5989 - val_accuracy: 0.6754\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5852 - accuracy: 0.6983 - val_loss: 0.5966 - val_accuracy: 0.6781\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5839 - accuracy: 0.6968 - val_loss: 0.5958 - val_accuracy: 0.6807\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5791 - accuracy: 0.6971 - val_loss: 0.5961 - val_accuracy: 0.6800\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5841 - accuracy: 0.6997 - val_loss: 0.5959 - val_accuracy: 0.6794\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 1s 11ms/step - loss: 0.5817 - accuracy: 0.7001 - val_loss: 0.5953 - val_accuracy: 0.6833\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5781 - accuracy: 0.7030 - val_loss: 0.5951 - val_accuracy: 0.6813\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5818 - accuracy: 0.6945 - val_loss: 0.5952 - val_accuracy: 0.6781\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5790 - accuracy: 0.7009 - val_loss: 0.5950 - val_accuracy: 0.6767\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 8ms/step - loss: 0.5820 - accuracy: 0.7017 - val_loss: 0.5948 - val_accuracy: 0.6761\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 10ms/step - loss: 0.5775 - accuracy: 0.7019 - val_loss: 0.5942 - val_accuracy: 0.6800\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5784 - accuracy: 0.7006 - val_loss: 0.5932 - val_accuracy: 0.6820\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5785 - accuracy: 0.7025 - val_loss: 0.5936 - val_accuracy: 0.6833\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5796 - accuracy: 0.7029 - val_loss: 0.5940 - val_accuracy: 0.6774\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5808 - accuracy: 0.7048 - val_loss: 0.5975 - val_accuracy: 0.6754\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5753 - accuracy: 0.7080 - val_loss: 0.5944 - val_accuracy: 0.6787\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5740 - accuracy: 0.7029 - val_loss: 0.5923 - val_accuracy: 0.6840\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5777 - accuracy: 0.6992 - val_loss: 0.5945 - val_accuracy: 0.6781\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.7034 - val_loss: 0.5921 - val_accuracy: 0.6787\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.6999 - val_loss: 0.5909 - val_accuracy: 0.6859\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5709 - accuracy: 0.7099 - val_loss: 0.5916 - val_accuracy: 0.6807\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5720 - accuracy: 0.7066 - val_loss: 0.5904 - val_accuracy: 0.6820\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5754 - accuracy: 0.7009 - val_loss: 0.5908 - val_accuracy: 0.6859\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5743 - accuracy: 0.7019 - val_loss: 0.5898 - val_accuracy: 0.6846\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5714 - accuracy: 0.7063 - val_loss: 0.5905 - val_accuracy: 0.6840\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5721 - accuracy: 0.7073 - val_loss: 0.5927 - val_accuracy: 0.6813\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5709 - accuracy: 0.7058 - val_loss: 0.5962 - val_accuracy: 0.6761\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5694 - accuracy: 0.7106 - val_loss: 0.5889 - val_accuracy: 0.6833\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.7066 - val_loss: 0.5892 - val_accuracy: 0.6840\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5721 - accuracy: 0.7004 - val_loss: 0.5881 - val_accuracy: 0.6840\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5704 - accuracy: 0.7134 - val_loss: 0.5884 - val_accuracy: 0.6840\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5698 - accuracy: 0.7117 - val_loss: 0.5890 - val_accuracy: 0.6846\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5643 - accuracy: 0.7101 - val_loss: 0.5887 - val_accuracy: 0.6846\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5719 - accuracy: 0.7066 - val_loss: 0.5900 - val_accuracy: 0.6879\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5702 - accuracy: 0.7081 - val_loss: 0.5875 - val_accuracy: 0.6873\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5679 - accuracy: 0.7089 - val_loss: 0.5871 - val_accuracy: 0.6866\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5684 - accuracy: 0.7102 - val_loss: 0.5868 - val_accuracy: 0.6833\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5693 - accuracy: 0.7088 - val_loss: 0.5872 - val_accuracy: 0.6853\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5649 - accuracy: 0.7134 - val_loss: 0.5881 - val_accuracy: 0.6866\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5657 - accuracy: 0.7134 - val_loss: 0.5866 - val_accuracy: 0.6859\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5655 - accuracy: 0.7112 - val_loss: 0.5869 - val_accuracy: 0.6859\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5654 - accuracy: 0.7107 - val_loss: 0.5862 - val_accuracy: 0.6873\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7132 - val_loss: 0.5856 - val_accuracy: 0.6879\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7170 - val_loss: 0.5857 - val_accuracy: 0.6859\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5636 - accuracy: 0.7114 - val_loss: 0.5845 - val_accuracy: 0.6879\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5651 - accuracy: 0.7153 - val_loss: 0.5851 - val_accuracy: 0.6879\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5661 - accuracy: 0.7135 - val_loss: 0.5875 - val_accuracy: 0.6879\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5607 - accuracy: 0.7163 - val_loss: 0.5843 - val_accuracy: 0.6853\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5632 - accuracy: 0.7163 - val_loss: 0.5845 - val_accuracy: 0.6840\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5632 - accuracy: 0.7132 - val_loss: 0.5857 - val_accuracy: 0.6886\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5636 - accuracy: 0.7130 - val_loss: 0.5833 - val_accuracy: 0.6919\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5576 - accuracy: 0.7144 - val_loss: 0.5831 - val_accuracy: 0.6886\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5644 - accuracy: 0.7152 - val_loss: 0.5837 - val_accuracy: 0.6919\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.7221 - val_loss: 0.5832 - val_accuracy: 0.6846\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5573 - accuracy: 0.7171 - val_loss: 0.5826 - val_accuracy: 0.6886\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5594 - accuracy: 0.7160 - val_loss: 0.5831 - val_accuracy: 0.6899\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5575 - accuracy: 0.7199 - val_loss: 0.5821 - val_accuracy: 0.6925\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5555 - accuracy: 0.7181 - val_loss: 0.5825 - val_accuracy: 0.6873\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7148 - val_loss: 0.5815 - val_accuracy: 0.6912\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5593 - accuracy: 0.7142 - val_loss: 0.5807 - val_accuracy: 0.6912\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5574 - accuracy: 0.7231 - val_loss: 0.5818 - val_accuracy: 0.6899\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5535 - accuracy: 0.7196 - val_loss: 0.5810 - val_accuracy: 0.6859\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5567 - accuracy: 0.7175 - val_loss: 0.5818 - val_accuracy: 0.6919\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5556 - accuracy: 0.7236 - val_loss: 0.5809 - val_accuracy: 0.6932\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7183 - val_loss: 0.5806 - val_accuracy: 0.6905\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5565 - accuracy: 0.7173 - val_loss: 0.5806 - val_accuracy: 0.6925\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5577 - accuracy: 0.7185 - val_loss: 0.5810 - val_accuracy: 0.6932\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7262 - val_loss: 0.5802 - val_accuracy: 0.6932\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5559 - accuracy: 0.7208 - val_loss: 0.5798 - val_accuracy: 0.6958\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5557 - accuracy: 0.7245 - val_loss: 0.5801 - val_accuracy: 0.6945\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5557 - accuracy: 0.7168 - val_loss: 0.5798 - val_accuracy: 0.6932\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5504 - accuracy: 0.7237 - val_loss: 0.5794 - val_accuracy: 0.6925\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5555 - accuracy: 0.7252 - val_loss: 0.5805 - val_accuracy: 0.6905\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5514 - accuracy: 0.7214 - val_loss: 0.5792 - val_accuracy: 0.6938\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5519 - accuracy: 0.7219 - val_loss: 0.5814 - val_accuracy: 0.6925\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5551 - accuracy: 0.7217 - val_loss: 0.5785 - val_accuracy: 0.6965\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.7257 - val_loss: 0.5781 - val_accuracy: 0.6938\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5491 - accuracy: 0.7298 - val_loss: 0.5779 - val_accuracy: 0.6938\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5510 - accuracy: 0.7221 - val_loss: 0.5775 - val_accuracy: 0.6965\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.7288 - val_loss: 0.5781 - val_accuracy: 0.6971\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5492 - accuracy: 0.7255 - val_loss: 0.5786 - val_accuracy: 0.6965\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5531 - accuracy: 0.7216 - val_loss: 0.5778 - val_accuracy: 0.6971\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.7306 - val_loss: 0.5777 - val_accuracy: 0.6978\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7270 - val_loss: 0.5780 - val_accuracy: 0.6978\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5455 - accuracy: 0.7254 - val_loss: 0.5779 - val_accuracy: 0.6978\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5502 - accuracy: 0.7254 - val_loss: 0.5765 - val_accuracy: 0.6971\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5467 - accuracy: 0.7306 - val_loss: 0.5763 - val_accuracy: 0.7004\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5469 - accuracy: 0.7303 - val_loss: 0.5760 - val_accuracy: 0.7024\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5488 - accuracy: 0.7262 - val_loss: 0.5771 - val_accuracy: 0.7004\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5454 - accuracy: 0.7249 - val_loss: 0.5768 - val_accuracy: 0.6997\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7288 - val_loss: 0.5754 - val_accuracy: 0.7043\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7301 - val_loss: 0.5763 - val_accuracy: 0.7037\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5424 - accuracy: 0.7275 - val_loss: 0.5757 - val_accuracy: 0.7017\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5429 - accuracy: 0.7273 - val_loss: 0.5759 - val_accuracy: 0.6997\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.7309 - val_loss: 0.5758 - val_accuracy: 0.6965\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5437 - accuracy: 0.7259 - val_loss: 0.5774 - val_accuracy: 0.6984\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5446 - accuracy: 0.7273 - val_loss: 0.5781 - val_accuracy: 0.6991\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5372 - accuracy: 0.7355 - val_loss: 0.5741 - val_accuracy: 0.7037\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5416 - accuracy: 0.7267 - val_loss: 0.5737 - val_accuracy: 0.7057\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5431 - accuracy: 0.7247 - val_loss: 0.5736 - val_accuracy: 0.7037\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5387 - accuracy: 0.7336 - val_loss: 0.5751 - val_accuracy: 0.7004\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5376 - accuracy: 0.7346 - val_loss: 0.5737 - val_accuracy: 0.7024\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.7295 - val_loss: 0.5749 - val_accuracy: 0.6951\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5409 - accuracy: 0.7313 - val_loss: 0.5743 - val_accuracy: 0.7004\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5380 - accuracy: 0.7318 - val_loss: 0.5723 - val_accuracy: 0.7050\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5407 - accuracy: 0.7374 - val_loss: 0.5751 - val_accuracy: 0.6978\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5354 - accuracy: 0.7301 - val_loss: 0.5731 - val_accuracy: 0.7004\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5395 - accuracy: 0.7354 - val_loss: 0.5723 - val_accuracy: 0.7076\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5379 - accuracy: 0.7392 - val_loss: 0.5726 - val_accuracy: 0.7076\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5372 - accuracy: 0.7370 - val_loss: 0.5724 - val_accuracy: 0.7063\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7375 - val_loss: 0.5723 - val_accuracy: 0.7017\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5359 - accuracy: 0.7362 - val_loss: 0.5721 - val_accuracy: 0.6997\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5385 - accuracy: 0.7305 - val_loss: 0.5755 - val_accuracy: 0.6958\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.7355 - val_loss: 0.5728 - val_accuracy: 0.7043\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.7115\n",
            "This is for dropout with a value of 0.20\n",
            "Valid accuracy: 0.7114702463150024\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 0.7285 - accuracy: 0.5419 - val_loss: 0.6609 - val_accuracy: 0.5999\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7009 - accuracy: 0.5664 - val_loss: 0.6572 - val_accuracy: 0.6229\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5736 - val_loss: 0.6528 - val_accuracy: 0.6222\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5790 - val_loss: 0.6513 - val_accuracy: 0.6281\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5795 - val_loss: 0.6485 - val_accuracy: 0.6367\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6717 - accuracy: 0.5951 - val_loss: 0.6483 - val_accuracy: 0.6445\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5976 - val_loss: 0.6467 - val_accuracy: 0.6472\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6655 - accuracy: 0.5954 - val_loss: 0.6463 - val_accuracy: 0.6498\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.6020 - val_loss: 0.6446 - val_accuracy: 0.6472\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6059 - val_loss: 0.6444 - val_accuracy: 0.6537\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.6101 - val_loss: 0.6429 - val_accuracy: 0.6491\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6575 - accuracy: 0.6196 - val_loss: 0.6427 - val_accuracy: 0.6603\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.6105 - val_loss: 0.6403 - val_accuracy: 0.6649\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.6209 - val_loss: 0.6416 - val_accuracy: 0.6623\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.6115 - val_loss: 0.6377 - val_accuracy: 0.6636\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6179 - val_loss: 0.6377 - val_accuracy: 0.6656\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6503 - accuracy: 0.6261 - val_loss: 0.6371 - val_accuracy: 0.6669\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6455 - accuracy: 0.6284 - val_loss: 0.6357 - val_accuracy: 0.6669\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6474 - accuracy: 0.6247 - val_loss: 0.6358 - val_accuracy: 0.6616\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6502 - accuracy: 0.6242 - val_loss: 0.6333 - val_accuracy: 0.6695\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6458 - accuracy: 0.6306 - val_loss: 0.6330 - val_accuracy: 0.6662\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6468 - accuracy: 0.6242 - val_loss: 0.6333 - val_accuracy: 0.6577\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6422 - accuracy: 0.6317 - val_loss: 0.6327 - val_accuracy: 0.6616\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6386 - val_loss: 0.6300 - val_accuracy: 0.6682\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6412 - accuracy: 0.6393 - val_loss: 0.6300 - val_accuracy: 0.6643\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6416 - accuracy: 0.6367 - val_loss: 0.6304 - val_accuracy: 0.6629\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6376 - accuracy: 0.6381 - val_loss: 0.6292 - val_accuracy: 0.6636\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6355 - accuracy: 0.6408 - val_loss: 0.6279 - val_accuracy: 0.6636\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6354 - accuracy: 0.6505 - val_loss: 0.6299 - val_accuracy: 0.6590\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6349 - accuracy: 0.6406 - val_loss: 0.6305 - val_accuracy: 0.6544\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6367 - accuracy: 0.6404 - val_loss: 0.6262 - val_accuracy: 0.6649\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6310 - accuracy: 0.6493 - val_loss: 0.6259 - val_accuracy: 0.6662\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6303 - accuracy: 0.6404 - val_loss: 0.6253 - val_accuracy: 0.6669\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6336 - accuracy: 0.6487 - val_loss: 0.6276 - val_accuracy: 0.6603\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6342 - accuracy: 0.6419 - val_loss: 0.6259 - val_accuracy: 0.6597\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6300 - accuracy: 0.6547 - val_loss: 0.6239 - val_accuracy: 0.6675\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6306 - accuracy: 0.6462 - val_loss: 0.6233 - val_accuracy: 0.6656\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6296 - accuracy: 0.6460 - val_loss: 0.6227 - val_accuracy: 0.6636\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6280 - accuracy: 0.6557 - val_loss: 0.6216 - val_accuracy: 0.6715\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6247 - accuracy: 0.6510 - val_loss: 0.6226 - val_accuracy: 0.6616\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6284 - accuracy: 0.6572 - val_loss: 0.6215 - val_accuracy: 0.6643\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.6518 - val_loss: 0.6204 - val_accuracy: 0.6669\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6226 - accuracy: 0.6597 - val_loss: 0.6209 - val_accuracy: 0.6643\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6238 - accuracy: 0.6564 - val_loss: 0.6214 - val_accuracy: 0.6616\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6196 - accuracy: 0.6605 - val_loss: 0.6192 - val_accuracy: 0.6616\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6207 - accuracy: 0.6598 - val_loss: 0.6182 - val_accuracy: 0.6649\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6203 - accuracy: 0.6567 - val_loss: 0.6180 - val_accuracy: 0.6689\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6204 - accuracy: 0.6549 - val_loss: 0.6175 - val_accuracy: 0.6669\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6186 - accuracy: 0.6595 - val_loss: 0.6176 - val_accuracy: 0.6616\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6180 - accuracy: 0.6567 - val_loss: 0.6174 - val_accuracy: 0.6610\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6193 - accuracy: 0.6583 - val_loss: 0.6154 - val_accuracy: 0.6702\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6181 - accuracy: 0.6633 - val_loss: 0.6156 - val_accuracy: 0.6675\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6179 - accuracy: 0.6587 - val_loss: 0.6151 - val_accuracy: 0.6675\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6114 - accuracy: 0.6667 - val_loss: 0.6149 - val_accuracy: 0.6675\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6128 - accuracy: 0.6748 - val_loss: 0.6136 - val_accuracy: 0.6702\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6154 - accuracy: 0.6766 - val_loss: 0.6133 - val_accuracy: 0.6708\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6131 - accuracy: 0.6643 - val_loss: 0.6129 - val_accuracy: 0.6689\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.6675 - val_loss: 0.6127 - val_accuracy: 0.6715\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6088 - accuracy: 0.6749 - val_loss: 0.6131 - val_accuracy: 0.6662\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.6664 - val_loss: 0.6116 - val_accuracy: 0.6689\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6130 - accuracy: 0.6675 - val_loss: 0.6129 - val_accuracy: 0.6649\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6097 - accuracy: 0.6666 - val_loss: 0.6109 - val_accuracy: 0.6728\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6106 - accuracy: 0.6685 - val_loss: 0.6119 - val_accuracy: 0.6682\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6087 - accuracy: 0.6746 - val_loss: 0.6104 - val_accuracy: 0.6721\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6097 - accuracy: 0.6698 - val_loss: 0.6111 - val_accuracy: 0.6695\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6097 - accuracy: 0.6726 - val_loss: 0.6105 - val_accuracy: 0.6721\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6075 - accuracy: 0.6687 - val_loss: 0.6090 - val_accuracy: 0.6728\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6024 - accuracy: 0.6767 - val_loss: 0.6090 - val_accuracy: 0.6741\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6096 - accuracy: 0.6713 - val_loss: 0.6083 - val_accuracy: 0.6728\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6049 - accuracy: 0.6761 - val_loss: 0.6093 - val_accuracy: 0.6728\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6082 - accuracy: 0.6726 - val_loss: 0.6078 - val_accuracy: 0.6728\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6023 - accuracy: 0.6779 - val_loss: 0.6079 - val_accuracy: 0.6702\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6076 - accuracy: 0.6682 - val_loss: 0.6073 - val_accuracy: 0.6715\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6039 - accuracy: 0.6805 - val_loss: 0.6062 - val_accuracy: 0.6741\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.6762 - val_loss: 0.6067 - val_accuracy: 0.6728\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.6766 - val_loss: 0.6076 - val_accuracy: 0.6748\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6040 - accuracy: 0.6779 - val_loss: 0.6068 - val_accuracy: 0.6754\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6024 - accuracy: 0.6758 - val_loss: 0.6064 - val_accuracy: 0.6741\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6028 - accuracy: 0.6822 - val_loss: 0.6051 - val_accuracy: 0.6761\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6812 - val_loss: 0.6044 - val_accuracy: 0.6754\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6017 - accuracy: 0.6804 - val_loss: 0.6047 - val_accuracy: 0.6787\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6049 - accuracy: 0.6792 - val_loss: 0.6050 - val_accuracy: 0.6774\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.6777 - val_loss: 0.6043 - val_accuracy: 0.6767\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5995 - accuracy: 0.6825 - val_loss: 0.6036 - val_accuracy: 0.6781\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.6810 - val_loss: 0.6038 - val_accuracy: 0.6807\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.6818 - val_loss: 0.6054 - val_accuracy: 0.6735\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6897 - val_loss: 0.6038 - val_accuracy: 0.6787\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6876 - val_loss: 0.6048 - val_accuracy: 0.6735\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.6818 - val_loss: 0.6027 - val_accuracy: 0.6787\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6932 - val_loss: 0.6036 - val_accuracy: 0.6754\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.6861 - val_loss: 0.6021 - val_accuracy: 0.6781\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5924 - accuracy: 0.6943 - val_loss: 0.6037 - val_accuracy: 0.6761\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.6900 - val_loss: 0.6029 - val_accuracy: 0.6774\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5914 - accuracy: 0.6859 - val_loss: 0.6035 - val_accuracy: 0.6774\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5958 - accuracy: 0.6869 - val_loss: 0.6014 - val_accuracy: 0.6800\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5943 - accuracy: 0.6869 - val_loss: 0.6012 - val_accuracy: 0.6807\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5914 - accuracy: 0.6868 - val_loss: 0.6008 - val_accuracy: 0.6800\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6856 - val_loss: 0.6007 - val_accuracy: 0.6781\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5925 - accuracy: 0.6873 - val_loss: 0.6006 - val_accuracy: 0.6794\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.6915 - val_loss: 0.6026 - val_accuracy: 0.6774\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5887 - accuracy: 0.6935 - val_loss: 0.5995 - val_accuracy: 0.6820\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5882 - accuracy: 0.6946 - val_loss: 0.5992 - val_accuracy: 0.6840\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5902 - accuracy: 0.6945 - val_loss: 0.5998 - val_accuracy: 0.6807\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5904 - accuracy: 0.6894 - val_loss: 0.6029 - val_accuracy: 0.6800\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.6917 - val_loss: 0.5992 - val_accuracy: 0.6827\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 0.6942 - val_loss: 0.5994 - val_accuracy: 0.6800\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6994 - val_loss: 0.5993 - val_accuracy: 0.6807\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.7043 - val_loss: 0.5980 - val_accuracy: 0.6859\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5873 - accuracy: 0.6961 - val_loss: 0.5977 - val_accuracy: 0.6859\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.6948 - val_loss: 0.5988 - val_accuracy: 0.6846\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5875 - accuracy: 0.6928 - val_loss: 0.5976 - val_accuracy: 0.6813\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6923 - val_loss: 0.5995 - val_accuracy: 0.6859\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5881 - accuracy: 0.6933 - val_loss: 0.5984 - val_accuracy: 0.6873\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5834 - accuracy: 0.6981 - val_loss: 0.5970 - val_accuracy: 0.6846\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5834 - accuracy: 0.7015 - val_loss: 0.5983 - val_accuracy: 0.6866\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5826 - accuracy: 0.6942 - val_loss: 0.5961 - val_accuracy: 0.6879\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.6933 - val_loss: 0.5972 - val_accuracy: 0.6859\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6904 - val_loss: 0.5959 - val_accuracy: 0.6879\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5816 - accuracy: 0.6992 - val_loss: 0.5955 - val_accuracy: 0.6886\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5822 - accuracy: 0.7025 - val_loss: 0.5955 - val_accuracy: 0.6853\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6912 - val_loss: 0.5965 - val_accuracy: 0.6859\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5800 - accuracy: 0.7022 - val_loss: 0.5970 - val_accuracy: 0.6873\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.6983 - val_loss: 0.5981 - val_accuracy: 0.6879\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5814 - accuracy: 0.7011 - val_loss: 0.5958 - val_accuracy: 0.6886\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.6958 - val_loss: 0.5953 - val_accuracy: 0.6905\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5780 - accuracy: 0.7001 - val_loss: 0.5957 - val_accuracy: 0.6919\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5788 - accuracy: 0.6999 - val_loss: 0.5954 - val_accuracy: 0.6912\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5808 - accuracy: 0.7020 - val_loss: 0.5954 - val_accuracy: 0.6932\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.7040 - val_loss: 0.5974 - val_accuracy: 0.6859\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5833 - accuracy: 0.6958 - val_loss: 0.5934 - val_accuracy: 0.6938\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5810 - accuracy: 0.7006 - val_loss: 0.5936 - val_accuracy: 0.6879\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5820 - accuracy: 0.7034 - val_loss: 0.5936 - val_accuracy: 0.6905\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5744 - accuracy: 0.7096 - val_loss: 0.5958 - val_accuracy: 0.6899\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5811 - accuracy: 0.6965 - val_loss: 0.5927 - val_accuracy: 0.6892\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5783 - accuracy: 0.7040 - val_loss: 0.5922 - val_accuracy: 0.6958\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5737 - accuracy: 0.7020 - val_loss: 0.5937 - val_accuracy: 0.6879\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5793 - accuracy: 0.7034 - val_loss: 0.5939 - val_accuracy: 0.6859\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5753 - accuracy: 0.7096 - val_loss: 0.5935 - val_accuracy: 0.6932\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5752 - accuracy: 0.7037 - val_loss: 0.5946 - val_accuracy: 0.6899\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.7107 - val_loss: 0.5935 - val_accuracy: 0.6945\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5737 - accuracy: 0.7080 - val_loss: 0.5927 - val_accuracy: 0.6958\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5744 - accuracy: 0.7052 - val_loss: 0.5921 - val_accuracy: 0.6925\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5756 - accuracy: 0.7014 - val_loss: 0.5923 - val_accuracy: 0.6912\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5745 - accuracy: 0.7037 - val_loss: 0.5934 - val_accuracy: 0.6899\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5770 - accuracy: 0.7038 - val_loss: 0.5940 - val_accuracy: 0.6912\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5693 - accuracy: 0.7142 - val_loss: 0.5914 - val_accuracy: 0.6925\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5748 - accuracy: 0.7052 - val_loss: 0.5918 - val_accuracy: 0.6938\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5758 - accuracy: 0.7093 - val_loss: 0.5905 - val_accuracy: 0.6905\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.7052 - val_loss: 0.5912 - val_accuracy: 0.6925\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5742 - accuracy: 0.7029 - val_loss: 0.5920 - val_accuracy: 0.6932\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.7047 - val_loss: 0.5914 - val_accuracy: 0.6932\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5730 - accuracy: 0.7068 - val_loss: 0.5899 - val_accuracy: 0.6945\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.7121 - val_loss: 0.5912 - val_accuracy: 0.6938\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5680 - accuracy: 0.7132 - val_loss: 0.5886 - val_accuracy: 0.6932\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7145 - val_loss: 0.5886 - val_accuracy: 0.6965\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5733 - accuracy: 0.7029 - val_loss: 0.5901 - val_accuracy: 0.6951\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5664 - accuracy: 0.7173 - val_loss: 0.5901 - val_accuracy: 0.6938\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5678 - accuracy: 0.7048 - val_loss: 0.5897 - val_accuracy: 0.6965\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5729 - accuracy: 0.7093 - val_loss: 0.5880 - val_accuracy: 0.6938\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5682 - accuracy: 0.7142 - val_loss: 0.5898 - val_accuracy: 0.6958\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7176 - val_loss: 0.5902 - val_accuracy: 0.6958\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5708 - accuracy: 0.7091 - val_loss: 0.5873 - val_accuracy: 0.7030\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5708 - accuracy: 0.7127 - val_loss: 0.5887 - val_accuracy: 0.6965\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5631 - accuracy: 0.7140 - val_loss: 0.5869 - val_accuracy: 0.6978\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5691 - accuracy: 0.7153 - val_loss: 0.5883 - val_accuracy: 0.6958\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5703 - accuracy: 0.7104 - val_loss: 0.5878 - val_accuracy: 0.6958\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5692 - accuracy: 0.7111 - val_loss: 0.5874 - val_accuracy: 0.6978\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7058 - val_loss: 0.5877 - val_accuracy: 0.6978\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5673 - accuracy: 0.7145 - val_loss: 0.5898 - val_accuracy: 0.6945\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5658 - accuracy: 0.7144 - val_loss: 0.5858 - val_accuracy: 0.6991\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5682 - accuracy: 0.7170 - val_loss: 0.5853 - val_accuracy: 0.7004\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5637 - accuracy: 0.7196 - val_loss: 0.5864 - val_accuracy: 0.7004\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5655 - accuracy: 0.7093 - val_loss: 0.5870 - val_accuracy: 0.6958\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7075 - val_loss: 0.5860 - val_accuracy: 0.6984\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5636 - accuracy: 0.7132 - val_loss: 0.5878 - val_accuracy: 0.6958\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5621 - accuracy: 0.7114 - val_loss: 0.5863 - val_accuracy: 0.6971\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5615 - accuracy: 0.7171 - val_loss: 0.5852 - val_accuracy: 0.7017\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.5593 - accuracy: 0.7176 - val_loss: 0.5897 - val_accuracy: 0.6919\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5599 - accuracy: 0.7213 - val_loss: 0.5886 - val_accuracy: 0.6945\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5610 - accuracy: 0.7157 - val_loss: 0.5840 - val_accuracy: 0.7011\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5620 - accuracy: 0.7145 - val_loss: 0.5845 - val_accuracy: 0.7011\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5677 - accuracy: 0.7114 - val_loss: 0.5841 - val_accuracy: 0.7017\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7165 - val_loss: 0.5844 - val_accuracy: 0.7024\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5608 - accuracy: 0.7227 - val_loss: 0.5841 - val_accuracy: 0.7004\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5599 - accuracy: 0.7216 - val_loss: 0.5847 - val_accuracy: 0.7024\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.7130 - val_loss: 0.5834 - val_accuracy: 0.7004\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5590 - accuracy: 0.7194 - val_loss: 0.5833 - val_accuracy: 0.7024\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5599 - accuracy: 0.7186 - val_loss: 0.5817 - val_accuracy: 0.7063\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7194 - val_loss: 0.5818 - val_accuracy: 0.7011\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5569 - accuracy: 0.7213 - val_loss: 0.5829 - val_accuracy: 0.6971\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5574 - accuracy: 0.7201 - val_loss: 0.5818 - val_accuracy: 0.7063\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5575 - accuracy: 0.7234 - val_loss: 0.5822 - val_accuracy: 0.6997\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.7199 - val_loss: 0.5826 - val_accuracy: 0.7011\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5633 - accuracy: 0.7162 - val_loss: 0.5819 - val_accuracy: 0.7017\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5608 - accuracy: 0.7206 - val_loss: 0.5823 - val_accuracy: 0.7024\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5588 - accuracy: 0.7157 - val_loss: 0.5821 - val_accuracy: 0.7037\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5566 - accuracy: 0.7259 - val_loss: 0.5824 - val_accuracy: 0.7004\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5523 - accuracy: 0.7247 - val_loss: 0.5819 - val_accuracy: 0.7024\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5534 - accuracy: 0.7273 - val_loss: 0.5841 - val_accuracy: 0.6978\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5619 - accuracy: 0.7145 - val_loss: 0.5821 - val_accuracy: 0.7030\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7087\n",
            "This is for dropout with a value of 0.30\n",
            "Valid accuracy: 0.7087110877037048\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.7651 - accuracy: 0.5302 - val_loss: 0.6772 - val_accuracy: 0.5887\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7261 - accuracy: 0.5338 - val_loss: 0.6693 - val_accuracy: 0.6091\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7069 - accuracy: 0.5590 - val_loss: 0.6649 - val_accuracy: 0.6137\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7007 - accuracy: 0.5506 - val_loss: 0.6623 - val_accuracy: 0.6156\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.5728 - val_loss: 0.6594 - val_accuracy: 0.6163\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6835 - accuracy: 0.5790 - val_loss: 0.6579 - val_accuracy: 0.6248\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5779 - val_loss: 0.6568 - val_accuracy: 0.6261\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6755 - accuracy: 0.5823 - val_loss: 0.6550 - val_accuracy: 0.6386\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6730 - accuracy: 0.5864 - val_loss: 0.6536 - val_accuracy: 0.6314\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6721 - accuracy: 0.5866 - val_loss: 0.6535 - val_accuracy: 0.6498\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6691 - accuracy: 0.5956 - val_loss: 0.6530 - val_accuracy: 0.6498\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6690 - accuracy: 0.5926 - val_loss: 0.6508 - val_accuracy: 0.6393\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6664 - accuracy: 0.6009 - val_loss: 0.6498 - val_accuracy: 0.6459\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.5966 - val_loss: 0.6486 - val_accuracy: 0.6491\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6114 - val_loss: 0.6477 - val_accuracy: 0.6570\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6613 - accuracy: 0.6036 - val_loss: 0.6475 - val_accuracy: 0.6577\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6639 - accuracy: 0.6009 - val_loss: 0.6459 - val_accuracy: 0.6583\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6546 - accuracy: 0.6176 - val_loss: 0.6446 - val_accuracy: 0.6583\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6601 - accuracy: 0.6074 - val_loss: 0.6444 - val_accuracy: 0.6616\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6546 - accuracy: 0.6229 - val_loss: 0.6431 - val_accuracy: 0.6603\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.6181 - val_loss: 0.6438 - val_accuracy: 0.6498\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6546 - accuracy: 0.6148 - val_loss: 0.6412 - val_accuracy: 0.6629\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6533 - accuracy: 0.6266 - val_loss: 0.6413 - val_accuracy: 0.6603\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6480 - accuracy: 0.6294 - val_loss: 0.6395 - val_accuracy: 0.6656\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6492 - accuracy: 0.6289 - val_loss: 0.6387 - val_accuracy: 0.6715\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6473 - accuracy: 0.6296 - val_loss: 0.6387 - val_accuracy: 0.6570\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6502 - accuracy: 0.6266 - val_loss: 0.6371 - val_accuracy: 0.6708\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6519 - accuracy: 0.6235 - val_loss: 0.6371 - val_accuracy: 0.6623\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6486 - accuracy: 0.6275 - val_loss: 0.6377 - val_accuracy: 0.6524\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6461 - accuracy: 0.6258 - val_loss: 0.6357 - val_accuracy: 0.6636\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.6257 - val_loss: 0.6354 - val_accuracy: 0.6557\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6265 - val_loss: 0.6353 - val_accuracy: 0.6511\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6443 - accuracy: 0.6324 - val_loss: 0.6346 - val_accuracy: 0.6570\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6444 - accuracy: 0.6291 - val_loss: 0.6332 - val_accuracy: 0.6629\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.6261 - val_loss: 0.6333 - val_accuracy: 0.6603\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6426 - accuracy: 0.6350 - val_loss: 0.6324 - val_accuracy: 0.6583\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6415 - accuracy: 0.6355 - val_loss: 0.6315 - val_accuracy: 0.6643\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6424 - accuracy: 0.6355 - val_loss: 0.6310 - val_accuracy: 0.6629\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6401 - accuracy: 0.6375 - val_loss: 0.6303 - val_accuracy: 0.6636\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6360 - val_loss: 0.6303 - val_accuracy: 0.6616\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6381 - val_loss: 0.6291 - val_accuracy: 0.6643\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6360 - accuracy: 0.6477 - val_loss: 0.6285 - val_accuracy: 0.6689\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6363 - accuracy: 0.6462 - val_loss: 0.6280 - val_accuracy: 0.6610\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6372 - accuracy: 0.6470 - val_loss: 0.6289 - val_accuracy: 0.6610\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6369 - accuracy: 0.6418 - val_loss: 0.6267 - val_accuracy: 0.6643\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6347 - accuracy: 0.6475 - val_loss: 0.6255 - val_accuracy: 0.6669\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6306 - accuracy: 0.6483 - val_loss: 0.6254 - val_accuracy: 0.6656\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6345 - accuracy: 0.6427 - val_loss: 0.6248 - val_accuracy: 0.6656\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6295 - accuracy: 0.6510 - val_loss: 0.6240 - val_accuracy: 0.6662\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6332 - accuracy: 0.6510 - val_loss: 0.6242 - val_accuracy: 0.6636\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6332 - accuracy: 0.6399 - val_loss: 0.6247 - val_accuracy: 0.6669\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6316 - accuracy: 0.6503 - val_loss: 0.6241 - val_accuracy: 0.6675\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 0.6516 - val_loss: 0.6240 - val_accuracy: 0.6675\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6283 - accuracy: 0.6457 - val_loss: 0.6233 - val_accuracy: 0.6682\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.6511 - val_loss: 0.6243 - val_accuracy: 0.6616\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6282 - accuracy: 0.6475 - val_loss: 0.6225 - val_accuracy: 0.6695\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6315 - accuracy: 0.6452 - val_loss: 0.6223 - val_accuracy: 0.6708\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.6524 - val_loss: 0.6211 - val_accuracy: 0.6695\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6258 - accuracy: 0.6587 - val_loss: 0.6214 - val_accuracy: 0.6669\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6254 - accuracy: 0.6605 - val_loss: 0.6199 - val_accuracy: 0.6662\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6240 - accuracy: 0.6572 - val_loss: 0.6201 - val_accuracy: 0.6669\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6256 - accuracy: 0.6514 - val_loss: 0.6208 - val_accuracy: 0.6695\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.6552 - val_loss: 0.6202 - val_accuracy: 0.6695\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6216 - accuracy: 0.6597 - val_loss: 0.6183 - val_accuracy: 0.6682\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6216 - accuracy: 0.6533 - val_loss: 0.6184 - val_accuracy: 0.6669\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6233 - accuracy: 0.6567 - val_loss: 0.6177 - val_accuracy: 0.6702\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6238 - accuracy: 0.6521 - val_loss: 0.6174 - val_accuracy: 0.6682\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6177 - accuracy: 0.6669 - val_loss: 0.6171 - val_accuracy: 0.6689\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6242 - accuracy: 0.6567 - val_loss: 0.6170 - val_accuracy: 0.6721\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6184 - accuracy: 0.6557 - val_loss: 0.6161 - val_accuracy: 0.6741\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6206 - accuracy: 0.6510 - val_loss: 0.6169 - val_accuracy: 0.6689\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6195 - accuracy: 0.6625 - val_loss: 0.6151 - val_accuracy: 0.6728\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6175 - accuracy: 0.6582 - val_loss: 0.6155 - val_accuracy: 0.6728\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6180 - accuracy: 0.6646 - val_loss: 0.6139 - val_accuracy: 0.6748\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6148 - accuracy: 0.6654 - val_loss: 0.6138 - val_accuracy: 0.6767\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6168 - accuracy: 0.6698 - val_loss: 0.6139 - val_accuracy: 0.6741\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6178 - accuracy: 0.6631 - val_loss: 0.6150 - val_accuracy: 0.6682\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6162 - accuracy: 0.6661 - val_loss: 0.6144 - val_accuracy: 0.6695\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6131 - accuracy: 0.6679 - val_loss: 0.6127 - val_accuracy: 0.6767\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6141 - accuracy: 0.6662 - val_loss: 0.6117 - val_accuracy: 0.6774\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6150 - accuracy: 0.6666 - val_loss: 0.6119 - val_accuracy: 0.6761\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6135 - accuracy: 0.6720 - val_loss: 0.6118 - val_accuracy: 0.6761\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6099 - accuracy: 0.6754 - val_loss: 0.6107 - val_accuracy: 0.6774\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6131 - accuracy: 0.6726 - val_loss: 0.6122 - val_accuracy: 0.6728\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6130 - accuracy: 0.6670 - val_loss: 0.6111 - val_accuracy: 0.6728\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6081 - accuracy: 0.6708 - val_loss: 0.6102 - val_accuracy: 0.6767\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6133 - accuracy: 0.6692 - val_loss: 0.6107 - val_accuracy: 0.6715\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6125 - accuracy: 0.6721 - val_loss: 0.6103 - val_accuracy: 0.6728\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6102 - accuracy: 0.6700 - val_loss: 0.6108 - val_accuracy: 0.6715\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.6743 - val_loss: 0.6092 - val_accuracy: 0.6748\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6124 - accuracy: 0.6677 - val_loss: 0.6086 - val_accuracy: 0.6761\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6097 - accuracy: 0.6759 - val_loss: 0.6083 - val_accuracy: 0.6767\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6122 - accuracy: 0.6682 - val_loss: 0.6077 - val_accuracy: 0.6787\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6061 - accuracy: 0.6731 - val_loss: 0.6075 - val_accuracy: 0.6761\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6041 - accuracy: 0.6721 - val_loss: 0.6062 - val_accuracy: 0.6833\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6078 - accuracy: 0.6807 - val_loss: 0.6072 - val_accuracy: 0.6721\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6066 - accuracy: 0.6772 - val_loss: 0.6065 - val_accuracy: 0.6761\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6067 - accuracy: 0.6779 - val_loss: 0.6059 - val_accuracy: 0.6794\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6069 - accuracy: 0.6799 - val_loss: 0.6062 - val_accuracy: 0.6827\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6072 - accuracy: 0.6758 - val_loss: 0.6060 - val_accuracy: 0.6800\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6047 - accuracy: 0.6802 - val_loss: 0.6065 - val_accuracy: 0.6794\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6050 - accuracy: 0.6789 - val_loss: 0.6064 - val_accuracy: 0.6781\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6049 - accuracy: 0.6758 - val_loss: 0.6051 - val_accuracy: 0.6787\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6013 - accuracy: 0.6848 - val_loss: 0.6048 - val_accuracy: 0.6781\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6066 - accuracy: 0.6726 - val_loss: 0.6050 - val_accuracy: 0.6781\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6034 - accuracy: 0.6818 - val_loss: 0.6046 - val_accuracy: 0.6813\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.6854 - val_loss: 0.6047 - val_accuracy: 0.6794\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5986 - accuracy: 0.6851 - val_loss: 0.6053 - val_accuracy: 0.6774\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6000 - accuracy: 0.6833 - val_loss: 0.6033 - val_accuracy: 0.6820\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6005 - accuracy: 0.6863 - val_loss: 0.6032 - val_accuracy: 0.6813\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6003 - accuracy: 0.6817 - val_loss: 0.6034 - val_accuracy: 0.6827\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6023 - accuracy: 0.6820 - val_loss: 0.6033 - val_accuracy: 0.6840\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6049 - accuracy: 0.6813 - val_loss: 0.6041 - val_accuracy: 0.6820\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5977 - accuracy: 0.6868 - val_loss: 0.6032 - val_accuracy: 0.6840\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5964 - accuracy: 0.6869 - val_loss: 0.6016 - val_accuracy: 0.6807\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.6864 - val_loss: 0.6020 - val_accuracy: 0.6840\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5954 - accuracy: 0.6846 - val_loss: 0.6012 - val_accuracy: 0.6833\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.6817 - val_loss: 0.6013 - val_accuracy: 0.6833\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.6858 - val_loss: 0.6011 - val_accuracy: 0.6813\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.6877 - val_loss: 0.6012 - val_accuracy: 0.6827\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.6800 - val_loss: 0.6016 - val_accuracy: 0.6820\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.6877 - val_loss: 0.6004 - val_accuracy: 0.6879\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5988 - accuracy: 0.6840 - val_loss: 0.6004 - val_accuracy: 0.6873\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6886 - val_loss: 0.6015 - val_accuracy: 0.6866\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5998 - accuracy: 0.6804 - val_loss: 0.6006 - val_accuracy: 0.6859\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5929 - accuracy: 0.6920 - val_loss: 0.6002 - val_accuracy: 0.6873\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5954 - accuracy: 0.6889 - val_loss: 0.6006 - val_accuracy: 0.6853\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5965 - accuracy: 0.6859 - val_loss: 0.5990 - val_accuracy: 0.6840\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5946 - accuracy: 0.6881 - val_loss: 0.5993 - val_accuracy: 0.6886\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.6915 - val_loss: 0.5997 - val_accuracy: 0.6866\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5901 - accuracy: 0.6945 - val_loss: 0.5984 - val_accuracy: 0.6859\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.6871 - val_loss: 0.5992 - val_accuracy: 0.6866\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5942 - accuracy: 0.6861 - val_loss: 0.5985 - val_accuracy: 0.6866\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5916 - accuracy: 0.6902 - val_loss: 0.5989 - val_accuracy: 0.6853\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5920 - accuracy: 0.6859 - val_loss: 0.5981 - val_accuracy: 0.6866\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5950 - accuracy: 0.6871 - val_loss: 0.5974 - val_accuracy: 0.6912\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5899 - accuracy: 0.6871 - val_loss: 0.5966 - val_accuracy: 0.6840\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5901 - accuracy: 0.6940 - val_loss: 0.5963 - val_accuracy: 0.6859\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6945 - val_loss: 0.5974 - val_accuracy: 0.6873\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5916 - accuracy: 0.6955 - val_loss: 0.5976 - val_accuracy: 0.6925\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5911 - accuracy: 0.6881 - val_loss: 0.5959 - val_accuracy: 0.6886\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5881 - accuracy: 0.6933 - val_loss: 0.5970 - val_accuracy: 0.6912\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5904 - accuracy: 0.6942 - val_loss: 0.5946 - val_accuracy: 0.6899\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.6861 - val_loss: 0.5959 - val_accuracy: 0.6886\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5911 - accuracy: 0.6917 - val_loss: 0.5950 - val_accuracy: 0.6892\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5887 - accuracy: 0.6900 - val_loss: 0.5952 - val_accuracy: 0.6886\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6966 - val_loss: 0.5938 - val_accuracy: 0.6899\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5890 - accuracy: 0.6896 - val_loss: 0.5942 - val_accuracy: 0.6879\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5830 - accuracy: 0.6953 - val_loss: 0.5949 - val_accuracy: 0.6925\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.6963 - val_loss: 0.5958 - val_accuracy: 0.6879\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5860 - accuracy: 0.6979 - val_loss: 0.5937 - val_accuracy: 0.6899\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5880 - accuracy: 0.6922 - val_loss: 0.5938 - val_accuracy: 0.6951\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 0.6969 - val_loss: 0.5939 - val_accuracy: 0.6899\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5842 - accuracy: 0.6988 - val_loss: 0.5934 - val_accuracy: 0.6899\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5877 - accuracy: 0.6988 - val_loss: 0.5927 - val_accuracy: 0.6919\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5832 - accuracy: 0.6988 - val_loss: 0.5935 - val_accuracy: 0.6905\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6965 - val_loss: 0.5917 - val_accuracy: 0.6925\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6974 - val_loss: 0.5914 - val_accuracy: 0.6951\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7043 - val_loss: 0.5925 - val_accuracy: 0.6912\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5782 - accuracy: 0.6968 - val_loss: 0.5907 - val_accuracy: 0.6925\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5810 - accuracy: 0.6960 - val_loss: 0.5928 - val_accuracy: 0.6899\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5813 - accuracy: 0.7038 - val_loss: 0.5903 - val_accuracy: 0.6945\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5798 - accuracy: 0.6974 - val_loss: 0.5897 - val_accuracy: 0.6925\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5831 - accuracy: 0.6976 - val_loss: 0.5903 - val_accuracy: 0.6965\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5831 - accuracy: 0.7011 - val_loss: 0.5904 - val_accuracy: 0.6951\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5772 - accuracy: 0.7038 - val_loss: 0.5924 - val_accuracy: 0.6932\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5761 - accuracy: 0.7084 - val_loss: 0.5901 - val_accuracy: 0.6945\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5801 - accuracy: 0.6997 - val_loss: 0.5899 - val_accuracy: 0.6965\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5772 - accuracy: 0.7042 - val_loss: 0.5894 - val_accuracy: 0.6958\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5792 - accuracy: 0.6981 - val_loss: 0.5922 - val_accuracy: 0.6905\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5806 - accuracy: 0.7073 - val_loss: 0.5901 - val_accuracy: 0.6958\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5816 - accuracy: 0.6973 - val_loss: 0.5885 - val_accuracy: 0.6938\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5802 - accuracy: 0.7040 - val_loss: 0.5892 - val_accuracy: 0.6951\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5787 - accuracy: 0.6992 - val_loss: 0.5889 - val_accuracy: 0.6932\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5765 - accuracy: 0.7034 - val_loss: 0.5885 - val_accuracy: 0.6965\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.7080 - val_loss: 0.5897 - val_accuracy: 0.6951\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.7037 - val_loss: 0.5873 - val_accuracy: 0.6984\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5747 - accuracy: 0.7081 - val_loss: 0.5891 - val_accuracy: 0.6945\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5791 - accuracy: 0.7001 - val_loss: 0.5876 - val_accuracy: 0.6958\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7030 - val_loss: 0.5880 - val_accuracy: 0.6925\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5723 - accuracy: 0.7094 - val_loss: 0.5881 - val_accuracy: 0.6951\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5779 - accuracy: 0.7027 - val_loss: 0.5869 - val_accuracy: 0.6965\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5744 - accuracy: 0.7065 - val_loss: 0.5863 - val_accuracy: 0.6984\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5745 - accuracy: 0.7058 - val_loss: 0.5854 - val_accuracy: 0.7004\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5730 - accuracy: 0.7032 - val_loss: 0.5862 - val_accuracy: 0.6978\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5737 - accuracy: 0.7114 - val_loss: 0.5871 - val_accuracy: 0.6951\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5731 - accuracy: 0.7080 - val_loss: 0.5866 - val_accuracy: 0.6965\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5725 - accuracy: 0.7080 - val_loss: 0.5866 - val_accuracy: 0.6958\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5711 - accuracy: 0.7060 - val_loss: 0.5851 - val_accuracy: 0.6984\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5724 - accuracy: 0.7075 - val_loss: 0.5854 - val_accuracy: 0.6978\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5755 - accuracy: 0.7038 - val_loss: 0.5850 - val_accuracy: 0.6997\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5696 - accuracy: 0.7106 - val_loss: 0.5837 - val_accuracy: 0.6991\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5710 - accuracy: 0.7109 - val_loss: 0.5852 - val_accuracy: 0.6971\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5681 - accuracy: 0.7075 - val_loss: 0.5831 - val_accuracy: 0.6997\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5726 - accuracy: 0.7088 - val_loss: 0.5844 - val_accuracy: 0.6984\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5667 - accuracy: 0.7114 - val_loss: 0.5836 - val_accuracy: 0.7011\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5683 - accuracy: 0.7127 - val_loss: 0.5827 - val_accuracy: 0.7043\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5720 - accuracy: 0.7081 - val_loss: 0.5836 - val_accuracy: 0.7011\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5707 - accuracy: 0.7080 - val_loss: 0.5860 - val_accuracy: 0.6958\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5693 - accuracy: 0.7112 - val_loss: 0.5829 - val_accuracy: 0.6991\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7063\n",
            "This is for dropout with a value of 0.40\n",
            "Valid accuracy: 0.7063460946083069\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 0.8112 - accuracy: 0.5141 - val_loss: 0.6765 - val_accuracy: 0.5913\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7394 - accuracy: 0.5360 - val_loss: 0.6724 - val_accuracy: 0.5900\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7228 - accuracy: 0.5401 - val_loss: 0.6684 - val_accuracy: 0.6025\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7015 - accuracy: 0.5554 - val_loss: 0.6668 - val_accuracy: 0.6018\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7066 - accuracy: 0.5466 - val_loss: 0.6658 - val_accuracy: 0.6091\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6980 - accuracy: 0.5637 - val_loss: 0.6643 - val_accuracy: 0.6058\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5680 - val_loss: 0.6638 - val_accuracy: 0.6110\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6844 - accuracy: 0.5726 - val_loss: 0.6637 - val_accuracy: 0.6156\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6809 - accuracy: 0.5690 - val_loss: 0.6620 - val_accuracy: 0.6176\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5823 - val_loss: 0.6606 - val_accuracy: 0.6216\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6774 - accuracy: 0.5797 - val_loss: 0.6602 - val_accuracy: 0.6268\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.5767 - val_loss: 0.6591 - val_accuracy: 0.6235\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.5953 - val_loss: 0.6579 - val_accuracy: 0.6268\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6754 - accuracy: 0.5869 - val_loss: 0.6572 - val_accuracy: 0.6268\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6692 - accuracy: 0.5951 - val_loss: 0.6555 - val_accuracy: 0.6229\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.5921 - val_loss: 0.6562 - val_accuracy: 0.6307\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.5880 - val_loss: 0.6553 - val_accuracy: 0.6314\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5976 - val_loss: 0.6540 - val_accuracy: 0.6314\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.6023 - val_loss: 0.6533 - val_accuracy: 0.6334\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5992 - val_loss: 0.6527 - val_accuracy: 0.6367\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.6020 - val_loss: 0.6530 - val_accuracy: 0.6367\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6639 - accuracy: 0.6002 - val_loss: 0.6526 - val_accuracy: 0.6340\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.6058 - val_loss: 0.6508 - val_accuracy: 0.6353\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6636 - accuracy: 0.6020 - val_loss: 0.6503 - val_accuracy: 0.6413\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6570 - accuracy: 0.6087 - val_loss: 0.6490 - val_accuracy: 0.6393\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.6109 - val_loss: 0.6487 - val_accuracy: 0.6406\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6592 - accuracy: 0.6114 - val_loss: 0.6481 - val_accuracy: 0.6393\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6557 - accuracy: 0.6156 - val_loss: 0.6474 - val_accuracy: 0.6399\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6537 - accuracy: 0.6110 - val_loss: 0.6468 - val_accuracy: 0.6426\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6540 - accuracy: 0.6166 - val_loss: 0.6458 - val_accuracy: 0.6472\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6538 - accuracy: 0.6217 - val_loss: 0.6448 - val_accuracy: 0.6459\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6556 - accuracy: 0.6105 - val_loss: 0.6452 - val_accuracy: 0.6472\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6537 - accuracy: 0.6191 - val_loss: 0.6445 - val_accuracy: 0.6478\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6500 - accuracy: 0.6197 - val_loss: 0.6430 - val_accuracy: 0.6491\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6532 - accuracy: 0.6197 - val_loss: 0.6434 - val_accuracy: 0.6452\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6578 - accuracy: 0.6135 - val_loss: 0.6426 - val_accuracy: 0.6518\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6508 - accuracy: 0.6257 - val_loss: 0.6425 - val_accuracy: 0.6452\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6486 - accuracy: 0.6212 - val_loss: 0.6418 - val_accuracy: 0.6432\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.6291 - val_loss: 0.6400 - val_accuracy: 0.6498\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.6176 - val_loss: 0.6397 - val_accuracy: 0.6478\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.6289 - val_loss: 0.6389 - val_accuracy: 0.6518\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6480 - accuracy: 0.6199 - val_loss: 0.6379 - val_accuracy: 0.6518\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6459 - accuracy: 0.6224 - val_loss: 0.6379 - val_accuracy: 0.6505\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6483 - accuracy: 0.6199 - val_loss: 0.6381 - val_accuracy: 0.6537\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6480 - accuracy: 0.6284 - val_loss: 0.6373 - val_accuracy: 0.6551\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6448 - accuracy: 0.6311 - val_loss: 0.6368 - val_accuracy: 0.6544\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6473 - accuracy: 0.6235 - val_loss: 0.6369 - val_accuracy: 0.6544\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.6326 - val_loss: 0.6357 - val_accuracy: 0.6564\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6411 - accuracy: 0.6388 - val_loss: 0.6351 - val_accuracy: 0.6564\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6434 - accuracy: 0.6265 - val_loss: 0.6350 - val_accuracy: 0.6557\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6437 - accuracy: 0.6270 - val_loss: 0.6349 - val_accuracy: 0.6498\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.6390 - val_loss: 0.6343 - val_accuracy: 0.6459\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.6409 - val_loss: 0.6329 - val_accuracy: 0.6564\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.6353 - val_loss: 0.6324 - val_accuracy: 0.6570\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.6399 - val_loss: 0.6330 - val_accuracy: 0.6491\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6380 - accuracy: 0.6309 - val_loss: 0.6325 - val_accuracy: 0.6485\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6388 - accuracy: 0.6436 - val_loss: 0.6307 - val_accuracy: 0.6564\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6343 - accuracy: 0.6457 - val_loss: 0.6306 - val_accuracy: 0.6544\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6361 - accuracy: 0.6399 - val_loss: 0.6303 - val_accuracy: 0.6485\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6362 - accuracy: 0.6411 - val_loss: 0.6291 - val_accuracy: 0.6537\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6361 - accuracy: 0.6450 - val_loss: 0.6289 - val_accuracy: 0.6531\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6346 - accuracy: 0.6483 - val_loss: 0.6291 - val_accuracy: 0.6491\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6329 - accuracy: 0.6467 - val_loss: 0.6284 - val_accuracy: 0.6485\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6319 - accuracy: 0.6510 - val_loss: 0.6272 - val_accuracy: 0.6518\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6350 - accuracy: 0.6416 - val_loss: 0.6274 - val_accuracy: 0.6518\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6326 - accuracy: 0.6508 - val_loss: 0.6277 - val_accuracy: 0.6505\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6314 - accuracy: 0.6483 - val_loss: 0.6280 - val_accuracy: 0.6452\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6342 - accuracy: 0.6390 - val_loss: 0.6263 - val_accuracy: 0.6557\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6304 - accuracy: 0.6498 - val_loss: 0.6266 - val_accuracy: 0.6498\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6306 - accuracy: 0.6565 - val_loss: 0.6256 - val_accuracy: 0.6544\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6299 - accuracy: 0.6508 - val_loss: 0.6253 - val_accuracy: 0.6518\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6295 - accuracy: 0.6488 - val_loss: 0.6242 - val_accuracy: 0.6537\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6300 - accuracy: 0.6465 - val_loss: 0.6234 - val_accuracy: 0.6570\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6266 - accuracy: 0.6488 - val_loss: 0.6236 - val_accuracy: 0.6531\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6274 - accuracy: 0.6516 - val_loss: 0.6234 - val_accuracy: 0.6518\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6299 - accuracy: 0.6521 - val_loss: 0.6241 - val_accuracy: 0.6498\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6269 - accuracy: 0.6528 - val_loss: 0.6227 - val_accuracy: 0.6511\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6274 - accuracy: 0.6491 - val_loss: 0.6225 - val_accuracy: 0.6498\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6233 - accuracy: 0.6560 - val_loss: 0.6211 - val_accuracy: 0.6511\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6250 - accuracy: 0.6549 - val_loss: 0.6213 - val_accuracy: 0.6518\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6288 - accuracy: 0.6533 - val_loss: 0.6210 - val_accuracy: 0.6511\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6237 - accuracy: 0.6556 - val_loss: 0.6201 - val_accuracy: 0.6537\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6231 - accuracy: 0.6560 - val_loss: 0.6194 - val_accuracy: 0.6524\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6258 - accuracy: 0.6593 - val_loss: 0.6189 - val_accuracy: 0.6537\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6241 - accuracy: 0.6560 - val_loss: 0.6197 - val_accuracy: 0.6603\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.6603 - val_loss: 0.6185 - val_accuracy: 0.6537\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.6644 - val_loss: 0.6184 - val_accuracy: 0.6544\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6228 - accuracy: 0.6629 - val_loss: 0.6182 - val_accuracy: 0.6531\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6207 - accuracy: 0.6643 - val_loss: 0.6171 - val_accuracy: 0.6544\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6185 - accuracy: 0.6677 - val_loss: 0.6176 - val_accuracy: 0.6551\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6223 - accuracy: 0.6511 - val_loss: 0.6166 - val_accuracy: 0.6551\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6230 - accuracy: 0.6560 - val_loss: 0.6160 - val_accuracy: 0.6603\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6221 - accuracy: 0.6638 - val_loss: 0.6161 - val_accuracy: 0.6577\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6204 - accuracy: 0.6648 - val_loss: 0.6156 - val_accuracy: 0.6603\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6189 - accuracy: 0.6567 - val_loss: 0.6150 - val_accuracy: 0.6597\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6646 - val_loss: 0.6153 - val_accuracy: 0.6623\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6186 - accuracy: 0.6636 - val_loss: 0.6140 - val_accuracy: 0.6597\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6174 - accuracy: 0.6605 - val_loss: 0.6139 - val_accuracy: 0.6616\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6151 - accuracy: 0.6623 - val_loss: 0.6136 - val_accuracy: 0.6616\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6180 - accuracy: 0.6620 - val_loss: 0.6138 - val_accuracy: 0.6629\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6141 - accuracy: 0.6656 - val_loss: 0.6133 - val_accuracy: 0.6610\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6664 - val_loss: 0.6125 - val_accuracy: 0.6610\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6143 - accuracy: 0.6723 - val_loss: 0.6123 - val_accuracy: 0.6636\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6144 - accuracy: 0.6680 - val_loss: 0.6132 - val_accuracy: 0.6603\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6115 - accuracy: 0.6731 - val_loss: 0.6120 - val_accuracy: 0.6616\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6142 - accuracy: 0.6652 - val_loss: 0.6119 - val_accuracy: 0.6629\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6145 - accuracy: 0.6702 - val_loss: 0.6114 - val_accuracy: 0.6616\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6079 - accuracy: 0.6702 - val_loss: 0.6101 - val_accuracy: 0.6610\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.6695 - val_loss: 0.6096 - val_accuracy: 0.6623\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6129 - accuracy: 0.6692 - val_loss: 0.6104 - val_accuracy: 0.6610\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6100 - accuracy: 0.6695 - val_loss: 0.6095 - val_accuracy: 0.6636\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6098 - accuracy: 0.6767 - val_loss: 0.6094 - val_accuracy: 0.6629\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6155 - accuracy: 0.6675 - val_loss: 0.6093 - val_accuracy: 0.6643\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6080 - accuracy: 0.6728 - val_loss: 0.6097 - val_accuracy: 0.6643\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6138 - accuracy: 0.6613 - val_loss: 0.6085 - val_accuracy: 0.6649\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6089 - accuracy: 0.6667 - val_loss: 0.6082 - val_accuracy: 0.6636\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6096 - accuracy: 0.6672 - val_loss: 0.6089 - val_accuracy: 0.6603\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6062 - accuracy: 0.6710 - val_loss: 0.6075 - val_accuracy: 0.6643\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6056 - accuracy: 0.6817 - val_loss: 0.6072 - val_accuracy: 0.6643\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6115 - accuracy: 0.6777 - val_loss: 0.6071 - val_accuracy: 0.6675\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6068 - accuracy: 0.6728 - val_loss: 0.6069 - val_accuracy: 0.6656\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6048 - accuracy: 0.6723 - val_loss: 0.6063 - val_accuracy: 0.6662\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6086 - accuracy: 0.6716 - val_loss: 0.6062 - val_accuracy: 0.6636\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6052 - accuracy: 0.6792 - val_loss: 0.6053 - val_accuracy: 0.6662\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6038 - accuracy: 0.6731 - val_loss: 0.6055 - val_accuracy: 0.6603\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6074 - accuracy: 0.6744 - val_loss: 0.6051 - val_accuracy: 0.6689\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6015 - accuracy: 0.6772 - val_loss: 0.6053 - val_accuracy: 0.6656\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.6805 - val_loss: 0.6051 - val_accuracy: 0.6669\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6067 - accuracy: 0.6677 - val_loss: 0.6047 - val_accuracy: 0.6669\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5975 - accuracy: 0.6912 - val_loss: 0.6045 - val_accuracy: 0.6669\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6056 - accuracy: 0.6743 - val_loss: 0.6036 - val_accuracy: 0.6656\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6016 - accuracy: 0.6820 - val_loss: 0.6032 - val_accuracy: 0.6669\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.6790 - val_loss: 0.6025 - val_accuracy: 0.6695\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6070 - accuracy: 0.6779 - val_loss: 0.6018 - val_accuracy: 0.6735\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6017 - accuracy: 0.6815 - val_loss: 0.6021 - val_accuracy: 0.6794\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6779 - val_loss: 0.6011 - val_accuracy: 0.6767\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6007 - accuracy: 0.6784 - val_loss: 0.6009 - val_accuracy: 0.6721\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6008 - accuracy: 0.6822 - val_loss: 0.6012 - val_accuracy: 0.6735\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5999 - accuracy: 0.6815 - val_loss: 0.6009 - val_accuracy: 0.6708\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6022 - accuracy: 0.6746 - val_loss: 0.6003 - val_accuracy: 0.6761\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6005 - accuracy: 0.6846 - val_loss: 0.6015 - val_accuracy: 0.6702\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5975 - accuracy: 0.6830 - val_loss: 0.5999 - val_accuracy: 0.6748\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6020 - accuracy: 0.6828 - val_loss: 0.6009 - val_accuracy: 0.6761\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5986 - accuracy: 0.6854 - val_loss: 0.6003 - val_accuracy: 0.6721\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5999 - accuracy: 0.6758 - val_loss: 0.6008 - val_accuracy: 0.6715\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.6864 - val_loss: 0.5997 - val_accuracy: 0.6735\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6006 - accuracy: 0.6779 - val_loss: 0.5995 - val_accuracy: 0.6781\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5967 - accuracy: 0.6835 - val_loss: 0.6001 - val_accuracy: 0.6754\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5961 - accuracy: 0.6873 - val_loss: 0.6000 - val_accuracy: 0.6741\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5958 - accuracy: 0.6853 - val_loss: 0.5989 - val_accuracy: 0.6715\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5960 - accuracy: 0.6825 - val_loss: 0.5993 - val_accuracy: 0.6735\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6886 - val_loss: 0.5998 - val_accuracy: 0.6675\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5941 - accuracy: 0.6907 - val_loss: 0.5973 - val_accuracy: 0.6794\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5932 - accuracy: 0.6905 - val_loss: 0.5971 - val_accuracy: 0.6787\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.6820 - val_loss: 0.5969 - val_accuracy: 0.6800\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5917 - accuracy: 0.6905 - val_loss: 0.5970 - val_accuracy: 0.6728\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.6912 - val_loss: 0.5967 - val_accuracy: 0.6794\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5950 - accuracy: 0.6891 - val_loss: 0.5978 - val_accuracy: 0.6807\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5914 - accuracy: 0.6914 - val_loss: 0.5968 - val_accuracy: 0.6820\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.6866 - val_loss: 0.5959 - val_accuracy: 0.6820\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5916 - accuracy: 0.6891 - val_loss: 0.5956 - val_accuracy: 0.6807\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.6815 - val_loss: 0.5965 - val_accuracy: 0.6827\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5943 - accuracy: 0.6874 - val_loss: 0.5952 - val_accuracy: 0.6807\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5927 - accuracy: 0.6874 - val_loss: 0.5955 - val_accuracy: 0.6820\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.6900 - val_loss: 0.5960 - val_accuracy: 0.6820\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.6877 - val_loss: 0.5957 - val_accuracy: 0.6827\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5892 - accuracy: 0.6871 - val_loss: 0.5944 - val_accuracy: 0.6840\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5924 - accuracy: 0.6922 - val_loss: 0.5950 - val_accuracy: 0.6846\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5869 - accuracy: 0.6974 - val_loss: 0.5939 - val_accuracy: 0.6873\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5901 - accuracy: 0.6965 - val_loss: 0.5934 - val_accuracy: 0.6840\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5884 - accuracy: 0.6923 - val_loss: 0.5937 - val_accuracy: 0.6833\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5927 - accuracy: 0.6886 - val_loss: 0.5936 - val_accuracy: 0.6840\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5872 - accuracy: 0.6927 - val_loss: 0.5924 - val_accuracy: 0.6840\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.6927 - val_loss: 0.5932 - val_accuracy: 0.6912\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5883 - accuracy: 0.6996 - val_loss: 0.5928 - val_accuracy: 0.6859\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5937 - accuracy: 0.6910 - val_loss: 0.5945 - val_accuracy: 0.6840\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5882 - accuracy: 0.6953 - val_loss: 0.5923 - val_accuracy: 0.6859\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6955 - val_loss: 0.5920 - val_accuracy: 0.6800\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5876 - accuracy: 0.6953 - val_loss: 0.5926 - val_accuracy: 0.6840\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.6909 - val_loss: 0.5920 - val_accuracy: 0.6859\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.5902 - accuracy: 0.6933 - val_loss: 0.5917 - val_accuracy: 0.6853\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5878 - accuracy: 0.6951 - val_loss: 0.5906 - val_accuracy: 0.6859\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5839 - accuracy: 0.6965 - val_loss: 0.5921 - val_accuracy: 0.6846\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5875 - accuracy: 0.6974 - val_loss: 0.5908 - val_accuracy: 0.6873\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5834 - accuracy: 0.6991 - val_loss: 0.5896 - val_accuracy: 0.6859\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5819 - accuracy: 0.6992 - val_loss: 0.5897 - val_accuracy: 0.6873\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5836 - accuracy: 0.7027 - val_loss: 0.5900 - val_accuracy: 0.6886\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5836 - accuracy: 0.7029 - val_loss: 0.5899 - val_accuracy: 0.6886\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5835 - accuracy: 0.7006 - val_loss: 0.5900 - val_accuracy: 0.6925\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5840 - accuracy: 0.6869 - val_loss: 0.5896 - val_accuracy: 0.6905\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5864 - accuracy: 0.6976 - val_loss: 0.5891 - val_accuracy: 0.6892\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.6925 - val_loss: 0.5904 - val_accuracy: 0.6873\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5876 - accuracy: 0.6986 - val_loss: 0.5896 - val_accuracy: 0.6892\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5799 - accuracy: 0.7009 - val_loss: 0.5890 - val_accuracy: 0.6899\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6991 - val_loss: 0.5888 - val_accuracy: 0.6899\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6992 - val_loss: 0.5881 - val_accuracy: 0.6905\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6966 - val_loss: 0.5893 - val_accuracy: 0.6912\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5864 - accuracy: 0.6983 - val_loss: 0.5886 - val_accuracy: 0.6919\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5839 - accuracy: 0.6951 - val_loss: 0.5885 - val_accuracy: 0.6925\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5746 - accuracy: 0.7078 - val_loss: 0.5881 - val_accuracy: 0.6886\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5814 - accuracy: 0.7075\n",
            "This is for dropout with a value of 0.50\n",
            "Valid accuracy: 0.7075285911560059\n"
          ]
        }
      ],
      "source": [
        "# Number of Dropout\n",
        "Dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "valid_acc_drops = {}\n",
        "for dp in Dropout:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "                name='dense_layer1', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hiddens,\n",
        "    name='dense_layer_2', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    model.compile(optimizer='SGD',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=batch_size, epochs=num_epochs,\n",
        "    verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_drops_valid_loss, NN_drops_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for dropout with a value of %.2f\" % dp)\n",
        "    print('Valid accuracy:', NN_drops_valid_acc)\n",
        "\n",
        "    valid_acc_drops[dp] = NN_drops_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnMR_oIuHydM",
        "outputId": "d877e43f-34fa-4b30-9034-f1a60e429ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0.1: 0.7232952117919922, 0.2: 0.7114702463150024, 0.3: 0.7087110877037048, 0.4: 0.7063460946083069, 0.5: 0.7075285911560059}\n",
            "0.1\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_drops)\n",
        "dp = max(valid_acc_drops, key=valid_acc_drops.get)\n",
        "print(dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EtHAP5He92O"
      },
      "source": [
        "#### 4.2.4 - Different optimizers\n",
        "We have explored stochastic gradient descent previouly, and here we may try\n",
        "the another two optimizers, 'RMSProp', 'Adam', and select one of them based\n",
        "on the better validation accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7I-m1G9e92O",
        "outputId": "49f94e39-add7-442e-9565-19cf7729839c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6811 - accuracy: 0.6017 - val_loss: 0.6282 - val_accuracy: 0.6577\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6212 - accuracy: 0.6585 - val_loss: 0.6135 - val_accuracy: 0.6675\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6020 - accuracy: 0.6779 - val_loss: 0.6276 - val_accuracy: 0.6557\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5868 - accuracy: 0.6919 - val_loss: 0.6310 - val_accuracy: 0.6623\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5717 - accuracy: 0.7038 - val_loss: 0.5922 - val_accuracy: 0.6892\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7150 - val_loss: 0.5946 - val_accuracy: 0.6892\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7263 - val_loss: 0.5901 - val_accuracy: 0.6846\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5351 - accuracy: 0.7314 - val_loss: 0.5834 - val_accuracy: 0.7017\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7393 - val_loss: 0.5864 - val_accuracy: 0.6971\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.7470 - val_loss: 0.5783 - val_accuracy: 0.7043\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5124 - accuracy: 0.7507 - val_loss: 0.6207 - val_accuracy: 0.6702\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7567 - val_loss: 0.6062 - val_accuracy: 0.6833\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4938 - accuracy: 0.7651 - val_loss: 0.5714 - val_accuracy: 0.7063\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 9ms/step - loss: 0.4835 - accuracy: 0.7720 - val_loss: 0.5768 - val_accuracy: 0.7254\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4743 - accuracy: 0.7763 - val_loss: 0.5836 - val_accuracy: 0.7043\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4679 - accuracy: 0.7776 - val_loss: 0.5693 - val_accuracy: 0.7194\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7856 - val_loss: 0.5761 - val_accuracy: 0.7155\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4536 - accuracy: 0.7868 - val_loss: 0.5769 - val_accuracy: 0.7148\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.7935 - val_loss: 0.5730 - val_accuracy: 0.7208\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4401 - accuracy: 0.7957 - val_loss: 0.5821 - val_accuracy: 0.7181\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.7914 - val_loss: 0.5992 - val_accuracy: 0.7004\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8091 - val_loss: 0.6279 - val_accuracy: 0.6886\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4192 - accuracy: 0.8049 - val_loss: 0.5843 - val_accuracy: 0.7227\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4136 - accuracy: 0.8116 - val_loss: 0.6185 - val_accuracy: 0.7017\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4123 - accuracy: 0.8068 - val_loss: 0.6078 - val_accuracy: 0.7122\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3965 - accuracy: 0.8206 - val_loss: 0.6661 - val_accuracy: 0.6767\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4005 - accuracy: 0.8198 - val_loss: 0.5905 - val_accuracy: 0.7181\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3859 - accuracy: 0.8295 - val_loss: 0.5991 - val_accuracy: 0.7188\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3858 - accuracy: 0.8282 - val_loss: 0.5913 - val_accuracy: 0.7254\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3761 - accuracy: 0.8364 - val_loss: 0.6002 - val_accuracy: 0.7234\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8394 - val_loss: 0.6251 - val_accuracy: 0.7201\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3730 - accuracy: 0.8367 - val_loss: 0.6015 - val_accuracy: 0.7260\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8449 - val_loss: 0.6317 - val_accuracy: 0.7188\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3593 - accuracy: 0.8395 - val_loss: 0.6072 - val_accuracy: 0.7214\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3535 - accuracy: 0.8464 - val_loss: 0.6203 - val_accuracy: 0.7227\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3456 - accuracy: 0.8474 - val_loss: 0.6316 - val_accuracy: 0.7286\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.8551 - val_loss: 0.6477 - val_accuracy: 0.7102\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8591 - val_loss: 0.6486 - val_accuracy: 0.7142\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.8568 - val_loss: 0.6513 - val_accuracy: 0.7168\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8647 - val_loss: 0.6483 - val_accuracy: 0.7181\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8597 - val_loss: 0.6767 - val_accuracy: 0.7122\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3126 - accuracy: 0.8661 - val_loss: 0.6360 - val_accuracy: 0.7201\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8612 - val_loss: 0.6571 - val_accuracy: 0.7116\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8640 - val_loss: 0.6735 - val_accuracy: 0.7129\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3013 - accuracy: 0.8732 - val_loss: 0.6634 - val_accuracy: 0.7385\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2987 - accuracy: 0.8753 - val_loss: 0.6697 - val_accuracy: 0.7109\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8770 - val_loss: 0.6836 - val_accuracy: 0.7194\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.8753 - val_loss: 0.7119 - val_accuracy: 0.7011\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.8796 - val_loss: 0.6991 - val_accuracy: 0.7293\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2865 - accuracy: 0.8778 - val_loss: 0.6919 - val_accuracy: 0.7063\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2798 - accuracy: 0.8834 - val_loss: 0.6885 - val_accuracy: 0.7142\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2719 - accuracy: 0.8895 - val_loss: 0.6968 - val_accuracy: 0.7116\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2748 - accuracy: 0.8839 - val_loss: 0.7696 - val_accuracy: 0.6932\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2668 - accuracy: 0.8916 - val_loss: 0.6965 - val_accuracy: 0.7148\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.8941 - val_loss: 0.7855 - val_accuracy: 0.6951\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2587 - accuracy: 0.8957 - val_loss: 0.7103 - val_accuracy: 0.7135\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2539 - accuracy: 0.8926 - val_loss: 0.7546 - val_accuracy: 0.7017\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2539 - accuracy: 0.8985 - val_loss: 0.7365 - val_accuracy: 0.7116\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2506 - accuracy: 0.8975 - val_loss: 0.7554 - val_accuracy: 0.7043\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2468 - accuracy: 0.8972 - val_loss: 0.7604 - val_accuracy: 0.7037\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2418 - accuracy: 0.9057 - val_loss: 0.7557 - val_accuracy: 0.7063\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2346 - accuracy: 0.9060 - val_loss: 0.7692 - val_accuracy: 0.7076\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2466 - accuracy: 0.8996 - val_loss: 0.7680 - val_accuracy: 0.7063\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2345 - accuracy: 0.9033 - val_loss: 0.7866 - val_accuracy: 0.7148\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 0.9052 - val_loss: 0.8033 - val_accuracy: 0.7135\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2306 - accuracy: 0.9054 - val_loss: 0.7929 - val_accuracy: 0.7050\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2245 - accuracy: 0.9113 - val_loss: 0.7753 - val_accuracy: 0.7122\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2238 - accuracy: 0.9097 - val_loss: 0.8166 - val_accuracy: 0.7129\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2224 - accuracy: 0.9125 - val_loss: 0.7695 - val_accuracy: 0.7247\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2126 - accuracy: 0.9154 - val_loss: 0.8231 - val_accuracy: 0.7076\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9129 - val_loss: 0.8291 - val_accuracy: 0.7089\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9164 - val_loss: 0.8538 - val_accuracy: 0.7043\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2008 - accuracy: 0.9231 - val_loss: 0.8564 - val_accuracy: 0.7043\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2085 - accuracy: 0.9156 - val_loss: 0.8572 - val_accuracy: 0.7122\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2032 - accuracy: 0.9192 - val_loss: 0.8482 - val_accuracy: 0.6965\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1976 - accuracy: 0.9262 - val_loss: 0.8421 - val_accuracy: 0.7063\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1975 - accuracy: 0.9210 - val_loss: 0.8654 - val_accuracy: 0.7050\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1986 - accuracy: 0.9253 - val_loss: 0.8731 - val_accuracy: 0.7070\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1947 - accuracy: 0.9210 - val_loss: 0.8807 - val_accuracy: 0.6978\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1957 - accuracy: 0.9244 - val_loss: 0.8718 - val_accuracy: 0.7109\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1824 - accuracy: 0.9299 - val_loss: 0.8754 - val_accuracy: 0.7168\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1830 - accuracy: 0.9299 - val_loss: 0.8801 - val_accuracy: 0.7070\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.9276 - val_loss: 0.9113 - val_accuracy: 0.7011\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9295 - val_loss: 0.8900 - val_accuracy: 0.7004\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1897 - accuracy: 0.9266 - val_loss: 0.8999 - val_accuracy: 0.7142\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1784 - accuracy: 0.9284 - val_loss: 0.9175 - val_accuracy: 0.7011\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1712 - accuracy: 0.9325 - val_loss: 0.9194 - val_accuracy: 0.7102\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1690 - accuracy: 0.9335 - val_loss: 0.9336 - val_accuracy: 0.7024\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9338 - val_loss: 0.9173 - val_accuracy: 0.7089\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9361 - val_loss: 0.9243 - val_accuracy: 0.7142\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1646 - accuracy: 0.9323 - val_loss: 0.9342 - val_accuracy: 0.7175\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9373 - val_loss: 0.9517 - val_accuracy: 0.7043\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.9345 - val_loss: 0.9728 - val_accuracy: 0.7102\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.9356 - val_loss: 0.9609 - val_accuracy: 0.7043\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1646 - accuracy: 0.9373 - val_loss: 0.9439 - val_accuracy: 0.7030\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9420 - val_loss: 0.9519 - val_accuracy: 0.7070\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1623 - accuracy: 0.9361 - val_loss: 1.0226 - val_accuracy: 0.6978\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9382 - val_loss: 0.9567 - val_accuracy: 0.7043\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9419 - val_loss: 0.9976 - val_accuracy: 0.7004\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9448 - val_loss: 0.9927 - val_accuracy: 0.7102\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9369 - val_loss: 0.9953 - val_accuracy: 0.7076\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1474 - accuracy: 0.9420 - val_loss: 1.0382 - val_accuracy: 0.7043\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1574 - accuracy: 0.9397 - val_loss: 1.0025 - val_accuracy: 0.7011\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1474 - accuracy: 0.9438 - val_loss: 0.9981 - val_accuracy: 0.7057\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9432 - val_loss: 1.0311 - val_accuracy: 0.7017\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1530 - accuracy: 0.9387 - val_loss: 1.0135 - val_accuracy: 0.6984\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9423 - val_loss: 1.0230 - val_accuracy: 0.7122\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9430 - val_loss: 1.0833 - val_accuracy: 0.6997\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9458 - val_loss: 1.0421 - val_accuracy: 0.6984\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9451 - val_loss: 1.0328 - val_accuracy: 0.7122\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9427 - val_loss: 1.0231 - val_accuracy: 0.7129\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1456 - accuracy: 0.9504 - val_loss: 1.0482 - val_accuracy: 0.7043\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9473 - val_loss: 1.0605 - val_accuracy: 0.7050\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9427 - val_loss: 1.0655 - val_accuracy: 0.7089\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9502 - val_loss: 1.0427 - val_accuracy: 0.7129\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9461 - val_loss: 1.0199 - val_accuracy: 0.7122\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9451 - val_loss: 1.0339 - val_accuracy: 0.7004\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9486 - val_loss: 1.0697 - val_accuracy: 0.7004\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1276 - accuracy: 0.9537 - val_loss: 1.1019 - val_accuracy: 0.7037\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9512 - val_loss: 1.0516 - val_accuracy: 0.7017\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9483 - val_loss: 1.0864 - val_accuracy: 0.7109\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9496 - val_loss: 1.1471 - val_accuracy: 0.6991\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9529 - val_loss: 1.1470 - val_accuracy: 0.6997\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9532 - val_loss: 1.1426 - val_accuracy: 0.6971\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.9545 - val_loss: 1.1303 - val_accuracy: 0.7070\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1229 - accuracy: 0.9535 - val_loss: 1.1378 - val_accuracy: 0.7116\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9604 - val_loss: 1.1750 - val_accuracy: 0.7083\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9604 - val_loss: 1.1863 - val_accuracy: 0.7050\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9542 - val_loss: 1.1355 - val_accuracy: 0.7129\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9517 - val_loss: 1.1283 - val_accuracy: 0.7030\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9499 - val_loss: 1.1592 - val_accuracy: 0.6965\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9550 - val_loss: 1.1362 - val_accuracy: 0.7057\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9552 - val_loss: 1.1581 - val_accuracy: 0.7168\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9538 - val_loss: 1.1481 - val_accuracy: 0.7024\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9563 - val_loss: 1.1722 - val_accuracy: 0.7155\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9576 - val_loss: 1.1438 - val_accuracy: 0.7142\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9589 - val_loss: 1.1944 - val_accuracy: 0.7057\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9547 - val_loss: 1.1977 - val_accuracy: 0.7043\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9599 - val_loss: 1.1952 - val_accuracy: 0.7063\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9607 - val_loss: 1.2203 - val_accuracy: 0.7076\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9593 - val_loss: 1.2039 - val_accuracy: 0.7109\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9599 - val_loss: 1.2140 - val_accuracy: 0.7083\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1146 - accuracy: 0.9575 - val_loss: 1.2309 - val_accuracy: 0.7070\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9596 - val_loss: 1.2680 - val_accuracy: 0.6997\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9566 - val_loss: 1.2142 - val_accuracy: 0.7116\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9594 - val_loss: 1.2171 - val_accuracy: 0.7155\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9566 - val_loss: 1.2372 - val_accuracy: 0.6991\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9634 - val_loss: 1.3103 - val_accuracy: 0.6965\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9604 - val_loss: 1.2865 - val_accuracy: 0.7063\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9632 - val_loss: 1.2808 - val_accuracy: 0.7129\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9604 - val_loss: 1.2404 - val_accuracy: 0.7070\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9566 - val_loss: 1.2597 - val_accuracy: 0.7096\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9680 - val_loss: 1.3084 - val_accuracy: 0.6971\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9612 - val_loss: 1.2309 - val_accuracy: 0.7057\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9675 - val_loss: 1.3073 - val_accuracy: 0.7076\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9640 - val_loss: 1.2845 - val_accuracy: 0.7162\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9602 - val_loss: 1.3126 - val_accuracy: 0.6997\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9648 - val_loss: 1.2881 - val_accuracy: 0.7089\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9639 - val_loss: 1.2544 - val_accuracy: 0.7122\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9611 - val_loss: 1.3101 - val_accuracy: 0.7116\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0944 - accuracy: 0.9691 - val_loss: 1.3737 - val_accuracy: 0.7057\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9639 - val_loss: 1.3035 - val_accuracy: 0.7063\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9599 - val_loss: 1.3361 - val_accuracy: 0.7083\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9612 - val_loss: 1.3654 - val_accuracy: 0.7050\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0971 - accuracy: 0.9639 - val_loss: 1.3941 - val_accuracy: 0.6984\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9660 - val_loss: 1.3315 - val_accuracy: 0.7129\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9694 - val_loss: 1.3905 - val_accuracy: 0.7011\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9642 - val_loss: 1.3343 - val_accuracy: 0.6997\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9680 - val_loss: 1.3248 - val_accuracy: 0.7240\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9632 - val_loss: 1.3190 - val_accuracy: 0.7004\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9609 - val_loss: 1.3219 - val_accuracy: 0.6958\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 0.9683 - val_loss: 1.3932 - val_accuracy: 0.7017\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0913 - accuracy: 0.9675 - val_loss: 1.3621 - val_accuracy: 0.7004\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9639 - val_loss: 1.3083 - val_accuracy: 0.7057\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9691 - val_loss: 1.3732 - val_accuracy: 0.7024\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9642 - val_loss: 1.3249 - val_accuracy: 0.7076\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9665 - val_loss: 1.3438 - val_accuracy: 0.7024\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9658 - val_loss: 1.4486 - val_accuracy: 0.7050\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9671 - val_loss: 1.3877 - val_accuracy: 0.6945\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9706 - val_loss: 1.4864 - val_accuracy: 0.6991\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9671 - val_loss: 1.4376 - val_accuracy: 0.7083\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9678 - val_loss: 1.4370 - val_accuracy: 0.7070\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9645 - val_loss: 1.3874 - val_accuracy: 0.7096\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9650 - val_loss: 1.4344 - val_accuracy: 0.7030\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 0.9665 - val_loss: 1.3770 - val_accuracy: 0.7057\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9691 - val_loss: 1.4045 - val_accuracy: 0.7063\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0984 - accuracy: 0.9652 - val_loss: 1.3611 - val_accuracy: 0.7102\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9694 - val_loss: 1.4127 - val_accuracy: 0.7070\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9667 - val_loss: 1.4524 - val_accuracy: 0.7116\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9676 - val_loss: 1.4116 - val_accuracy: 0.7057\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9724 - val_loss: 1.4386 - val_accuracy: 0.7063\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0865 - accuracy: 0.9671 - val_loss: 1.4406 - val_accuracy: 0.7083\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9719 - val_loss: 1.5156 - val_accuracy: 0.7050\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9671 - val_loss: 1.4277 - val_accuracy: 0.7129\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9699 - val_loss: 1.4969 - val_accuracy: 0.6984\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9711 - val_loss: 1.4617 - val_accuracy: 0.7116\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9691 - val_loss: 1.4541 - val_accuracy: 0.7116\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9650 - val_loss: 1.4410 - val_accuracy: 0.7017\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9709 - val_loss: 1.4708 - val_accuracy: 0.7030\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0801 - accuracy: 0.9721 - val_loss: 1.4882 - val_accuracy: 0.7070\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 1.3169 - accuracy: 0.7245\n",
            "This is for optimizer RMSProp\n",
            "Valid accuracy: 0.7244777083396912\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 0.6547 - accuracy: 0.6168 - val_loss: 0.6271 - val_accuracy: 0.6465\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6189 - accuracy: 0.6620 - val_loss: 0.6110 - val_accuracy: 0.6702\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5933 - accuracy: 0.6902 - val_loss: 0.6124 - val_accuracy: 0.6767\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5800 - accuracy: 0.7048 - val_loss: 0.5997 - val_accuracy: 0.6886\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5660 - accuracy: 0.7139 - val_loss: 0.5933 - val_accuracy: 0.6859\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5541 - accuracy: 0.7193 - val_loss: 0.5805 - val_accuracy: 0.7089\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5384 - accuracy: 0.7316 - val_loss: 0.5833 - val_accuracy: 0.7024\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5275 - accuracy: 0.7421 - val_loss: 0.5737 - val_accuracy: 0.7083\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5107 - accuracy: 0.7523 - val_loss: 0.5687 - val_accuracy: 0.7168\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.7592 - val_loss: 0.5754 - val_accuracy: 0.7116\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5055 - accuracy: 0.7544 - val_loss: 0.5679 - val_accuracy: 0.7227\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4784 - accuracy: 0.7712 - val_loss: 0.5719 - val_accuracy: 0.7155\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4710 - accuracy: 0.7814 - val_loss: 0.5914 - val_accuracy: 0.7063\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.7812 - val_loss: 0.5824 - val_accuracy: 0.7168\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4550 - accuracy: 0.7845 - val_loss: 0.5803 - val_accuracy: 0.7208\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7966 - val_loss: 0.5743 - val_accuracy: 0.7273\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8011 - val_loss: 0.5793 - val_accuracy: 0.7280\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8035 - val_loss: 0.5977 - val_accuracy: 0.7214\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.8088 - val_loss: 0.5823 - val_accuracy: 0.7372\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4011 - accuracy: 0.8182 - val_loss: 0.5881 - val_accuracy: 0.7194\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8192 - val_loss: 0.5910 - val_accuracy: 0.7280\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3828 - accuracy: 0.8246 - val_loss: 0.6098 - val_accuracy: 0.7142\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8223 - val_loss: 0.5999 - val_accuracy: 0.7267\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8336 - val_loss: 0.6182 - val_accuracy: 0.7201\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8367 - val_loss: 0.6302 - val_accuracy: 0.7214\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.8362 - val_loss: 0.6443 - val_accuracy: 0.7155\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3539 - accuracy: 0.8448 - val_loss: 0.6221 - val_accuracy: 0.7306\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3463 - accuracy: 0.8482 - val_loss: 0.6275 - val_accuracy: 0.7293\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3363 - accuracy: 0.8525 - val_loss: 0.6506 - val_accuracy: 0.7168\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8579 - val_loss: 0.6505 - val_accuracy: 0.7234\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8579 - val_loss: 0.6527 - val_accuracy: 0.7240\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8604 - val_loss: 0.6751 - val_accuracy: 0.7102\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3128 - accuracy: 0.8635 - val_loss: 0.6868 - val_accuracy: 0.7109\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8678 - val_loss: 0.6684 - val_accuracy: 0.7240\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.8719 - val_loss: 0.6770 - val_accuracy: 0.7142\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3010 - accuracy: 0.8734 - val_loss: 0.6980 - val_accuracy: 0.7181\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2988 - accuracy: 0.8742 - val_loss: 0.6920 - val_accuracy: 0.7254\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2852 - accuracy: 0.8811 - val_loss: 0.7084 - val_accuracy: 0.7201\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.8780 - val_loss: 0.6914 - val_accuracy: 0.7181\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2764 - accuracy: 0.8852 - val_loss: 0.7106 - val_accuracy: 0.7267\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2758 - accuracy: 0.8819 - val_loss: 0.7194 - val_accuracy: 0.7254\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2708 - accuracy: 0.8880 - val_loss: 0.7266 - val_accuracy: 0.7214\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2615 - accuracy: 0.8931 - val_loss: 0.7340 - val_accuracy: 0.7208\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2519 - accuracy: 0.8947 - val_loss: 0.7460 - val_accuracy: 0.7175\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2526 - accuracy: 0.8957 - val_loss: 0.7724 - val_accuracy: 0.7194\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2482 - accuracy: 0.8996 - val_loss: 0.7525 - val_accuracy: 0.7254\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9021 - val_loss: 0.7903 - val_accuracy: 0.7155\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2357 - accuracy: 0.9011 - val_loss: 0.7680 - val_accuracy: 0.7175\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2339 - accuracy: 0.9033 - val_loss: 0.7791 - val_accuracy: 0.7181\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2330 - accuracy: 0.9016 - val_loss: 0.8094 - val_accuracy: 0.7194\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2324 - accuracy: 0.9005 - val_loss: 0.8100 - val_accuracy: 0.7234\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2318 - accuracy: 0.9080 - val_loss: 0.8015 - val_accuracy: 0.7260\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2278 - accuracy: 0.9033 - val_loss: 0.8202 - val_accuracy: 0.7234\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9115 - val_loss: 0.8085 - val_accuracy: 0.7188\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.9100 - val_loss: 0.8146 - val_accuracy: 0.7201\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2100 - accuracy: 0.9159 - val_loss: 0.8359 - val_accuracy: 0.7300\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2187 - accuracy: 0.9108 - val_loss: 0.7953 - val_accuracy: 0.7352\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9182 - val_loss: 0.8487 - val_accuracy: 0.7352\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2042 - accuracy: 0.9189 - val_loss: 0.8362 - val_accuracy: 0.7293\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9157 - val_loss: 0.8791 - val_accuracy: 0.7181\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2007 - accuracy: 0.9182 - val_loss: 0.8629 - val_accuracy: 0.7293\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9216 - val_loss: 0.8672 - val_accuracy: 0.7221\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.9267 - val_loss: 0.8660 - val_accuracy: 0.7234\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1954 - accuracy: 0.9236 - val_loss: 0.8868 - val_accuracy: 0.7221\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9272 - val_loss: 0.8728 - val_accuracy: 0.7175\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1881 - accuracy: 0.9246 - val_loss: 0.8788 - val_accuracy: 0.7273\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9271 - val_loss: 0.8815 - val_accuracy: 0.7188\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1898 - accuracy: 0.9276 - val_loss: 0.8796 - val_accuracy: 0.7273\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1732 - accuracy: 0.9356 - val_loss: 0.9172 - val_accuracy: 0.7247\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9290 - val_loss: 0.8915 - val_accuracy: 0.7286\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9358 - val_loss: 0.9178 - val_accuracy: 0.7214\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.9272 - val_loss: 0.9242 - val_accuracy: 0.7227\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9331 - val_loss: 0.9496 - val_accuracy: 0.7129\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9310 - val_loss: 0.9716 - val_accuracy: 0.7221\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9345 - val_loss: 0.9127 - val_accuracy: 0.7293\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9364 - val_loss: 0.9259 - val_accuracy: 0.7208\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9359 - val_loss: 0.9271 - val_accuracy: 0.7273\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9353 - val_loss: 0.9486 - val_accuracy: 0.7214\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9420 - val_loss: 1.0088 - val_accuracy: 0.7168\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9407 - val_loss: 0.9375 - val_accuracy: 0.7168\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9446 - val_loss: 1.0042 - val_accuracy: 0.7083\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.9354 - val_loss: 0.9987 - val_accuracy: 0.7194\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.9394 - val_loss: 0.9780 - val_accuracy: 0.7208\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9442 - val_loss: 1.0093 - val_accuracy: 0.7221\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9387 - val_loss: 0.9755 - val_accuracy: 0.7286\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9466 - val_loss: 1.0222 - val_accuracy: 0.7221\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9427 - val_loss: 1.0032 - val_accuracy: 0.7162\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9427 - val_loss: 1.0236 - val_accuracy: 0.7201\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9466 - val_loss: 1.0042 - val_accuracy: 0.7208\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9460 - val_loss: 1.0612 - val_accuracy: 0.7155\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9473 - val_loss: 1.0510 - val_accuracy: 0.7175\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9428 - val_loss: 1.0932 - val_accuracy: 0.7188\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9468 - val_loss: 1.0333 - val_accuracy: 0.7194\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1373 - accuracy: 0.9512 - val_loss: 1.0747 - val_accuracy: 0.7208\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9494 - val_loss: 1.0411 - val_accuracy: 0.7122\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9527 - val_loss: 1.0463 - val_accuracy: 0.7201\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9550 - val_loss: 1.0628 - val_accuracy: 0.7162\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1417 - accuracy: 0.9456 - val_loss: 1.0377 - val_accuracy: 0.7070\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9488 - val_loss: 1.0674 - val_accuracy: 0.7168\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9496 - val_loss: 1.0721 - val_accuracy: 0.7135\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.9548 - val_loss: 1.0724 - val_accuracy: 0.7175\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9540 - val_loss: 1.0714 - val_accuracy: 0.7273\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9527 - val_loss: 1.0867 - val_accuracy: 0.7194\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1246 - accuracy: 0.9515 - val_loss: 1.0887 - val_accuracy: 0.7254\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1251 - accuracy: 0.9532 - val_loss: 1.1267 - val_accuracy: 0.7102\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9525 - val_loss: 1.1177 - val_accuracy: 0.7155\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9561 - val_loss: 1.1435 - val_accuracy: 0.7083\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9524 - val_loss: 1.0940 - val_accuracy: 0.7148\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9524 - val_loss: 1.0894 - val_accuracy: 0.7227\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9560 - val_loss: 1.1214 - val_accuracy: 0.7306\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9517 - val_loss: 1.1291 - val_accuracy: 0.7227\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9552 - val_loss: 1.1251 - val_accuracy: 0.7188\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9571 - val_loss: 1.1270 - val_accuracy: 0.7168\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 1.1349 - val_accuracy: 0.7168\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9578 - val_loss: 1.1330 - val_accuracy: 0.7227\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9584 - val_loss: 1.1220 - val_accuracy: 0.7116\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9593 - val_loss: 1.1534 - val_accuracy: 0.7168\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9565 - val_loss: 1.1456 - val_accuracy: 0.7188\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1114 - accuracy: 0.9611 - val_loss: 1.1689 - val_accuracy: 0.7135\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1114 - accuracy: 0.9617 - val_loss: 1.1676 - val_accuracy: 0.7070\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9642 - val_loss: 1.1987 - val_accuracy: 0.7155\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9627 - val_loss: 1.1891 - val_accuracy: 0.7142\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9648 - val_loss: 1.1967 - val_accuracy: 0.7175\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9598 - val_loss: 1.1896 - val_accuracy: 0.7142\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9622 - val_loss: 1.1952 - val_accuracy: 0.7208\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9612 - val_loss: 1.2184 - val_accuracy: 0.7122\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9644 - val_loss: 1.2160 - val_accuracy: 0.7129\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1137 - accuracy: 0.9589 - val_loss: 1.1866 - val_accuracy: 0.7214\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9645 - val_loss: 1.2557 - val_accuracy: 0.7089\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.9665 - val_loss: 1.2410 - val_accuracy: 0.7116\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9640 - val_loss: 1.1838 - val_accuracy: 0.7175\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9662 - val_loss: 1.2473 - val_accuracy: 0.7116\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9563 - val_loss: 1.2403 - val_accuracy: 0.7148\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9611 - val_loss: 1.1865 - val_accuracy: 0.7122\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9593 - val_loss: 1.1952 - val_accuracy: 0.7168\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9647 - val_loss: 1.2191 - val_accuracy: 0.7076\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9658 - val_loss: 1.2870 - val_accuracy: 0.7155\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9624 - val_loss: 1.2437 - val_accuracy: 0.7129\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1011 - accuracy: 0.9599 - val_loss: 1.2186 - val_accuracy: 0.7057\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9635 - val_loss: 1.2156 - val_accuracy: 0.7135\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9696 - val_loss: 1.2071 - val_accuracy: 0.7155\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1045 - accuracy: 0.9596 - val_loss: 1.2396 - val_accuracy: 0.7155\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9658 - val_loss: 1.2295 - val_accuracy: 0.7181\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9650 - val_loss: 1.2509 - val_accuracy: 0.7076\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.9619 - val_loss: 1.2332 - val_accuracy: 0.7247\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9635 - val_loss: 1.2738 - val_accuracy: 0.7102\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9637 - val_loss: 1.2208 - val_accuracy: 0.7227\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9673 - val_loss: 1.2878 - val_accuracy: 0.7234\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0966 - accuracy: 0.9645 - val_loss: 1.2388 - val_accuracy: 0.7286\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9662 - val_loss: 1.2642 - val_accuracy: 0.7247\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9678 - val_loss: 1.2921 - val_accuracy: 0.7135\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9683 - val_loss: 1.2762 - val_accuracy: 0.7201\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9709 - val_loss: 1.2723 - val_accuracy: 0.7102\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9647 - val_loss: 1.2417 - val_accuracy: 0.7240\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9662 - val_loss: 1.2134 - val_accuracy: 0.7286\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9673 - val_loss: 1.2550 - val_accuracy: 0.7332\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.9686 - val_loss: 1.2671 - val_accuracy: 0.7201\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0916 - accuracy: 0.9647 - val_loss: 1.2638 - val_accuracy: 0.7188\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9698 - val_loss: 1.2975 - val_accuracy: 0.7181\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9736 - val_loss: 1.2724 - val_accuracy: 0.7129\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9713 - val_loss: 1.2787 - val_accuracy: 0.7168\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9673 - val_loss: 1.2909 - val_accuracy: 0.7175\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9678 - val_loss: 1.2862 - val_accuracy: 0.7234\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0882 - accuracy: 0.9675 - val_loss: 1.2703 - val_accuracy: 0.7254\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9667 - val_loss: 1.2922 - val_accuracy: 0.7188\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9671 - val_loss: 1.3273 - val_accuracy: 0.7221\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9694 - val_loss: 1.2842 - val_accuracy: 0.7221\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9663 - val_loss: 1.2850 - val_accuracy: 0.7142\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9681 - val_loss: 1.3002 - val_accuracy: 0.7214\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.9731 - val_loss: 1.3191 - val_accuracy: 0.7240\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9704 - val_loss: 1.3726 - val_accuracy: 0.7135\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.9703 - val_loss: 1.3180 - val_accuracy: 0.7162\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9690 - val_loss: 1.3560 - val_accuracy: 0.7194\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9716 - val_loss: 1.3571 - val_accuracy: 0.7188\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9708 - val_loss: 1.3337 - val_accuracy: 0.7070\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9704 - val_loss: 1.3340 - val_accuracy: 0.7155\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0877 - accuracy: 0.9644 - val_loss: 1.3760 - val_accuracy: 0.7227\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.9703 - val_loss: 1.3472 - val_accuracy: 0.7181\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9736 - val_loss: 1.3628 - val_accuracy: 0.7221\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9698 - val_loss: 1.3423 - val_accuracy: 0.7194\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9731 - val_loss: 1.4025 - val_accuracy: 0.7162\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.9729 - val_loss: 1.3711 - val_accuracy: 0.7267\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.9701 - val_loss: 1.4286 - val_accuracy: 0.7102\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9719 - val_loss: 1.3246 - val_accuracy: 0.7273\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9719 - val_loss: 1.3696 - val_accuracy: 0.7116\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9717 - val_loss: 1.3622 - val_accuracy: 0.7280\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9714 - val_loss: 1.3524 - val_accuracy: 0.7096\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9704 - val_loss: 1.3860 - val_accuracy: 0.7043\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9727 - val_loss: 1.3880 - val_accuracy: 0.7155\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9732 - val_loss: 1.4280 - val_accuracy: 0.7004\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9726 - val_loss: 1.4236 - val_accuracy: 0.7142\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9716 - val_loss: 1.4090 - val_accuracy: 0.7168\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9750 - val_loss: 1.3708 - val_accuracy: 0.7162\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9683 - val_loss: 1.3801 - val_accuracy: 0.7234\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9722 - val_loss: 1.4309 - val_accuracy: 0.7089\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9745 - val_loss: 1.3705 - val_accuracy: 0.7267\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9749 - val_loss: 1.4005 - val_accuracy: 0.7129\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 1.3873 - val_accuracy: 0.7162\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9750 - val_loss: 1.4167 - val_accuracy: 0.7188\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9711 - val_loss: 1.3855 - val_accuracy: 0.7221\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 1.3180 - accuracy: 0.7221\n",
            "This is for optimizer Adam\n",
            "Valid accuracy: 0.7221127152442932\n"
          ]
        }
      ],
      "source": [
        "optimizers = ['RMSProp', 'Adam']\n",
        "valid_acc_optimizers = {}\n",
        "\n",
        "for optimizer in optimizers: \n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "                  name='dense_layer1', activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "  model.add(keras.layers.Dense(n_hiddens,\n",
        "  name='dense_layer_2', activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "  model.add(keras.layers.Dense(n_classes,\n",
        "      name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "  # Summary of the model.\n",
        "  model.summary()\n",
        "\n",
        "  # Compiling the model.\n",
        "  model.compile(optimizer=optimizer,\n",
        "  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Training the model.\n",
        "  model.fit(XTrain_mixture, yTrain, batch_size=batch_size, epochs=num_epochs,\n",
        "  verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "  # Evaluating the model.\n",
        "  NN_optimizer_valid_loss, NN_optimizer_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "\n",
        "  print(\"This is for optimizer %s\" % optimizer)\n",
        "  print('Valid accuracy:', NN_optimizer_valid_acc)\n",
        "\n",
        "  valid_acc_optimizers[optimizer] = NN_optimizer_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIp3D13ubfb7",
        "outputId": "2df4fa3a-dc12-4e02-9941-025c9c4f8bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RMSProp': 0.7244777083396912, 'Adam': 0.7221127152442932}\n",
            "RMSProp\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_optimizers)\n",
        "optimizer = max(valid_acc_optimizers, key=valid_acc_optimizers.get)\n",
        "\n",
        "# Preferred Optimizer\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqbB-ik7e92P"
      },
      "source": [
        "#### 4.2.5 - Number of epoches\n",
        "Number of epoches 25, 50, 100, 200, 300 has been tested and picked based on validation accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aVOwTlae92P",
        "outputId": "dae3c0be-5e15-4037-de32-3b2483e8e082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6680 - accuracy: 0.6094 - val_loss: 0.6286 - val_accuracy: 0.6511\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6185 - accuracy: 0.6664 - val_loss: 0.6156 - val_accuracy: 0.6695\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.6822 - val_loss: 0.6023 - val_accuracy: 0.6859\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5870 - accuracy: 0.6922 - val_loss: 0.6034 - val_accuracy: 0.6735\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5735 - accuracy: 0.7055 - val_loss: 0.5910 - val_accuracy: 0.6859\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5656 - accuracy: 0.7135 - val_loss: 0.6103 - val_accuracy: 0.6767\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.7263 - val_loss: 0.5897 - val_accuracy: 0.6971\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.7301 - val_loss: 0.5898 - val_accuracy: 0.7057\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7387 - val_loss: 0.6101 - val_accuracy: 0.6840\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7401 - val_loss: 0.5777 - val_accuracy: 0.7076\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.7553 - val_loss: 0.5950 - val_accuracy: 0.7050\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5098 - accuracy: 0.7520 - val_loss: 0.5984 - val_accuracy: 0.7011\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4998 - accuracy: 0.7579 - val_loss: 0.5970 - val_accuracy: 0.7109\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7694 - val_loss: 0.6044 - val_accuracy: 0.6945\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.7704 - val_loss: 0.5881 - val_accuracy: 0.7030\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7705 - val_loss: 0.5857 - val_accuracy: 0.7194\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.7792 - val_loss: 0.6087 - val_accuracy: 0.7043\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7797 - val_loss: 0.6056 - val_accuracy: 0.6971\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.7858 - val_loss: 0.5897 - val_accuracy: 0.7063\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.7950 - val_loss: 0.5812 - val_accuracy: 0.7142\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7966 - val_loss: 0.5771 - val_accuracy: 0.7089\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4368 - accuracy: 0.8021 - val_loss: 0.5858 - val_accuracy: 0.7102\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8019 - val_loss: 0.5900 - val_accuracy: 0.7214\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8098 - val_loss: 0.5866 - val_accuracy: 0.7260\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8111 - val_loss: 0.6043 - val_accuracy: 0.7083\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.7406\n",
            "This is for epoch with a value of 25\n",
            "Valid accuracy: 0.7406385540962219\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6593 - accuracy: 0.6135 - val_loss: 0.7410 - val_accuracy: 0.5434\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6232 - accuracy: 0.6598 - val_loss: 0.6146 - val_accuracy: 0.6682\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5994 - accuracy: 0.6782 - val_loss: 0.6043 - val_accuracy: 0.6715\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.6971 - val_loss: 0.5950 - val_accuracy: 0.6853\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5742 - accuracy: 0.7034 - val_loss: 0.5924 - val_accuracy: 0.6984\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5633 - accuracy: 0.7139 - val_loss: 0.5861 - val_accuracy: 0.6978\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5516 - accuracy: 0.7260 - val_loss: 0.5773 - val_accuracy: 0.7083\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.7300 - val_loss: 0.5788 - val_accuracy: 0.7089\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5320 - accuracy: 0.7403 - val_loss: 0.5810 - val_accuracy: 0.7076\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5213 - accuracy: 0.7452 - val_loss: 0.5925 - val_accuracy: 0.6925\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5098 - accuracy: 0.7492 - val_loss: 0.5687 - val_accuracy: 0.7162\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5023 - accuracy: 0.7572 - val_loss: 0.5879 - val_accuracy: 0.6971\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4976 - accuracy: 0.7695 - val_loss: 0.5728 - val_accuracy: 0.7168\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4854 - accuracy: 0.7704 - val_loss: 0.5797 - val_accuracy: 0.7063\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4746 - accuracy: 0.7727 - val_loss: 0.5873 - val_accuracy: 0.7017\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4703 - accuracy: 0.7779 - val_loss: 0.5703 - val_accuracy: 0.7188\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4667 - accuracy: 0.7850 - val_loss: 0.5876 - val_accuracy: 0.7129\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4526 - accuracy: 0.7879 - val_loss: 0.5890 - val_accuracy: 0.7109\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.7891 - val_loss: 0.5731 - val_accuracy: 0.7181\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4443 - accuracy: 0.7989 - val_loss: 0.5747 - val_accuracy: 0.7214\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7983 - val_loss: 0.5752 - val_accuracy: 0.7254\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4282 - accuracy: 0.8091 - val_loss: 0.6101 - val_accuracy: 0.7129\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8075 - val_loss: 0.5862 - val_accuracy: 0.7221\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8127 - val_loss: 0.6108 - val_accuracy: 0.7017\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4108 - accuracy: 0.8155 - val_loss: 0.6093 - val_accuracy: 0.7089\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4015 - accuracy: 0.8119 - val_loss: 0.6021 - val_accuracy: 0.7011\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3938 - accuracy: 0.8213 - val_loss: 0.6057 - val_accuracy: 0.7168\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3905 - accuracy: 0.8219 - val_loss: 0.6078 - val_accuracy: 0.7057\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8308 - val_loss: 0.6055 - val_accuracy: 0.7188\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8282 - val_loss: 0.5987 - val_accuracy: 0.7240\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3753 - accuracy: 0.8320 - val_loss: 0.6152 - val_accuracy: 0.7148\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3648 - accuracy: 0.8372 - val_loss: 0.6388 - val_accuracy: 0.7102\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8441 - val_loss: 0.6286 - val_accuracy: 0.7175\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8436 - val_loss: 0.6411 - val_accuracy: 0.7109\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8430 - val_loss: 0.6303 - val_accuracy: 0.7096\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3451 - accuracy: 0.8499 - val_loss: 0.6705 - val_accuracy: 0.6899\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3386 - accuracy: 0.8497 - val_loss: 0.6415 - val_accuracy: 0.7050\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3371 - accuracy: 0.8550 - val_loss: 0.6893 - val_accuracy: 0.6945\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8540 - val_loss: 0.6581 - val_accuracy: 0.7234\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.8574 - val_loss: 0.6418 - val_accuracy: 0.7201\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8615 - val_loss: 0.6618 - val_accuracy: 0.7116\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8584 - val_loss: 0.6794 - val_accuracy: 0.7024\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8614 - val_loss: 0.6673 - val_accuracy: 0.7122\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3017 - accuracy: 0.8732 - val_loss: 0.6710 - val_accuracy: 0.7129\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3116 - accuracy: 0.8660 - val_loss: 0.6688 - val_accuracy: 0.7201\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2916 - accuracy: 0.8765 - val_loss: 0.6894 - val_accuracy: 0.6991\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2978 - accuracy: 0.8760 - val_loss: 0.6925 - val_accuracy: 0.7162\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2963 - accuracy: 0.8747 - val_loss: 0.6771 - val_accuracy: 0.7188\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.8788 - val_loss: 0.7060 - val_accuracy: 0.7109\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2771 - accuracy: 0.8832 - val_loss: 0.7077 - val_accuracy: 0.7057\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.7335\n",
            "This is for epoch with a value of 50\n",
            "Valid accuracy: 0.7335435748100281\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6627 - accuracy: 0.6081 - val_loss: 0.6297 - val_accuracy: 0.6531\n",
            "Epoch 2/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6285 - accuracy: 0.6546 - val_loss: 0.6196 - val_accuracy: 0.6610\n",
            "Epoch 3/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6063 - accuracy: 0.6743 - val_loss: 0.6030 - val_accuracy: 0.6767\n",
            "Epoch 4/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5913 - accuracy: 0.6891 - val_loss: 0.5987 - val_accuracy: 0.6800\n",
            "Epoch 5/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.7006 - val_loss: 0.5914 - val_accuracy: 0.6813\n",
            "Epoch 6/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5663 - accuracy: 0.7078 - val_loss: 0.5803 - val_accuracy: 0.6978\n",
            "Epoch 7/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5555 - accuracy: 0.7178 - val_loss: 0.5904 - val_accuracy: 0.6951\n",
            "Epoch 8/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5419 - accuracy: 0.7314 - val_loss: 0.5736 - val_accuracy: 0.7096\n",
            "Epoch 9/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5325 - accuracy: 0.7415 - val_loss: 0.6623 - val_accuracy: 0.6478\n",
            "Epoch 10/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5256 - accuracy: 0.7446 - val_loss: 0.5708 - val_accuracy: 0.7043\n",
            "Epoch 11/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5143 - accuracy: 0.7502 - val_loss: 0.5738 - val_accuracy: 0.6997\n",
            "Epoch 12/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7566 - val_loss: 0.5930 - val_accuracy: 0.7037\n",
            "Epoch 13/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4934 - accuracy: 0.7664 - val_loss: 0.5691 - val_accuracy: 0.7083\n",
            "Epoch 14/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4906 - accuracy: 0.7702 - val_loss: 0.5687 - val_accuracy: 0.7188\n",
            "Epoch 15/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4793 - accuracy: 0.7707 - val_loss: 0.5733 - val_accuracy: 0.7148\n",
            "Epoch 16/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4735 - accuracy: 0.7761 - val_loss: 0.5808 - val_accuracy: 0.7129\n",
            "Epoch 17/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7796 - val_loss: 0.5641 - val_accuracy: 0.7280\n",
            "Epoch 18/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7814 - val_loss: 0.5704 - val_accuracy: 0.7142\n",
            "Epoch 19/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4529 - accuracy: 0.7901 - val_loss: 0.5723 - val_accuracy: 0.7168\n",
            "Epoch 20/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7950 - val_loss: 0.5749 - val_accuracy: 0.7168\n",
            "Epoch 21/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7983 - val_loss: 0.5767 - val_accuracy: 0.7194\n",
            "Epoch 22/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8031 - val_loss: 0.5841 - val_accuracy: 0.7155\n",
            "Epoch 23/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8017 - val_loss: 0.5795 - val_accuracy: 0.7254\n",
            "Epoch 24/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4211 - accuracy: 0.8037 - val_loss: 0.5813 - val_accuracy: 0.7208\n",
            "Epoch 25/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4105 - accuracy: 0.8119 - val_loss: 0.5974 - val_accuracy: 0.7155\n",
            "Epoch 26/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4076 - accuracy: 0.8118 - val_loss: 0.5883 - val_accuracy: 0.7175\n",
            "Epoch 27/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4016 - accuracy: 0.8170 - val_loss: 0.5838 - val_accuracy: 0.7221\n",
            "Epoch 28/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3914 - accuracy: 0.8219 - val_loss: 0.6203 - val_accuracy: 0.7214\n",
            "Epoch 29/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3895 - accuracy: 0.8259 - val_loss: 0.5821 - val_accuracy: 0.7175\n",
            "Epoch 30/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3834 - accuracy: 0.8254 - val_loss: 0.6107 - val_accuracy: 0.7194\n",
            "Epoch 31/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3847 - accuracy: 0.8265 - val_loss: 0.5979 - val_accuracy: 0.7109\n",
            "Epoch 32/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3808 - accuracy: 0.8282 - val_loss: 0.6304 - val_accuracy: 0.7135\n",
            "Epoch 33/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3668 - accuracy: 0.8357 - val_loss: 0.6076 - val_accuracy: 0.7273\n",
            "Epoch 34/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3590 - accuracy: 0.8481 - val_loss: 0.6195 - val_accuracy: 0.7050\n",
            "Epoch 35/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3524 - accuracy: 0.8472 - val_loss: 0.6256 - val_accuracy: 0.7083\n",
            "Epoch 36/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.8428 - val_loss: 0.6295 - val_accuracy: 0.7070\n",
            "Epoch 37/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3429 - accuracy: 0.8472 - val_loss: 0.6229 - val_accuracy: 0.7050\n",
            "Epoch 38/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.8482 - val_loss: 0.6241 - val_accuracy: 0.7181\n",
            "Epoch 39/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3376 - accuracy: 0.8479 - val_loss: 0.6344 - val_accuracy: 0.7135\n",
            "Epoch 40/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8561 - val_loss: 0.6221 - val_accuracy: 0.7194\n",
            "Epoch 41/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.8581 - val_loss: 0.6493 - val_accuracy: 0.7247\n",
            "Epoch 42/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3218 - accuracy: 0.8591 - val_loss: 0.6389 - val_accuracy: 0.7247\n",
            "Epoch 43/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8628 - val_loss: 0.6617 - val_accuracy: 0.7057\n",
            "Epoch 44/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3103 - accuracy: 0.8670 - val_loss: 0.6595 - val_accuracy: 0.7076\n",
            "Epoch 45/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.8755 - val_loss: 0.6459 - val_accuracy: 0.7227\n",
            "Epoch 46/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3066 - accuracy: 0.8689 - val_loss: 0.6489 - val_accuracy: 0.7168\n",
            "Epoch 47/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2969 - accuracy: 0.8783 - val_loss: 0.6446 - val_accuracy: 0.7096\n",
            "Epoch 48/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2901 - accuracy: 0.8780 - val_loss: 0.6674 - val_accuracy: 0.7089\n",
            "Epoch 49/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2838 - accuracy: 0.8847 - val_loss: 0.7075 - val_accuracy: 0.6951\n",
            "Epoch 50/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2892 - accuracy: 0.8757 - val_loss: 0.6813 - val_accuracy: 0.7122\n",
            "Epoch 51/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2871 - accuracy: 0.8781 - val_loss: 0.6767 - val_accuracy: 0.7168\n",
            "Epoch 52/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.8752 - val_loss: 0.6724 - val_accuracy: 0.7148\n",
            "Epoch 53/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2765 - accuracy: 0.8834 - val_loss: 0.6992 - val_accuracy: 0.7162\n",
            "Epoch 54/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8878 - val_loss: 0.7020 - val_accuracy: 0.7116\n",
            "Epoch 55/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2672 - accuracy: 0.8878 - val_loss: 0.7066 - val_accuracy: 0.7168\n",
            "Epoch 56/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2581 - accuracy: 0.8939 - val_loss: 0.7205 - val_accuracy: 0.7142\n",
            "Epoch 57/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2599 - accuracy: 0.8904 - val_loss: 0.7143 - val_accuracy: 0.7109\n",
            "Epoch 58/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.8950 - val_loss: 0.7085 - val_accuracy: 0.7214\n",
            "Epoch 59/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2502 - accuracy: 0.8998 - val_loss: 0.7508 - val_accuracy: 0.7030\n",
            "Epoch 60/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.8987 - val_loss: 0.7308 - val_accuracy: 0.7188\n",
            "Epoch 61/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2487 - accuracy: 0.8934 - val_loss: 0.7367 - val_accuracy: 0.7247\n",
            "Epoch 62/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2409 - accuracy: 0.9005 - val_loss: 0.7228 - val_accuracy: 0.7135\n",
            "Epoch 63/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2427 - accuracy: 0.9026 - val_loss: 0.7884 - val_accuracy: 0.6971\n",
            "Epoch 64/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9010 - val_loss: 0.7652 - val_accuracy: 0.7057\n",
            "Epoch 65/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.9041 - val_loss: 0.7473 - val_accuracy: 0.7135\n",
            "Epoch 66/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9072 - val_loss: 0.7700 - val_accuracy: 0.7175\n",
            "Epoch 67/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.9077 - val_loss: 0.7932 - val_accuracy: 0.7116\n",
            "Epoch 68/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2239 - accuracy: 0.9065 - val_loss: 0.7675 - val_accuracy: 0.7162\n",
            "Epoch 69/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2223 - accuracy: 0.9083 - val_loss: 0.7743 - val_accuracy: 0.7247\n",
            "Epoch 70/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2317 - accuracy: 0.9046 - val_loss: 0.7776 - val_accuracy: 0.7135\n",
            "Epoch 71/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2157 - accuracy: 0.9131 - val_loss: 0.7848 - val_accuracy: 0.7122\n",
            "Epoch 72/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2089 - accuracy: 0.9179 - val_loss: 0.8034 - val_accuracy: 0.7234\n",
            "Epoch 73/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9152 - val_loss: 0.7939 - val_accuracy: 0.7227\n",
            "Epoch 74/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9187 - val_loss: 0.8271 - val_accuracy: 0.7063\n",
            "Epoch 75/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2108 - accuracy: 0.9161 - val_loss: 0.8429 - val_accuracy: 0.7109\n",
            "Epoch 76/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2004 - accuracy: 0.9192 - val_loss: 0.7932 - val_accuracy: 0.7201\n",
            "Epoch 77/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2037 - accuracy: 0.9192 - val_loss: 0.8220 - val_accuracy: 0.7142\n",
            "Epoch 78/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9254 - val_loss: 0.8379 - val_accuracy: 0.7135\n",
            "Epoch 79/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1987 - accuracy: 0.9175 - val_loss: 0.8313 - val_accuracy: 0.7201\n",
            "Epoch 80/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9190 - val_loss: 0.8726 - val_accuracy: 0.7030\n",
            "Epoch 81/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.9285 - val_loss: 0.8421 - val_accuracy: 0.7024\n",
            "Epoch 82/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9236 - val_loss: 0.8453 - val_accuracy: 0.7201\n",
            "Epoch 83/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1828 - accuracy: 0.9281 - val_loss: 0.8613 - val_accuracy: 0.7194\n",
            "Epoch 84/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9256 - val_loss: 0.8735 - val_accuracy: 0.7070\n",
            "Epoch 85/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9341 - val_loss: 0.8653 - val_accuracy: 0.7116\n",
            "Epoch 86/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1775 - accuracy: 0.9310 - val_loss: 0.9141 - val_accuracy: 0.7043\n",
            "Epoch 87/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1847 - accuracy: 0.9290 - val_loss: 0.8814 - val_accuracy: 0.7135\n",
            "Epoch 88/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9292 - val_loss: 0.8758 - val_accuracy: 0.7057\n",
            "Epoch 89/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9318 - val_loss: 0.9247 - val_accuracy: 0.6938\n",
            "Epoch 90/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9361 - val_loss: 0.8779 - val_accuracy: 0.7247\n",
            "Epoch 91/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1715 - accuracy: 0.9348 - val_loss: 0.9266 - val_accuracy: 0.7135\n",
            "Epoch 92/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1639 - accuracy: 0.9368 - val_loss: 0.9291 - val_accuracy: 0.7030\n",
            "Epoch 93/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.9345 - val_loss: 0.9338 - val_accuracy: 0.7102\n",
            "Epoch 94/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1637 - accuracy: 0.9373 - val_loss: 0.9377 - val_accuracy: 0.7155\n",
            "Epoch 95/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1606 - accuracy: 0.9419 - val_loss: 0.9513 - val_accuracy: 0.7162\n",
            "Epoch 96/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9371 - val_loss: 0.9687 - val_accuracy: 0.7129\n",
            "Epoch 97/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9377 - val_loss: 1.0253 - val_accuracy: 0.6866\n",
            "Epoch 98/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9348 - val_loss: 1.0095 - val_accuracy: 0.7011\n",
            "Epoch 99/100\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.9430 - val_loss: 0.9862 - val_accuracy: 0.7004\n",
            "Epoch 100/100\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9368 - val_loss: 0.9864 - val_accuracy: 0.7076\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.8992 - accuracy: 0.7217\n",
            "This is for epoch with a value of 100\n",
            "Valid accuracy: 0.7217185497283936\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6711 - accuracy: 0.6061 - val_loss: 0.6545 - val_accuracy: 0.6110\n",
            "Epoch 2/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6140 - accuracy: 0.6670 - val_loss: 0.6123 - val_accuracy: 0.6643\n",
            "Epoch 3/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5986 - accuracy: 0.6828 - val_loss: 0.5999 - val_accuracy: 0.6794\n",
            "Epoch 4/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.6989 - val_loss: 0.5913 - val_accuracy: 0.6932\n",
            "Epoch 5/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5674 - accuracy: 0.7112 - val_loss: 0.5883 - val_accuracy: 0.6991\n",
            "Epoch 6/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5577 - accuracy: 0.7158 - val_loss: 0.5838 - val_accuracy: 0.6932\n",
            "Epoch 7/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.7303 - val_loss: 0.5840 - val_accuracy: 0.7004\n",
            "Epoch 8/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5303 - accuracy: 0.7378 - val_loss: 0.5726 - val_accuracy: 0.7129\n",
            "Epoch 9/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5196 - accuracy: 0.7515 - val_loss: 0.6086 - val_accuracy: 0.6833\n",
            "Epoch 10/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5072 - accuracy: 0.7553 - val_loss: 0.5720 - val_accuracy: 0.7188\n",
            "Epoch 11/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4978 - accuracy: 0.7618 - val_loss: 0.5797 - val_accuracy: 0.6984\n",
            "Epoch 12/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4965 - accuracy: 0.7603 - val_loss: 0.5685 - val_accuracy: 0.7201\n",
            "Epoch 13/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.7695 - val_loss: 0.5711 - val_accuracy: 0.7280\n",
            "Epoch 14/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4722 - accuracy: 0.7697 - val_loss: 0.5718 - val_accuracy: 0.7234\n",
            "Epoch 15/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.7799 - val_loss: 0.5829 - val_accuracy: 0.7129\n",
            "Epoch 16/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7906 - val_loss: 0.5645 - val_accuracy: 0.7365\n",
            "Epoch 17/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4517 - accuracy: 0.7870 - val_loss: 0.5876 - val_accuracy: 0.7148\n",
            "Epoch 18/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4410 - accuracy: 0.8006 - val_loss: 0.5724 - val_accuracy: 0.7194\n",
            "Epoch 19/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4306 - accuracy: 0.7976 - val_loss: 0.5785 - val_accuracy: 0.7326\n",
            "Epoch 20/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8026 - val_loss: 0.5757 - val_accuracy: 0.7306\n",
            "Epoch 21/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8070 - val_loss: 0.6575 - val_accuracy: 0.6892\n",
            "Epoch 22/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8154 - val_loss: 0.5848 - val_accuracy: 0.7208\n",
            "Epoch 23/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4071 - accuracy: 0.8103 - val_loss: 0.5734 - val_accuracy: 0.7293\n",
            "Epoch 24/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3995 - accuracy: 0.8215 - val_loss: 0.6120 - val_accuracy: 0.7076\n",
            "Epoch 25/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8198 - val_loss: 0.6070 - val_accuracy: 0.7208\n",
            "Epoch 26/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3816 - accuracy: 0.8244 - val_loss: 0.6018 - val_accuracy: 0.7273\n",
            "Epoch 27/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3844 - accuracy: 0.8252 - val_loss: 0.6105 - val_accuracy: 0.7240\n",
            "Epoch 28/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3737 - accuracy: 0.8326 - val_loss: 0.6283 - val_accuracy: 0.7083\n",
            "Epoch 29/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3637 - accuracy: 0.8398 - val_loss: 0.6621 - val_accuracy: 0.6958\n",
            "Epoch 30/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8418 - val_loss: 0.6329 - val_accuracy: 0.7011\n",
            "Epoch 31/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.8458 - val_loss: 0.6371 - val_accuracy: 0.7116\n",
            "Epoch 32/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3490 - accuracy: 0.8476 - val_loss: 0.6644 - val_accuracy: 0.6919\n",
            "Epoch 33/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3450 - accuracy: 0.8472 - val_loss: 0.6449 - val_accuracy: 0.7214\n",
            "Epoch 34/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8458 - val_loss: 0.6621 - val_accuracy: 0.7116\n",
            "Epoch 35/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8563 - val_loss: 0.7275 - val_accuracy: 0.6754\n",
            "Epoch 36/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3282 - accuracy: 0.8632 - val_loss: 0.6572 - val_accuracy: 0.7181\n",
            "Epoch 37/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8605 - val_loss: 0.6729 - val_accuracy: 0.6984\n",
            "Epoch 38/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 0.8586 - val_loss: 0.6716 - val_accuracy: 0.7116\n",
            "Epoch 39/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.8614 - val_loss: 0.6691 - val_accuracy: 0.7208\n",
            "Epoch 40/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3020 - accuracy: 0.8707 - val_loss: 0.7017 - val_accuracy: 0.7168\n",
            "Epoch 41/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2935 - accuracy: 0.8724 - val_loss: 0.7103 - val_accuracy: 0.6971\n",
            "Epoch 42/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2996 - accuracy: 0.8722 - val_loss: 0.6763 - val_accuracy: 0.7096\n",
            "Epoch 43/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2909 - accuracy: 0.8776 - val_loss: 0.7135 - val_accuracy: 0.7011\n",
            "Epoch 44/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.8801 - val_loss: 0.7143 - val_accuracy: 0.7070\n",
            "Epoch 45/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2886 - accuracy: 0.8780 - val_loss: 0.7098 - val_accuracy: 0.7096\n",
            "Epoch 46/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2849 - accuracy: 0.8789 - val_loss: 0.7255 - val_accuracy: 0.6978\n",
            "Epoch 47/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2801 - accuracy: 0.8885 - val_loss: 0.7015 - val_accuracy: 0.7254\n",
            "Epoch 48/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.8880 - val_loss: 0.7199 - val_accuracy: 0.7221\n",
            "Epoch 49/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2686 - accuracy: 0.8858 - val_loss: 0.7385 - val_accuracy: 0.7089\n",
            "Epoch 50/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2636 - accuracy: 0.8880 - val_loss: 0.7739 - val_accuracy: 0.7017\n",
            "Epoch 51/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2619 - accuracy: 0.8941 - val_loss: 0.7655 - val_accuracy: 0.7011\n",
            "Epoch 52/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.8952 - val_loss: 0.7524 - val_accuracy: 0.7267\n",
            "Epoch 53/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2530 - accuracy: 0.8952 - val_loss: 0.8409 - val_accuracy: 0.6735\n",
            "Epoch 54/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2506 - accuracy: 0.8993 - val_loss: 0.7651 - val_accuracy: 0.7102\n",
            "Epoch 55/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2514 - accuracy: 0.8931 - val_loss: 0.7705 - val_accuracy: 0.6991\n",
            "Epoch 56/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2406 - accuracy: 0.9011 - val_loss: 0.7590 - val_accuracy: 0.7135\n",
            "Epoch 57/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2415 - accuracy: 0.9019 - val_loss: 0.7694 - val_accuracy: 0.7070\n",
            "Epoch 58/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2355 - accuracy: 0.9031 - val_loss: 0.7909 - val_accuracy: 0.7116\n",
            "Epoch 59/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2312 - accuracy: 0.9044 - val_loss: 0.8151 - val_accuracy: 0.6932\n",
            "Epoch 60/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2305 - accuracy: 0.9033 - val_loss: 0.8240 - val_accuracy: 0.6886\n",
            "Epoch 61/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2326 - accuracy: 0.9037 - val_loss: 0.8134 - val_accuracy: 0.7181\n",
            "Epoch 62/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9059 - val_loss: 0.8207 - val_accuracy: 0.7227\n",
            "Epoch 63/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.9083 - val_loss: 0.8333 - val_accuracy: 0.7070\n",
            "Epoch 64/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2108 - accuracy: 0.9146 - val_loss: 0.8409 - val_accuracy: 0.6945\n",
            "Epoch 65/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9121 - val_loss: 0.8322 - val_accuracy: 0.6997\n",
            "Epoch 66/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2127 - accuracy: 0.9115 - val_loss: 0.8643 - val_accuracy: 0.6938\n",
            "Epoch 67/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2047 - accuracy: 0.9213 - val_loss: 0.8445 - val_accuracy: 0.7037\n",
            "Epoch 68/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9120 - val_loss: 0.9157 - val_accuracy: 0.6859\n",
            "Epoch 69/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9205 - val_loss: 0.8802 - val_accuracy: 0.7208\n",
            "Epoch 70/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9210 - val_loss: 0.8607 - val_accuracy: 0.7116\n",
            "Epoch 71/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1941 - accuracy: 0.9226 - val_loss: 0.8786 - val_accuracy: 0.7102\n",
            "Epoch 72/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1956 - accuracy: 0.9180 - val_loss: 0.9255 - val_accuracy: 0.6800\n",
            "Epoch 73/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1938 - accuracy: 0.9228 - val_loss: 0.8845 - val_accuracy: 0.7083\n",
            "Epoch 74/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1918 - accuracy: 0.9262 - val_loss: 0.9075 - val_accuracy: 0.7030\n",
            "Epoch 75/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1834 - accuracy: 0.9274 - val_loss: 0.9522 - val_accuracy: 0.6991\n",
            "Epoch 76/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9292 - val_loss: 0.9204 - val_accuracy: 0.6905\n",
            "Epoch 77/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.9282 - val_loss: 0.9161 - val_accuracy: 0.7070\n",
            "Epoch 78/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.9253 - val_loss: 0.9262 - val_accuracy: 0.7043\n",
            "Epoch 79/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1869 - accuracy: 0.9236 - val_loss: 0.9004 - val_accuracy: 0.7102\n",
            "Epoch 80/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1639 - accuracy: 0.9366 - val_loss: 1.0298 - val_accuracy: 0.6754\n",
            "Epoch 81/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1740 - accuracy: 0.9330 - val_loss: 0.9273 - val_accuracy: 0.7148\n",
            "Epoch 82/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.9363 - val_loss: 0.9538 - val_accuracy: 0.7043\n",
            "Epoch 83/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1761 - accuracy: 0.9302 - val_loss: 0.9966 - val_accuracy: 0.6919\n",
            "Epoch 84/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9338 - val_loss: 0.9413 - val_accuracy: 0.7168\n",
            "Epoch 85/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1669 - accuracy: 0.9323 - val_loss: 0.9653 - val_accuracy: 0.7076\n",
            "Epoch 86/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9338 - val_loss: 0.9886 - val_accuracy: 0.7030\n",
            "Epoch 87/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1658 - accuracy: 0.9341 - val_loss: 0.9817 - val_accuracy: 0.7030\n",
            "Epoch 88/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9366 - val_loss: 0.9881 - val_accuracy: 0.7030\n",
            "Epoch 89/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9377 - val_loss: 0.9972 - val_accuracy: 0.7109\n",
            "Epoch 90/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9414 - val_loss: 1.0223 - val_accuracy: 0.7070\n",
            "Epoch 91/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.9331 - val_loss: 0.9971 - val_accuracy: 0.7070\n",
            "Epoch 92/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9410 - val_loss: 1.0505 - val_accuracy: 0.6997\n",
            "Epoch 93/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1567 - accuracy: 0.9368 - val_loss: 1.0255 - val_accuracy: 0.7030\n",
            "Epoch 94/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9382 - val_loss: 1.0161 - val_accuracy: 0.7116\n",
            "Epoch 95/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9425 - val_loss: 1.1092 - val_accuracy: 0.6932\n",
            "Epoch 96/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9422 - val_loss: 1.0548 - val_accuracy: 0.6899\n",
            "Epoch 97/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.9433 - val_loss: 1.0589 - val_accuracy: 0.7089\n",
            "Epoch 98/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9456 - val_loss: 1.0425 - val_accuracy: 0.7155\n",
            "Epoch 99/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1432 - accuracy: 0.9468 - val_loss: 1.0653 - val_accuracy: 0.7070\n",
            "Epoch 100/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1414 - accuracy: 0.9448 - val_loss: 1.0753 - val_accuracy: 0.7043\n",
            "Epoch 101/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9497 - val_loss: 1.1592 - val_accuracy: 0.6866\n",
            "Epoch 102/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1328 - accuracy: 0.9514 - val_loss: 1.0572 - val_accuracy: 0.7116\n",
            "Epoch 103/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.9456 - val_loss: 1.0838 - val_accuracy: 0.7050\n",
            "Epoch 104/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9433 - val_loss: 1.0399 - val_accuracy: 0.7089\n",
            "Epoch 105/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9450 - val_loss: 1.0552 - val_accuracy: 0.6978\n",
            "Epoch 106/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9514 - val_loss: 1.0593 - val_accuracy: 0.7148\n",
            "Epoch 107/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9465 - val_loss: 1.1091 - val_accuracy: 0.6958\n",
            "Epoch 108/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1341 - accuracy: 0.9527 - val_loss: 1.0837 - val_accuracy: 0.7083\n",
            "Epoch 109/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9466 - val_loss: 1.1089 - val_accuracy: 0.7083\n",
            "Epoch 110/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9469 - val_loss: 1.0858 - val_accuracy: 0.7102\n",
            "Epoch 111/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9502 - val_loss: 1.1045 - val_accuracy: 0.7037\n",
            "Epoch 112/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9491 - val_loss: 1.1232 - val_accuracy: 0.6945\n",
            "Epoch 113/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9450 - val_loss: 1.0835 - val_accuracy: 0.7070\n",
            "Epoch 114/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9563 - val_loss: 1.1134 - val_accuracy: 0.7043\n",
            "Epoch 115/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1236 - accuracy: 0.9520 - val_loss: 1.1306 - val_accuracy: 0.7024\n",
            "Epoch 116/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9563 - val_loss: 1.1714 - val_accuracy: 0.6945\n",
            "Epoch 117/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1265 - accuracy: 0.9517 - val_loss: 1.1423 - val_accuracy: 0.7142\n",
            "Epoch 118/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1146 - accuracy: 0.9566 - val_loss: 1.1640 - val_accuracy: 0.7155\n",
            "Epoch 119/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9573 - val_loss: 1.1525 - val_accuracy: 0.7089\n",
            "Epoch 120/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9542 - val_loss: 1.1289 - val_accuracy: 0.7057\n",
            "Epoch 121/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9571 - val_loss: 1.1733 - val_accuracy: 0.7017\n",
            "Epoch 122/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9566 - val_loss: 1.1736 - val_accuracy: 0.6991\n",
            "Epoch 123/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.9596 - val_loss: 1.2061 - val_accuracy: 0.7129\n",
            "Epoch 124/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9543 - val_loss: 1.2072 - val_accuracy: 0.7024\n",
            "Epoch 125/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9548 - val_loss: 1.1740 - val_accuracy: 0.7129\n",
            "Epoch 126/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1170 - accuracy: 0.9578 - val_loss: 1.1729 - val_accuracy: 0.7076\n",
            "Epoch 127/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.9565 - val_loss: 1.1453 - val_accuracy: 0.7043\n",
            "Epoch 128/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1170 - accuracy: 0.9535 - val_loss: 1.1670 - val_accuracy: 0.7037\n",
            "Epoch 129/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1156 - accuracy: 0.9555 - val_loss: 1.2161 - val_accuracy: 0.7043\n",
            "Epoch 130/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9588 - val_loss: 1.1695 - val_accuracy: 0.6958\n",
            "Epoch 131/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9550 - val_loss: 1.2079 - val_accuracy: 0.7004\n",
            "Epoch 132/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9640 - val_loss: 1.2477 - val_accuracy: 0.7083\n",
            "Epoch 133/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1205 - accuracy: 0.9571 - val_loss: 1.1964 - val_accuracy: 0.7017\n",
            "Epoch 134/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9612 - val_loss: 1.2663 - val_accuracy: 0.7050\n",
            "Epoch 135/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9543 - val_loss: 1.1916 - val_accuracy: 0.7024\n",
            "Epoch 136/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9634 - val_loss: 1.2450 - val_accuracy: 0.6951\n",
            "Epoch 137/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9596 - val_loss: 1.2850 - val_accuracy: 0.7017\n",
            "Epoch 138/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9573 - val_loss: 1.2275 - val_accuracy: 0.7129\n",
            "Epoch 139/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.9655 - val_loss: 1.1788 - val_accuracy: 0.7109\n",
            "Epoch 140/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9639 - val_loss: 1.2584 - val_accuracy: 0.7011\n",
            "Epoch 141/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 1.2372 - val_accuracy: 0.6991\n",
            "Epoch 142/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9622 - val_loss: 1.2477 - val_accuracy: 0.7083\n",
            "Epoch 143/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9596 - val_loss: 1.2738 - val_accuracy: 0.7037\n",
            "Epoch 144/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9640 - val_loss: 1.2841 - val_accuracy: 0.7135\n",
            "Epoch 145/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9624 - val_loss: 1.2469 - val_accuracy: 0.7076\n",
            "Epoch 146/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9602 - val_loss: 1.3049 - val_accuracy: 0.7017\n",
            "Epoch 147/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9642 - val_loss: 1.2397 - val_accuracy: 0.7076\n",
            "Epoch 148/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0962 - accuracy: 0.9642 - val_loss: 1.2642 - val_accuracy: 0.7135\n",
            "Epoch 149/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9607 - val_loss: 1.2727 - val_accuracy: 0.7070\n",
            "Epoch 150/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9632 - val_loss: 1.3283 - val_accuracy: 0.7050\n",
            "Epoch 151/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1015 - accuracy: 0.9611 - val_loss: 1.2597 - val_accuracy: 0.7011\n",
            "Epoch 152/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.9699 - val_loss: 1.2993 - val_accuracy: 0.7089\n",
            "Epoch 153/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9650 - val_loss: 1.3580 - val_accuracy: 0.6965\n",
            "Epoch 154/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9637 - val_loss: 1.2990 - val_accuracy: 0.7017\n",
            "Epoch 155/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9653 - val_loss: 1.3114 - val_accuracy: 0.6997\n",
            "Epoch 156/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9662 - val_loss: 1.3231 - val_accuracy: 0.7214\n",
            "Epoch 157/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0912 - accuracy: 0.9667 - val_loss: 1.4799 - val_accuracy: 0.6833\n",
            "Epoch 158/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9663 - val_loss: 1.3313 - val_accuracy: 0.7096\n",
            "Epoch 159/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9640 - val_loss: 1.3573 - val_accuracy: 0.6965\n",
            "Epoch 160/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9668 - val_loss: 1.3394 - val_accuracy: 0.7076\n",
            "Epoch 161/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9676 - val_loss: 1.3670 - val_accuracy: 0.6965\n",
            "Epoch 162/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9670 - val_loss: 1.3445 - val_accuracy: 0.7057\n",
            "Epoch 163/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9630 - val_loss: 1.3501 - val_accuracy: 0.7037\n",
            "Epoch 164/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9647 - val_loss: 1.3217 - val_accuracy: 0.7057\n",
            "Epoch 165/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0945 - accuracy: 0.9650 - val_loss: 1.3633 - val_accuracy: 0.7089\n",
            "Epoch 166/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9690 - val_loss: 1.3059 - val_accuracy: 0.7024\n",
            "Epoch 167/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9683 - val_loss: 1.3145 - val_accuracy: 0.7201\n",
            "Epoch 168/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9691 - val_loss: 1.3726 - val_accuracy: 0.7116\n",
            "Epoch 169/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.9681 - val_loss: 1.3837 - val_accuracy: 0.7083\n",
            "Epoch 170/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9716 - val_loss: 1.3526 - val_accuracy: 0.7083\n",
            "Epoch 171/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9690 - val_loss: 1.3831 - val_accuracy: 0.7070\n",
            "Epoch 172/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9650 - val_loss: 1.4009 - val_accuracy: 0.7175\n",
            "Epoch 173/200\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9693 - val_loss: 1.4210 - val_accuracy: 0.7096\n",
            "Epoch 174/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.9713 - val_loss: 1.3987 - val_accuracy: 0.6978\n",
            "Epoch 175/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.9694 - val_loss: 1.4091 - val_accuracy: 0.6984\n",
            "Epoch 176/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0845 - accuracy: 0.9671 - val_loss: 1.3993 - val_accuracy: 0.7024\n",
            "Epoch 177/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9688 - val_loss: 1.3846 - val_accuracy: 0.7070\n",
            "Epoch 178/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9727 - val_loss: 1.3920 - val_accuracy: 0.7155\n",
            "Epoch 179/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9711 - val_loss: 1.4033 - val_accuracy: 0.7057\n",
            "Epoch 180/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9694 - val_loss: 1.4015 - val_accuracy: 0.7142\n",
            "Epoch 181/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9660 - val_loss: 1.3482 - val_accuracy: 0.7135\n",
            "Epoch 182/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9644 - val_loss: 1.4149 - val_accuracy: 0.7142\n",
            "Epoch 183/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9698 - val_loss: 1.4004 - val_accuracy: 0.7116\n",
            "Epoch 184/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9690 - val_loss: 1.4169 - val_accuracy: 0.7050\n",
            "Epoch 185/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9706 - val_loss: 1.4128 - val_accuracy: 0.7089\n",
            "Epoch 186/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9680 - val_loss: 1.3908 - val_accuracy: 0.7148\n",
            "Epoch 187/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9696 - val_loss: 1.4380 - val_accuracy: 0.7057\n",
            "Epoch 188/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9734 - val_loss: 1.4519 - val_accuracy: 0.6978\n",
            "Epoch 189/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 1.4769 - val_accuracy: 0.6912\n",
            "Epoch 190/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9681 - val_loss: 1.4221 - val_accuracy: 0.6991\n",
            "Epoch 191/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9699 - val_loss: 1.4263 - val_accuracy: 0.7070\n",
            "Epoch 192/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0788 - accuracy: 0.9714 - val_loss: 1.4859 - val_accuracy: 0.7102\n",
            "Epoch 193/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9721 - val_loss: 1.4550 - val_accuracy: 0.7070\n",
            "Epoch 194/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9734 - val_loss: 1.4578 - val_accuracy: 0.7096\n",
            "Epoch 195/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9726 - val_loss: 1.4498 - val_accuracy: 0.7135\n",
            "Epoch 196/200\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9694 - val_loss: 1.4974 - val_accuracy: 0.6997\n",
            "Epoch 197/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9699 - val_loss: 1.4617 - val_accuracy: 0.7030\n",
            "Epoch 198/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9726 - val_loss: 1.4509 - val_accuracy: 0.7043\n",
            "Epoch 199/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9691 - val_loss: 1.4083 - val_accuracy: 0.7057\n",
            "Epoch 200/200\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0850 - accuracy: 0.9721 - val_loss: 1.4265 - val_accuracy: 0.7076\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 1.4517 - accuracy: 0.7091\n",
            "This is for epoch with a value of 200\n",
            "Valid accuracy: 0.7091052532196045\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 0.6763 - accuracy: 0.5994 - val_loss: 0.6332 - val_accuracy: 0.6452\n",
            "Epoch 2/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6157 - accuracy: 0.6623 - val_loss: 0.6195 - val_accuracy: 0.6728\n",
            "Epoch 3/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6003 - accuracy: 0.6812 - val_loss: 0.6014 - val_accuracy: 0.6807\n",
            "Epoch 4/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5912 - accuracy: 0.6937 - val_loss: 0.5950 - val_accuracy: 0.6833\n",
            "Epoch 5/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5747 - accuracy: 0.7061 - val_loss: 0.6139 - val_accuracy: 0.6682\n",
            "Epoch 6/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5594 - accuracy: 0.7216 - val_loss: 0.5853 - val_accuracy: 0.6912\n",
            "Epoch 7/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5493 - accuracy: 0.7250 - val_loss: 0.5817 - val_accuracy: 0.6997\n",
            "Epoch 8/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.7349 - val_loss: 0.5784 - val_accuracy: 0.7089\n",
            "Epoch 9/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5235 - accuracy: 0.7446 - val_loss: 0.5829 - val_accuracy: 0.7024\n",
            "Epoch 10/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5138 - accuracy: 0.7515 - val_loss: 0.5683 - val_accuracy: 0.7011\n",
            "Epoch 11/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5029 - accuracy: 0.7513 - val_loss: 0.5688 - val_accuracy: 0.7089\n",
            "Epoch 12/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5013 - accuracy: 0.7536 - val_loss: 0.5594 - val_accuracy: 0.7168\n",
            "Epoch 13/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4890 - accuracy: 0.7658 - val_loss: 0.6117 - val_accuracy: 0.6800\n",
            "Epoch 14/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.7704 - val_loss: 0.5805 - val_accuracy: 0.7201\n",
            "Epoch 15/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4676 - accuracy: 0.7804 - val_loss: 0.5600 - val_accuracy: 0.7280\n",
            "Epoch 16/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7792 - val_loss: 0.5682 - val_accuracy: 0.7214\n",
            "Epoch 17/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.7860 - val_loss: 0.5558 - val_accuracy: 0.7313\n",
            "Epoch 18/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.7934 - val_loss: 0.5597 - val_accuracy: 0.7286\n",
            "Epoch 19/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4448 - accuracy: 0.7911 - val_loss: 0.5666 - val_accuracy: 0.7273\n",
            "Epoch 20/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.7989 - val_loss: 0.5990 - val_accuracy: 0.7162\n",
            "Epoch 21/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4301 - accuracy: 0.8004 - val_loss: 0.5686 - val_accuracy: 0.7240\n",
            "Epoch 22/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4214 - accuracy: 0.8083 - val_loss: 0.5721 - val_accuracy: 0.7332\n",
            "Epoch 23/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4176 - accuracy: 0.8095 - val_loss: 0.5840 - val_accuracy: 0.7201\n",
            "Epoch 24/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4145 - accuracy: 0.8096 - val_loss: 0.5924 - val_accuracy: 0.7162\n",
            "Epoch 25/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4059 - accuracy: 0.8165 - val_loss: 0.5789 - val_accuracy: 0.7175\n",
            "Epoch 26/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3962 - accuracy: 0.8270 - val_loss: 0.5836 - val_accuracy: 0.7286\n",
            "Epoch 27/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.8318 - val_loss: 0.6523 - val_accuracy: 0.6945\n",
            "Epoch 28/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.8288 - val_loss: 0.5900 - val_accuracy: 0.7306\n",
            "Epoch 29/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.8377 - val_loss: 0.5860 - val_accuracy: 0.7221\n",
            "Epoch 30/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8392 - val_loss: 0.6074 - val_accuracy: 0.7194\n",
            "Epoch 31/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8390 - val_loss: 0.5884 - val_accuracy: 0.7221\n",
            "Epoch 32/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8389 - val_loss: 0.5948 - val_accuracy: 0.7240\n",
            "Epoch 33/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3516 - accuracy: 0.8463 - val_loss: 0.6053 - val_accuracy: 0.7227\n",
            "Epoch 34/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8490 - val_loss: 0.6126 - val_accuracy: 0.7240\n",
            "Epoch 35/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3432 - accuracy: 0.8538 - val_loss: 0.6231 - val_accuracy: 0.7096\n",
            "Epoch 36/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3400 - accuracy: 0.8550 - val_loss: 0.6286 - val_accuracy: 0.7280\n",
            "Epoch 37/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8556 - val_loss: 0.6061 - val_accuracy: 0.7240\n",
            "Epoch 38/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8546 - val_loss: 0.6153 - val_accuracy: 0.7247\n",
            "Epoch 39/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8670 - val_loss: 0.6201 - val_accuracy: 0.7208\n",
            "Epoch 40/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8668 - val_loss: 0.6163 - val_accuracy: 0.7234\n",
            "Epoch 41/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8656 - val_loss: 0.6421 - val_accuracy: 0.7293\n",
            "Epoch 42/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8678 - val_loss: 0.6586 - val_accuracy: 0.7175\n",
            "Epoch 43/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8709 - val_loss: 0.6613 - val_accuracy: 0.7208\n",
            "Epoch 44/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3064 - accuracy: 0.8720 - val_loss: 0.6458 - val_accuracy: 0.7181\n",
            "Epoch 45/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8753 - val_loss: 0.6664 - val_accuracy: 0.7227\n",
            "Epoch 46/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2924 - accuracy: 0.8803 - val_loss: 0.6470 - val_accuracy: 0.7313\n",
            "Epoch 47/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2884 - accuracy: 0.8761 - val_loss: 0.6516 - val_accuracy: 0.7227\n",
            "Epoch 48/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2882 - accuracy: 0.8760 - val_loss: 0.6538 - val_accuracy: 0.7254\n",
            "Epoch 49/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2871 - accuracy: 0.8780 - val_loss: 0.6543 - val_accuracy: 0.7260\n",
            "Epoch 50/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.8799 - val_loss: 0.6788 - val_accuracy: 0.7168\n",
            "Epoch 51/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.8899 - val_loss: 0.6618 - val_accuracy: 0.7135\n",
            "Epoch 52/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.8873 - val_loss: 0.7057 - val_accuracy: 0.7096\n",
            "Epoch 53/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2691 - accuracy: 0.8798 - val_loss: 0.7145 - val_accuracy: 0.7168\n",
            "Epoch 54/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2597 - accuracy: 0.8929 - val_loss: 0.6916 - val_accuracy: 0.7083\n",
            "Epoch 55/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2721 - accuracy: 0.8839 - val_loss: 0.6847 - val_accuracy: 0.7227\n",
            "Epoch 56/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2520 - accuracy: 0.8964 - val_loss: 0.7195 - val_accuracy: 0.7129\n",
            "Epoch 57/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2521 - accuracy: 0.8950 - val_loss: 0.7033 - val_accuracy: 0.7083\n",
            "Epoch 58/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2508 - accuracy: 0.8980 - val_loss: 0.7388 - val_accuracy: 0.7116\n",
            "Epoch 59/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9029 - val_loss: 0.7566 - val_accuracy: 0.7135\n",
            "Epoch 60/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2512 - accuracy: 0.8952 - val_loss: 0.7278 - val_accuracy: 0.7240\n",
            "Epoch 61/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9056 - val_loss: 0.7309 - val_accuracy: 0.7168\n",
            "Epoch 62/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2413 - accuracy: 0.9029 - val_loss: 0.7372 - val_accuracy: 0.7254\n",
            "Epoch 63/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.8988 - val_loss: 0.7387 - val_accuracy: 0.7135\n",
            "Epoch 64/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2431 - accuracy: 0.8993 - val_loss: 0.7487 - val_accuracy: 0.7181\n",
            "Epoch 65/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2330 - accuracy: 0.9026 - val_loss: 0.7551 - val_accuracy: 0.7129\n",
            "Epoch 66/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2311 - accuracy: 0.9051 - val_loss: 0.7595 - val_accuracy: 0.7116\n",
            "Epoch 67/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2181 - accuracy: 0.9083 - val_loss: 0.7689 - val_accuracy: 0.7221\n",
            "Epoch 68/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2257 - accuracy: 0.9072 - val_loss: 0.7379 - val_accuracy: 0.7194\n",
            "Epoch 69/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2249 - accuracy: 0.9120 - val_loss: 0.7877 - val_accuracy: 0.7142\n",
            "Epoch 70/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.2156 - accuracy: 0.9143 - val_loss: 0.7641 - val_accuracy: 0.7175\n",
            "Epoch 71/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2164 - accuracy: 0.9131 - val_loss: 0.7539 - val_accuracy: 0.7089\n",
            "Epoch 72/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2092 - accuracy: 0.9157 - val_loss: 0.8170 - val_accuracy: 0.7155\n",
            "Epoch 73/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2121 - accuracy: 0.9131 - val_loss: 0.8098 - val_accuracy: 0.7096\n",
            "Epoch 74/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2089 - accuracy: 0.9166 - val_loss: 0.7739 - val_accuracy: 0.7181\n",
            "Epoch 75/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2023 - accuracy: 0.9177 - val_loss: 0.8044 - val_accuracy: 0.7194\n",
            "Epoch 76/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1971 - accuracy: 0.9205 - val_loss: 0.8367 - val_accuracy: 0.7181\n",
            "Epoch 77/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9187 - val_loss: 0.8178 - val_accuracy: 0.7214\n",
            "Epoch 78/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2029 - accuracy: 0.9205 - val_loss: 0.8048 - val_accuracy: 0.7273\n",
            "Epoch 79/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2006 - accuracy: 0.9193 - val_loss: 0.8090 - val_accuracy: 0.7155\n",
            "Epoch 80/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9276 - val_loss: 0.8291 - val_accuracy: 0.7234\n",
            "Epoch 81/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1862 - accuracy: 0.9239 - val_loss: 0.8649 - val_accuracy: 0.7162\n",
            "Epoch 82/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1853 - accuracy: 0.9284 - val_loss: 0.8337 - val_accuracy: 0.7181\n",
            "Epoch 83/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1806 - accuracy: 0.9292 - val_loss: 0.8606 - val_accuracy: 0.7194\n",
            "Epoch 84/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1830 - accuracy: 0.9262 - val_loss: 0.8550 - val_accuracy: 0.7148\n",
            "Epoch 85/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1789 - accuracy: 0.9313 - val_loss: 0.8693 - val_accuracy: 0.7109\n",
            "Epoch 86/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1711 - accuracy: 0.9333 - val_loss: 0.8638 - val_accuracy: 0.7096\n",
            "Epoch 87/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1803 - accuracy: 0.9304 - val_loss: 0.8846 - val_accuracy: 0.7201\n",
            "Epoch 88/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9327 - val_loss: 0.8588 - val_accuracy: 0.7208\n",
            "Epoch 89/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9358 - val_loss: 0.8688 - val_accuracy: 0.7260\n",
            "Epoch 90/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9341 - val_loss: 0.9256 - val_accuracy: 0.7155\n",
            "Epoch 91/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9346 - val_loss: 0.9256 - val_accuracy: 0.7155\n",
            "Epoch 92/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1635 - accuracy: 0.9354 - val_loss: 0.9182 - val_accuracy: 0.7214\n",
            "Epoch 93/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9404 - val_loss: 0.9120 - val_accuracy: 0.7116\n",
            "Epoch 94/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.9386 - val_loss: 0.9208 - val_accuracy: 0.7011\n",
            "Epoch 95/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1649 - accuracy: 0.9363 - val_loss: 0.9054 - val_accuracy: 0.7148\n",
            "Epoch 96/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9374 - val_loss: 0.8937 - val_accuracy: 0.7142\n",
            "Epoch 97/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9397 - val_loss: 0.9078 - val_accuracy: 0.7181\n",
            "Epoch 98/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9382 - val_loss: 0.9519 - val_accuracy: 0.7254\n",
            "Epoch 99/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9414 - val_loss: 0.9440 - val_accuracy: 0.7201\n",
            "Epoch 100/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1456 - accuracy: 0.9428 - val_loss: 0.9675 - val_accuracy: 0.7286\n",
            "Epoch 101/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9384 - val_loss: 0.9441 - val_accuracy: 0.7214\n",
            "Epoch 102/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1558 - accuracy: 0.9384 - val_loss: 0.9646 - val_accuracy: 0.7221\n",
            "Epoch 103/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9404 - val_loss: 0.9397 - val_accuracy: 0.7201\n",
            "Epoch 104/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9417 - val_loss: 0.9248 - val_accuracy: 0.7194\n",
            "Epoch 105/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9419 - val_loss: 0.9540 - val_accuracy: 0.7155\n",
            "Epoch 106/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.9468 - val_loss: 0.9556 - val_accuracy: 0.7102\n",
            "Epoch 107/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9469 - val_loss: 0.9600 - val_accuracy: 0.7240\n",
            "Epoch 108/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9460 - val_loss: 0.9724 - val_accuracy: 0.7201\n",
            "Epoch 109/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9491 - val_loss: 0.9789 - val_accuracy: 0.7240\n",
            "Epoch 110/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9484 - val_loss: 0.9561 - val_accuracy: 0.7254\n",
            "Epoch 111/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9517 - val_loss: 0.9730 - val_accuracy: 0.7247\n",
            "Epoch 112/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9466 - val_loss: 0.9934 - val_accuracy: 0.7221\n",
            "Epoch 113/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9492 - val_loss: 0.9682 - val_accuracy: 0.7280\n",
            "Epoch 114/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9465 - val_loss: 1.0035 - val_accuracy: 0.7201\n",
            "Epoch 115/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1310 - accuracy: 0.9534 - val_loss: 0.9982 - val_accuracy: 0.7227\n",
            "Epoch 116/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1430 - accuracy: 0.9469 - val_loss: 0.9817 - val_accuracy: 0.7148\n",
            "Epoch 117/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.9524 - val_loss: 1.0302 - val_accuracy: 0.7063\n",
            "Epoch 118/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1246 - accuracy: 0.9524 - val_loss: 0.9875 - val_accuracy: 0.7234\n",
            "Epoch 119/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9468 - val_loss: 0.9830 - val_accuracy: 0.7214\n",
            "Epoch 120/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9542 - val_loss: 1.0235 - val_accuracy: 0.7260\n",
            "Epoch 121/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9504 - val_loss: 1.0168 - val_accuracy: 0.7247\n",
            "Epoch 122/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9547 - val_loss: 1.0315 - val_accuracy: 0.7293\n",
            "Epoch 123/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9557 - val_loss: 1.0373 - val_accuracy: 0.7188\n",
            "Epoch 124/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9512 - val_loss: 1.0452 - val_accuracy: 0.7260\n",
            "Epoch 125/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9535 - val_loss: 1.0481 - val_accuracy: 0.7247\n",
            "Epoch 126/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9555 - val_loss: 1.0691 - val_accuracy: 0.7286\n",
            "Epoch 127/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9522 - val_loss: 1.0387 - val_accuracy: 0.7247\n",
            "Epoch 128/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9553 - val_loss: 1.0559 - val_accuracy: 0.7162\n",
            "Epoch 129/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1155 - accuracy: 0.9553 - val_loss: 1.0742 - val_accuracy: 0.7168\n",
            "Epoch 130/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9573 - val_loss: 1.0478 - val_accuracy: 0.7319\n",
            "Epoch 131/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.9547 - val_loss: 1.1035 - val_accuracy: 0.7208\n",
            "Epoch 132/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9543 - val_loss: 1.0573 - val_accuracy: 0.7247\n",
            "Epoch 133/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9586 - val_loss: 1.0833 - val_accuracy: 0.7188\n",
            "Epoch 134/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9558 - val_loss: 1.0732 - val_accuracy: 0.7240\n",
            "Epoch 135/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9586 - val_loss: 1.0761 - val_accuracy: 0.7254\n",
            "Epoch 136/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1176 - accuracy: 0.9568 - val_loss: 1.1093 - val_accuracy: 0.7129\n",
            "Epoch 137/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1097 - accuracy: 0.9593 - val_loss: 1.1181 - val_accuracy: 0.7168\n",
            "Epoch 138/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9565 - val_loss: 1.1046 - val_accuracy: 0.7260\n",
            "Epoch 139/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1129 - accuracy: 0.9588 - val_loss: 1.1442 - val_accuracy: 0.7122\n",
            "Epoch 140/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9589 - val_loss: 1.1467 - val_accuracy: 0.7221\n",
            "Epoch 141/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1131 - accuracy: 0.9591 - val_loss: 1.1657 - val_accuracy: 0.7234\n",
            "Epoch 142/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9594 - val_loss: 1.1196 - val_accuracy: 0.7273\n",
            "Epoch 143/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1075 - accuracy: 0.9607 - val_loss: 1.1432 - val_accuracy: 0.7142\n",
            "Epoch 144/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9629 - val_loss: 1.1370 - val_accuracy: 0.7227\n",
            "Epoch 145/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9604 - val_loss: 1.1698 - val_accuracy: 0.7168\n",
            "Epoch 146/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9601 - val_loss: 1.1888 - val_accuracy: 0.7122\n",
            "Epoch 147/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9611 - val_loss: 1.1597 - val_accuracy: 0.7155\n",
            "Epoch 148/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9621 - val_loss: 1.2686 - val_accuracy: 0.7050\n",
            "Epoch 149/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9644 - val_loss: 1.1442 - val_accuracy: 0.7142\n",
            "Epoch 150/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9612 - val_loss: 1.1863 - val_accuracy: 0.7221\n",
            "Epoch 151/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9584 - val_loss: 1.1907 - val_accuracy: 0.7116\n",
            "Epoch 152/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.9698 - val_loss: 1.1895 - val_accuracy: 0.7201\n",
            "Epoch 153/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9648 - val_loss: 1.2102 - val_accuracy: 0.7280\n",
            "Epoch 154/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1011 - accuracy: 0.9634 - val_loss: 1.1691 - val_accuracy: 0.7227\n",
            "Epoch 155/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9622 - val_loss: 1.1910 - val_accuracy: 0.7188\n",
            "Epoch 156/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1017 - accuracy: 0.9622 - val_loss: 1.2021 - val_accuracy: 0.7260\n",
            "Epoch 157/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1048 - accuracy: 0.9619 - val_loss: 1.2335 - val_accuracy: 0.7148\n",
            "Epoch 158/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1095 - accuracy: 0.9593 - val_loss: 1.1946 - val_accuracy: 0.7181\n",
            "Epoch 159/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9639 - val_loss: 1.2271 - val_accuracy: 0.7234\n",
            "Epoch 160/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9647 - val_loss: 1.2346 - val_accuracy: 0.7254\n",
            "Epoch 161/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0933 - accuracy: 0.9657 - val_loss: 1.2141 - val_accuracy: 0.7162\n",
            "Epoch 162/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9690 - val_loss: 1.1963 - val_accuracy: 0.7208\n",
            "Epoch 163/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1009 - accuracy: 0.9612 - val_loss: 1.2370 - val_accuracy: 0.7089\n",
            "Epoch 164/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0849 - accuracy: 0.9693 - val_loss: 1.2371 - val_accuracy: 0.7240\n",
            "Epoch 165/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.9676 - val_loss: 1.2643 - val_accuracy: 0.7083\n",
            "Epoch 166/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0900 - accuracy: 0.9681 - val_loss: 1.2284 - val_accuracy: 0.7201\n",
            "Epoch 167/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 0.9667 - val_loss: 1.2997 - val_accuracy: 0.7168\n",
            "Epoch 168/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0938 - accuracy: 0.9648 - val_loss: 1.2395 - val_accuracy: 0.7175\n",
            "Epoch 169/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9658 - val_loss: 1.1830 - val_accuracy: 0.7135\n",
            "Epoch 170/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.9658 - val_loss: 1.2413 - val_accuracy: 0.7181\n",
            "Epoch 171/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0906 - accuracy: 0.9680 - val_loss: 1.2577 - val_accuracy: 0.7194\n",
            "Epoch 172/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9678 - val_loss: 1.2651 - val_accuracy: 0.7227\n",
            "Epoch 173/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9657 - val_loss: 1.2954 - val_accuracy: 0.7175\n",
            "Epoch 174/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9670 - val_loss: 1.2887 - val_accuracy: 0.7148\n",
            "Epoch 175/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9678 - val_loss: 1.2902 - val_accuracy: 0.7175\n",
            "Epoch 176/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9693 - val_loss: 1.3188 - val_accuracy: 0.7208\n",
            "Epoch 177/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9681 - val_loss: 1.2722 - val_accuracy: 0.7148\n",
            "Epoch 178/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9713 - val_loss: 1.3029 - val_accuracy: 0.7227\n",
            "Epoch 179/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9688 - val_loss: 1.3019 - val_accuracy: 0.7221\n",
            "Epoch 180/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0894 - accuracy: 0.9713 - val_loss: 1.2834 - val_accuracy: 0.7148\n",
            "Epoch 181/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9698 - val_loss: 1.3577 - val_accuracy: 0.7050\n",
            "Epoch 182/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0883 - accuracy: 0.9698 - val_loss: 1.2786 - val_accuracy: 0.7162\n",
            "Epoch 183/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9709 - val_loss: 1.2625 - val_accuracy: 0.7214\n",
            "Epoch 184/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0912 - accuracy: 0.9676 - val_loss: 1.3287 - val_accuracy: 0.7181\n",
            "Epoch 185/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0869 - accuracy: 0.9691 - val_loss: 1.2923 - val_accuracy: 0.7194\n",
            "Epoch 186/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0865 - accuracy: 0.9688 - val_loss: 1.3098 - val_accuracy: 0.7201\n",
            "Epoch 187/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9691 - val_loss: 1.3053 - val_accuracy: 0.7122\n",
            "Epoch 188/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9744 - val_loss: 1.3588 - val_accuracy: 0.7194\n",
            "Epoch 189/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9694 - val_loss: 1.3569 - val_accuracy: 0.7148\n",
            "Epoch 190/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9655 - val_loss: 1.3296 - val_accuracy: 0.7168\n",
            "Epoch 191/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9704 - val_loss: 1.3090 - val_accuracy: 0.7280\n",
            "Epoch 192/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9742 - val_loss: 1.4085 - val_accuracy: 0.7188\n",
            "Epoch 193/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9688 - val_loss: 1.3553 - val_accuracy: 0.7148\n",
            "Epoch 194/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9711 - val_loss: 1.3389 - val_accuracy: 0.7155\n",
            "Epoch 195/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9683 - val_loss: 1.4075 - val_accuracy: 0.7122\n",
            "Epoch 196/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9701 - val_loss: 1.3677 - val_accuracy: 0.7181\n",
            "Epoch 197/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9714 - val_loss: 1.3556 - val_accuracy: 0.7273\n",
            "Epoch 198/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9721 - val_loss: 1.3578 - val_accuracy: 0.7273\n",
            "Epoch 199/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9709 - val_loss: 1.3606 - val_accuracy: 0.7142\n",
            "Epoch 200/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.9731 - val_loss: 1.3813 - val_accuracy: 0.7227\n",
            "Epoch 201/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9724 - val_loss: 1.4028 - val_accuracy: 0.7194\n",
            "Epoch 202/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9726 - val_loss: 1.4039 - val_accuracy: 0.7214\n",
            "Epoch 203/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9717 - val_loss: 1.3967 - val_accuracy: 0.7194\n",
            "Epoch 204/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9706 - val_loss: 1.3780 - val_accuracy: 0.7201\n",
            "Epoch 205/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9704 - val_loss: 1.3765 - val_accuracy: 0.7155\n",
            "Epoch 206/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9713 - val_loss: 1.3536 - val_accuracy: 0.7201\n",
            "Epoch 207/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0793 - accuracy: 0.9716 - val_loss: 1.4865 - val_accuracy: 0.7083\n",
            "Epoch 208/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9734 - val_loss: 1.4119 - val_accuracy: 0.7181\n",
            "Epoch 209/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9747 - val_loss: 1.3842 - val_accuracy: 0.7142\n",
            "Epoch 210/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.9693 - val_loss: 1.3285 - val_accuracy: 0.7254\n",
            "Epoch 211/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9714 - val_loss: 1.4117 - val_accuracy: 0.7083\n",
            "Epoch 212/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9717 - val_loss: 1.3731 - val_accuracy: 0.7162\n",
            "Epoch 213/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9749 - val_loss: 1.3829 - val_accuracy: 0.7188\n",
            "Epoch 214/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9719 - val_loss: 1.4177 - val_accuracy: 0.7162\n",
            "Epoch 215/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9696 - val_loss: 1.3977 - val_accuracy: 0.7116\n",
            "Epoch 216/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9732 - val_loss: 1.4290 - val_accuracy: 0.7208\n",
            "Epoch 217/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9716 - val_loss: 1.4577 - val_accuracy: 0.7247\n",
            "Epoch 218/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0723 - accuracy: 0.9732 - val_loss: 1.4139 - val_accuracy: 0.7096\n",
            "Epoch 219/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9740 - val_loss: 1.4174 - val_accuracy: 0.7227\n",
            "Epoch 220/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9724 - val_loss: 1.4538 - val_accuracy: 0.7201\n",
            "Epoch 221/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9750 - val_loss: 1.4108 - val_accuracy: 0.7208\n",
            "Epoch 222/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9716 - val_loss: 1.4089 - val_accuracy: 0.7227\n",
            "Epoch 223/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9767 - val_loss: 1.4527 - val_accuracy: 0.7221\n",
            "Epoch 224/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9749 - val_loss: 1.4561 - val_accuracy: 0.7221\n",
            "Epoch 225/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0673 - accuracy: 0.9740 - val_loss: 1.4500 - val_accuracy: 0.7188\n",
            "Epoch 226/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9755 - val_loss: 1.4625 - val_accuracy: 0.7116\n",
            "Epoch 227/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9754 - val_loss: 1.4642 - val_accuracy: 0.7155\n",
            "Epoch 228/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9759 - val_loss: 1.4323 - val_accuracy: 0.7175\n",
            "Epoch 229/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 1.4431 - val_accuracy: 0.7201\n",
            "Epoch 230/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.9713 - val_loss: 1.4810 - val_accuracy: 0.7194\n",
            "Epoch 231/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9734 - val_loss: 1.4609 - val_accuracy: 0.7135\n",
            "Epoch 232/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9768 - val_loss: 1.4895 - val_accuracy: 0.7181\n",
            "Epoch 233/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9777 - val_loss: 1.5479 - val_accuracy: 0.7063\n",
            "Epoch 234/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9742 - val_loss: 1.5502 - val_accuracy: 0.7109\n",
            "Epoch 235/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.9732 - val_loss: 1.5194 - val_accuracy: 0.7089\n",
            "Epoch 236/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9739 - val_loss: 1.4674 - val_accuracy: 0.7070\n",
            "Epoch 237/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9757 - val_loss: 1.4815 - val_accuracy: 0.7142\n",
            "Epoch 238/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9729 - val_loss: 1.5212 - val_accuracy: 0.7030\n",
            "Epoch 239/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0662 - accuracy: 0.9757 - val_loss: 1.5171 - val_accuracy: 0.7214\n",
            "Epoch 240/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.9755 - val_loss: 1.5130 - val_accuracy: 0.7148\n",
            "Epoch 241/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0653 - accuracy: 0.9755 - val_loss: 1.5912 - val_accuracy: 0.7076\n",
            "Epoch 242/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9737 - val_loss: 1.4311 - val_accuracy: 0.7168\n",
            "Epoch 243/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9745 - val_loss: 1.4941 - val_accuracy: 0.7234\n",
            "Epoch 244/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0690 - accuracy: 0.9767 - val_loss: 1.4984 - val_accuracy: 0.7175\n",
            "Epoch 245/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9737 - val_loss: 1.4888 - val_accuracy: 0.7194\n",
            "Epoch 246/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9785 - val_loss: 1.5558 - val_accuracy: 0.7076\n",
            "Epoch 247/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9754 - val_loss: 1.5296 - val_accuracy: 0.7129\n",
            "Epoch 248/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.9767 - val_loss: 1.5266 - val_accuracy: 0.7168\n",
            "Epoch 249/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9752 - val_loss: 1.5365 - val_accuracy: 0.7122\n",
            "Epoch 250/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0666 - accuracy: 0.9763 - val_loss: 1.5242 - val_accuracy: 0.7096\n",
            "Epoch 251/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9765 - val_loss: 1.6258 - val_accuracy: 0.6997\n",
            "Epoch 252/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 1.4907 - val_accuracy: 0.7234\n",
            "Epoch 253/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9744 - val_loss: 1.6120 - val_accuracy: 0.7030\n",
            "Epoch 254/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9745 - val_loss: 1.5078 - val_accuracy: 0.7155\n",
            "Epoch 255/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9772 - val_loss: 1.5510 - val_accuracy: 0.7168\n",
            "Epoch 256/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9760 - val_loss: 1.5230 - val_accuracy: 0.7076\n",
            "Epoch 257/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9778 - val_loss: 1.5196 - val_accuracy: 0.7135\n",
            "Epoch 258/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9788 - val_loss: 1.5952 - val_accuracy: 0.7175\n",
            "Epoch 259/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9747 - val_loss: 1.5397 - val_accuracy: 0.7129\n",
            "Epoch 260/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 1.5549 - val_accuracy: 0.7057\n",
            "Epoch 261/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0611 - accuracy: 0.9760 - val_loss: 1.5682 - val_accuracy: 0.7070\n",
            "Epoch 262/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9783 - val_loss: 1.5743 - val_accuracy: 0.7050\n",
            "Epoch 263/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9798 - val_loss: 1.5914 - val_accuracy: 0.7102\n",
            "Epoch 264/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9778 - val_loss: 1.6111 - val_accuracy: 0.7116\n",
            "Epoch 265/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9762 - val_loss: 1.5887 - val_accuracy: 0.7057\n",
            "Epoch 266/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9772 - val_loss: 1.5407 - val_accuracy: 0.7096\n",
            "Epoch 267/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.9767 - val_loss: 1.5934 - val_accuracy: 0.7188\n",
            "Epoch 268/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9778 - val_loss: 1.5766 - val_accuracy: 0.7083\n",
            "Epoch 269/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9757 - val_loss: 1.5741 - val_accuracy: 0.7247\n",
            "Epoch 270/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9757 - val_loss: 1.5782 - val_accuracy: 0.7181\n",
            "Epoch 271/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 0.9762 - val_loss: 1.5872 - val_accuracy: 0.7070\n",
            "Epoch 272/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9782 - val_loss: 1.5307 - val_accuracy: 0.7221\n",
            "Epoch 273/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0627 - accuracy: 0.9780 - val_loss: 1.6357 - val_accuracy: 0.7148\n",
            "Epoch 274/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.9773 - val_loss: 1.5337 - val_accuracy: 0.7208\n",
            "Epoch 275/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9786 - val_loss: 1.5814 - val_accuracy: 0.7188\n",
            "Epoch 276/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9736 - val_loss: 1.5658 - val_accuracy: 0.7089\n",
            "Epoch 277/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 1.6899 - val_accuracy: 0.7024\n",
            "Epoch 278/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9755 - val_loss: 1.5949 - val_accuracy: 0.7043\n",
            "Epoch 279/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9772 - val_loss: 1.5730 - val_accuracy: 0.7221\n",
            "Epoch 280/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9778 - val_loss: 1.6229 - val_accuracy: 0.7148\n",
            "Epoch 281/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.9788 - val_loss: 1.6054 - val_accuracy: 0.7037\n",
            "Epoch 282/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9790 - val_loss: 1.5878 - val_accuracy: 0.7155\n",
            "Epoch 283/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9783 - val_loss: 1.6104 - val_accuracy: 0.7129\n",
            "Epoch 284/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9767 - val_loss: 1.5950 - val_accuracy: 0.7102\n",
            "Epoch 285/300\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9770 - val_loss: 1.6162 - val_accuracy: 0.7181\n",
            "Epoch 286/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0582 - accuracy: 0.9796 - val_loss: 1.6533 - val_accuracy: 0.7063\n",
            "Epoch 287/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 1.6828 - val_accuracy: 0.7142\n",
            "Epoch 288/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9791 - val_loss: 1.6155 - val_accuracy: 0.7168\n",
            "Epoch 289/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0615 - accuracy: 0.9791 - val_loss: 1.5841 - val_accuracy: 0.7240\n",
            "Epoch 290/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9803 - val_loss: 1.6178 - val_accuracy: 0.7109\n",
            "Epoch 291/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9765 - val_loss: 1.5990 - val_accuracy: 0.7089\n",
            "Epoch 292/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0539 - accuracy: 0.9791 - val_loss: 1.6150 - val_accuracy: 0.7109\n",
            "Epoch 293/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0601 - accuracy: 0.9783 - val_loss: 1.6070 - val_accuracy: 0.7109\n",
            "Epoch 294/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0543 - accuracy: 0.9814 - val_loss: 1.6896 - val_accuracy: 0.7116\n",
            "Epoch 295/300\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0620 - accuracy: 0.9777 - val_loss: 1.7022 - val_accuracy: 0.7063\n",
            "Epoch 296/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9818 - val_loss: 1.6465 - val_accuracy: 0.7109\n",
            "Epoch 297/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9772 - val_loss: 1.6851 - val_accuracy: 0.7089\n",
            "Epoch 298/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0619 - accuracy: 0.9778 - val_loss: 1.6742 - val_accuracy: 0.7168\n",
            "Epoch 299/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9785 - val_loss: 1.6603 - val_accuracy: 0.7070\n",
            "Epoch 300/300\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9772 - val_loss: 1.6831 - val_accuracy: 0.7168\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 1.5538 - accuracy: 0.7197\n",
            "This is for epoch with a value of 300\n",
            "Valid accuracy: 0.7197477221488953\n"
          ]
        }
      ],
      "source": [
        "num_epoches = [25, 50, 100, 200, 300]\n",
        "\n",
        "valid_acc_epoch = {}\n",
        "for epoch in num_epoches:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "                name='dense_layer1', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hiddens,\n",
        "    name='dense_layer_2', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    model.compile(optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=batch_size, epochs=epoch,\n",
        "    verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_epoch_valid_acc, NN_epoch_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for epoch with a value of %i\" % epoch)\n",
        "    print('Valid accuracy:', NN_epoch_valid_acc)\n",
        "\n",
        "    valid_acc_epoch[epoch] = NN_epoch_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX32XFvPwH2H",
        "outputId": "c760ddda-46be-4b97-b630-faed7b456ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{25: 0.7406385540962219, 50: 0.7335435748100281, 100: 0.7217185497283936, 200: 0.7091052532196045, 300: 0.7197477221488953}\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_epoch)\n",
        "epoch = max(valid_acc_epoch, key=valid_acc_epoch.get)\n",
        "print(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTJn8wie92Q"
      },
      "source": [
        "#### 4.2.6 - Number of batch size\n",
        "batch_size = [32, 64, 128, 256, 512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psq5s1GXe92Q",
        "outputId": "4ebe8e8a-b3ee-4a80-eccd-bc9b665c5915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6270 - val_loss: 0.6163 - val_accuracy: 0.6675\n",
            "Epoch 2/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6835 - val_loss: 0.6109 - val_accuracy: 0.6583\n",
            "Epoch 3/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.5806 - accuracy: 0.6997 - val_loss: 0.6001 - val_accuracy: 0.6899\n",
            "Epoch 4/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.5619 - accuracy: 0.7119 - val_loss: 0.5915 - val_accuracy: 0.6813\n",
            "Epoch 5/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.5427 - accuracy: 0.7249 - val_loss: 0.6039 - val_accuracy: 0.6859\n",
            "Epoch 6/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7446 - val_loss: 0.5801 - val_accuracy: 0.7089\n",
            "Epoch 7/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.5107 - accuracy: 0.7510 - val_loss: 0.6033 - val_accuracy: 0.7030\n",
            "Epoch 8/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.4951 - accuracy: 0.7569 - val_loss: 0.6072 - val_accuracy: 0.6925\n",
            "Epoch 9/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.4868 - accuracy: 0.7656 - val_loss: 0.5754 - val_accuracy: 0.7155\n",
            "Epoch 10/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.4755 - accuracy: 0.7730 - val_loss: 0.5804 - val_accuracy: 0.7155\n",
            "Epoch 11/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.4640 - accuracy: 0.7825 - val_loss: 0.6173 - val_accuracy: 0.7004\n",
            "Epoch 12/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.4517 - accuracy: 0.7901 - val_loss: 0.6057 - val_accuracy: 0.7011\n",
            "Epoch 13/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.4420 - accuracy: 0.7943 - val_loss: 0.5952 - val_accuracy: 0.7254\n",
            "Epoch 14/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.4292 - accuracy: 0.8004 - val_loss: 0.5977 - val_accuracy: 0.7221\n",
            "Epoch 15/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.4142 - accuracy: 0.8054 - val_loss: 0.6161 - val_accuracy: 0.7122\n",
            "Epoch 16/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.4110 - accuracy: 0.8108 - val_loss: 0.6406 - val_accuracy: 0.7057\n",
            "Epoch 17/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.4037 - accuracy: 0.8157 - val_loss: 0.6451 - val_accuracy: 0.7247\n",
            "Epoch 18/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.3958 - accuracy: 0.8244 - val_loss: 0.6196 - val_accuracy: 0.7227\n",
            "Epoch 19/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.3796 - accuracy: 0.8298 - val_loss: 0.6326 - val_accuracy: 0.7181\n",
            "Epoch 20/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.3768 - accuracy: 0.8308 - val_loss: 0.6359 - val_accuracy: 0.7208\n",
            "Epoch 21/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.3703 - accuracy: 0.8346 - val_loss: 0.6717 - val_accuracy: 0.7057\n",
            "Epoch 22/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.3573 - accuracy: 0.8453 - val_loss: 0.6720 - val_accuracy: 0.7247\n",
            "Epoch 23/25\n",
            "191/191 [==============================] - 1s 3ms/step - loss: 0.3538 - accuracy: 0.8489 - val_loss: 0.6544 - val_accuracy: 0.7339\n",
            "Epoch 24/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.3425 - accuracy: 0.8492 - val_loss: 0.6571 - val_accuracy: 0.7280\n",
            "Epoch 25/25\n",
            "191/191 [==============================] - 1s 4ms/step - loss: 0.3413 - accuracy: 0.8505 - val_loss: 0.6947 - val_accuracy: 0.7254\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.7241\n",
            "This is for batch size with a value of 32\n",
            "Valid accuracy: 0.7240835428237915\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "96/96 [==============================] - 1s 5ms/step - loss: 0.6610 - accuracy: 0.6132 - val_loss: 0.6390 - val_accuracy: 0.6432\n",
            "Epoch 2/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.6150 - accuracy: 0.6649 - val_loss: 0.6631 - val_accuracy: 0.6176\n",
            "Epoch 3/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.5906 - accuracy: 0.6925 - val_loss: 0.5982 - val_accuracy: 0.6827\n",
            "Epoch 4/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.5724 - accuracy: 0.7061 - val_loss: 0.6442 - val_accuracy: 0.6524\n",
            "Epoch 5/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.5622 - accuracy: 0.7109 - val_loss: 0.5933 - val_accuracy: 0.6853\n",
            "Epoch 6/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.7254 - val_loss: 0.6063 - val_accuracy: 0.6827\n",
            "Epoch 7/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7370 - val_loss: 0.6871 - val_accuracy: 0.6261\n",
            "Epoch 8/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 0.7454 - val_loss: 0.5730 - val_accuracy: 0.7043\n",
            "Epoch 9/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7525 - val_loss: 0.5732 - val_accuracy: 0.7116\n",
            "Epoch 10/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4995 - accuracy: 0.7544 - val_loss: 0.5742 - val_accuracy: 0.7050\n",
            "Epoch 11/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7679 - val_loss: 0.5774 - val_accuracy: 0.7076\n",
            "Epoch 12/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.7725 - val_loss: 0.5700 - val_accuracy: 0.7168\n",
            "Epoch 13/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.7809 - val_loss: 0.6211 - val_accuracy: 0.7076\n",
            "Epoch 14/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4591 - accuracy: 0.7771 - val_loss: 0.5813 - val_accuracy: 0.7148\n",
            "Epoch 15/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.7898 - val_loss: 0.5735 - val_accuracy: 0.7247\n",
            "Epoch 16/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7950 - val_loss: 0.5926 - val_accuracy: 0.7175\n",
            "Epoch 17/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8037 - val_loss: 0.5996 - val_accuracy: 0.7129\n",
            "Epoch 18/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8019 - val_loss: 0.6013 - val_accuracy: 0.7208\n",
            "Epoch 19/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.8098 - val_loss: 0.6054 - val_accuracy: 0.7280\n",
            "Epoch 20/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.4113 - accuracy: 0.8129 - val_loss: 0.6328 - val_accuracy: 0.7076\n",
            "Epoch 21/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8216 - val_loss: 0.6312 - val_accuracy: 0.7057\n",
            "Epoch 22/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8249 - val_loss: 0.6210 - val_accuracy: 0.7076\n",
            "Epoch 23/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8279 - val_loss: 0.6340 - val_accuracy: 0.7050\n",
            "Epoch 24/25\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8338 - val_loss: 0.7288 - val_accuracy: 0.6721\n",
            "Epoch 25/25\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8390 - val_loss: 0.6484 - val_accuracy: 0.7037\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.7127\n",
            "This is for batch size with a value of 64\n",
            "Valid accuracy: 0.7126527428627014\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 6ms/step - loss: 0.6685 - accuracy: 0.6048 - val_loss: 0.6427 - val_accuracy: 0.6307\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6159 - accuracy: 0.6641 - val_loss: 0.6200 - val_accuracy: 0.6544\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.6813 - val_loss: 0.6276 - val_accuracy: 0.6544\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5864 - accuracy: 0.6915 - val_loss: 0.5948 - val_accuracy: 0.6905\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5754 - accuracy: 0.7024 - val_loss: 0.5959 - val_accuracy: 0.6866\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5630 - accuracy: 0.7173 - val_loss: 0.5831 - val_accuracy: 0.7109\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5524 - accuracy: 0.7226 - val_loss: 0.5830 - val_accuracy: 0.6997\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5426 - accuracy: 0.7337 - val_loss: 0.5812 - val_accuracy: 0.7004\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5301 - accuracy: 0.7387 - val_loss: 0.5780 - val_accuracy: 0.7083\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5197 - accuracy: 0.7452 - val_loss: 0.5685 - val_accuracy: 0.7240\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5110 - accuracy: 0.7526 - val_loss: 0.5592 - val_accuracy: 0.7260\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5012 - accuracy: 0.7589 - val_loss: 0.5604 - val_accuracy: 0.7168\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4935 - accuracy: 0.7656 - val_loss: 0.6015 - val_accuracy: 0.6859\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7669 - val_loss: 0.5637 - val_accuracy: 0.7280\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4802 - accuracy: 0.7745 - val_loss: 0.5701 - val_accuracy: 0.7214\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4695 - accuracy: 0.7796 - val_loss: 0.5643 - val_accuracy: 0.7234\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4645 - accuracy: 0.7801 - val_loss: 0.6017 - val_accuracy: 0.6997\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.7848 - val_loss: 0.5853 - val_accuracy: 0.7109\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4464 - accuracy: 0.7884 - val_loss: 0.5736 - val_accuracy: 0.7188\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.7948 - val_loss: 0.5712 - val_accuracy: 0.7227\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.7999 - val_loss: 0.6135 - val_accuracy: 0.6951\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4241 - accuracy: 0.8088 - val_loss: 0.5864 - val_accuracy: 0.7188\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8073 - val_loss: 0.5895 - val_accuracy: 0.7194\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4140 - accuracy: 0.8091 - val_loss: 0.6382 - val_accuracy: 0.6873\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4063 - accuracy: 0.8196 - val_loss: 0.5929 - val_accuracy: 0.7175\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7308\n",
            "This is for batch size with a value of 128\n",
            "Valid accuracy: 0.7307844161987305\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "24/24 [==============================] - 1s 10ms/step - loss: 0.6784 - accuracy: 0.5838 - val_loss: 0.6301 - val_accuracy: 0.6557\n",
            "Epoch 2/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.6485 - val_loss: 0.6185 - val_accuracy: 0.6597\n",
            "Epoch 3/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.6127 - accuracy: 0.6639 - val_loss: 0.6387 - val_accuracy: 0.6281\n",
            "Epoch 4/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.6002 - accuracy: 0.6771 - val_loss: 0.6140 - val_accuracy: 0.6669\n",
            "Epoch 5/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5900 - accuracy: 0.6884 - val_loss: 0.5961 - val_accuracy: 0.6846\n",
            "Epoch 6/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5793 - accuracy: 0.7020 - val_loss: 0.6047 - val_accuracy: 0.6787\n",
            "Epoch 7/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5683 - accuracy: 0.7093 - val_loss: 0.5869 - val_accuracy: 0.6905\n",
            "Epoch 8/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5564 - accuracy: 0.7201 - val_loss: 0.5838 - val_accuracy: 0.6951\n",
            "Epoch 9/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5518 - accuracy: 0.7211 - val_loss: 0.5975 - val_accuracy: 0.6761\n",
            "Epoch 10/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5412 - accuracy: 0.7309 - val_loss: 0.5780 - val_accuracy: 0.7070\n",
            "Epoch 11/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5308 - accuracy: 0.7337 - val_loss: 0.5814 - val_accuracy: 0.6984\n",
            "Epoch 12/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5213 - accuracy: 0.7429 - val_loss: 0.5897 - val_accuracy: 0.6938\n",
            "Epoch 13/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5219 - accuracy: 0.7449 - val_loss: 0.5708 - val_accuracy: 0.7129\n",
            "Epoch 14/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5100 - accuracy: 0.7507 - val_loss: 0.5687 - val_accuracy: 0.7102\n",
            "Epoch 15/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5031 - accuracy: 0.7645 - val_loss: 0.5672 - val_accuracy: 0.7181\n",
            "Epoch 16/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4943 - accuracy: 0.7643 - val_loss: 0.5628 - val_accuracy: 0.7194\n",
            "Epoch 17/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.7677 - val_loss: 0.5737 - val_accuracy: 0.7221\n",
            "Epoch 18/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4822 - accuracy: 0.7761 - val_loss: 0.5629 - val_accuracy: 0.7227\n",
            "Epoch 19/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4764 - accuracy: 0.7728 - val_loss: 0.5765 - val_accuracy: 0.7162\n",
            "Epoch 20/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.7781 - val_loss: 0.5602 - val_accuracy: 0.7306\n",
            "Epoch 21/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4690 - accuracy: 0.7812 - val_loss: 0.5708 - val_accuracy: 0.7221\n",
            "Epoch 22/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4679 - accuracy: 0.7743 - val_loss: 0.5709 - val_accuracy: 0.7234\n",
            "Epoch 23/25\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4492 - accuracy: 0.7881 - val_loss: 0.5652 - val_accuracy: 0.7365\n",
            "Epoch 24/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4521 - accuracy: 0.7875 - val_loss: 0.5654 - val_accuracy: 0.7280\n",
            "Epoch 25/25\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.7957 - val_loss: 0.5926 - val_accuracy: 0.7057\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7190\n",
            "This is for batch size with a value of 256\n",
            "Valid accuracy: 0.718959391117096\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "12/12 [==============================] - 1s 18ms/step - loss: 0.7112 - accuracy: 0.5698 - val_loss: 0.6587 - val_accuracy: 0.6137\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6605 - accuracy: 0.6069 - val_loss: 0.6380 - val_accuracy: 0.6393\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6359 - accuracy: 0.6482 - val_loss: 0.6518 - val_accuracy: 0.6137\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6231 - accuracy: 0.6559 - val_loss: 0.6162 - val_accuracy: 0.6564\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6107 - accuracy: 0.6679 - val_loss: 0.6094 - val_accuracy: 0.6656\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.6815 - val_loss: 0.6039 - val_accuracy: 0.6807\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.6858 - val_loss: 0.5999 - val_accuracy: 0.6774\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5845 - accuracy: 0.6969 - val_loss: 0.6058 - val_accuracy: 0.6721\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5876 - accuracy: 0.6876 - val_loss: 0.5938 - val_accuracy: 0.6833\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5695 - accuracy: 0.7125 - val_loss: 0.5905 - val_accuracy: 0.6879\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5674 - accuracy: 0.7135 - val_loss: 0.5962 - val_accuracy: 0.6846\n",
            "Epoch 12/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5714 - accuracy: 0.7081 - val_loss: 0.5870 - val_accuracy: 0.6859\n",
            "Epoch 13/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5559 - accuracy: 0.7208 - val_loss: 0.5863 - val_accuracy: 0.6905\n",
            "Epoch 14/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5589 - accuracy: 0.7119 - val_loss: 0.5837 - val_accuracy: 0.6978\n",
            "Epoch 15/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5415 - accuracy: 0.7316 - val_loss: 0.5811 - val_accuracy: 0.6958\n",
            "Epoch 16/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5473 - accuracy: 0.7278 - val_loss: 0.5799 - val_accuracy: 0.7004\n",
            "Epoch 17/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5404 - accuracy: 0.7291 - val_loss: 0.5892 - val_accuracy: 0.6892\n",
            "Epoch 18/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5254 - accuracy: 0.7398 - val_loss: 0.5833 - val_accuracy: 0.6958\n",
            "Epoch 19/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5326 - accuracy: 0.7341 - val_loss: 0.5737 - val_accuracy: 0.6991\n",
            "Epoch 20/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5250 - accuracy: 0.7424 - val_loss: 0.5734 - val_accuracy: 0.7076\n",
            "Epoch 21/25\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5141 - accuracy: 0.7495 - val_loss: 0.5928 - val_accuracy: 0.6912\n",
            "Epoch 22/25\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5134 - accuracy: 0.7528 - val_loss: 0.5755 - val_accuracy: 0.7162\n",
            "Epoch 23/25\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5117 - accuracy: 0.7530 - val_loss: 0.5675 - val_accuracy: 0.7194\n",
            "Epoch 24/25\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5081 - accuracy: 0.7551 - val_loss: 0.5813 - val_accuracy: 0.6978\n",
            "Epoch 25/25\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.7569 - val_loss: 0.5672 - val_accuracy: 0.7208\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7091\n",
            "This is for batch size with a value of 512\n",
            "Valid accuracy: 0.7091052532196045\n"
          ]
        }
      ],
      "source": [
        "batch_size = [32, 64, 128, 256, 512]\n",
        "\n",
        "valid_acc_batch_size = {}\n",
        "for bp in batch_size:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "                name='dense_layer1', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hiddens,\n",
        "    name='dense_layer_2', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    model.compile(optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=bp, epochs=epoch,\n",
        "    verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_bs_valid_loss, NN_bs_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for batch size with a value of %i\" % bp)\n",
        "    print('Valid accuracy:', NN_bs_valid_acc)\n",
        "\n",
        "    valid_acc_batch_size[bp] = NN_bs_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NGBCA_8wH2H",
        "outputId": "602b1868-a1b8-4f33-f338-7577e29129c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{32: 0.7240835428237915, 64: 0.7126527428627014, 128: 0.7307844161987305, 256: 0.718959391117096, 512: 0.7091052532196045}\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_batch_size)\n",
        "bp = max(valid_acc_batch_size, key=valid_acc_batch_size.get)\n",
        "print(bp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7DlfhALe92Q"
      },
      "source": [
        "#### 4.2.6 - Learning Rate\n",
        "learning_rate = [0.001, 0.005, 0.01, 0.1, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOnjjjFCe92Q",
        "outputId": "1f0bb9a9-c315-4274-9d73-1c27d664c71e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6680 - accuracy: 0.6079 - val_loss: 0.6216 - val_accuracy: 0.6603\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6226 - accuracy: 0.6649 - val_loss: 0.6113 - val_accuracy: 0.6702\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6022 - accuracy: 0.6804 - val_loss: 0.6056 - val_accuracy: 0.6767\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5846 - accuracy: 0.6942 - val_loss: 0.5974 - val_accuracy: 0.6899\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5755 - accuracy: 0.7093 - val_loss: 0.5886 - val_accuracy: 0.7024\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5616 - accuracy: 0.7137 - val_loss: 0.5874 - val_accuracy: 0.7096\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5547 - accuracy: 0.7211 - val_loss: 0.5853 - val_accuracy: 0.7083\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5470 - accuracy: 0.7272 - val_loss: 0.5768 - val_accuracy: 0.7116\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5318 - accuracy: 0.7378 - val_loss: 0.5833 - val_accuracy: 0.7004\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.7515 - val_loss: 0.6119 - val_accuracy: 0.6715\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5165 - accuracy: 0.7500 - val_loss: 0.5635 - val_accuracy: 0.7286\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4986 - accuracy: 0.7569 - val_loss: 0.5768 - val_accuracy: 0.7017\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.7663 - val_loss: 0.6071 - val_accuracy: 0.6879\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4866 - accuracy: 0.7697 - val_loss: 0.5768 - val_accuracy: 0.7162\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7737 - val_loss: 0.5768 - val_accuracy: 0.7194\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4700 - accuracy: 0.7764 - val_loss: 0.5711 - val_accuracy: 0.7267\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.7817 - val_loss: 0.5707 - val_accuracy: 0.7240\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4563 - accuracy: 0.7842 - val_loss: 0.5887 - val_accuracy: 0.7076\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.7919 - val_loss: 0.5727 - val_accuracy: 0.7326\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4432 - accuracy: 0.7917 - val_loss: 0.6157 - val_accuracy: 0.6971\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4350 - accuracy: 0.7947 - val_loss: 0.5723 - val_accuracy: 0.7201\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.8065 - val_loss: 0.5976 - val_accuracy: 0.7247\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4221 - accuracy: 0.8049 - val_loss: 0.5969 - val_accuracy: 0.7102\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8081 - val_loss: 0.5935 - val_accuracy: 0.7240\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4069 - accuracy: 0.8119 - val_loss: 0.5877 - val_accuracy: 0.7385\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.7304\n",
            "This is for learning rate with a value of 0.00\n",
            "Valid accuracy: 0.7303902506828308\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.7157 - accuracy: 0.6230 - val_loss: 0.6216 - val_accuracy: 0.6656\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6123 - accuracy: 0.6716 - val_loss: 0.6042 - val_accuracy: 0.6767\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5834 - accuracy: 0.7043 - val_loss: 0.6099 - val_accuracy: 0.6813\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7139 - val_loss: 0.5760 - val_accuracy: 0.6978\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5409 - accuracy: 0.7365 - val_loss: 0.6107 - val_accuracy: 0.6813\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5204 - accuracy: 0.7444 - val_loss: 0.5938 - val_accuracy: 0.7024\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5105 - accuracy: 0.7528 - val_loss: 0.5724 - val_accuracy: 0.7267\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4867 - accuracy: 0.7674 - val_loss: 0.5965 - val_accuracy: 0.7116\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7746 - val_loss: 0.5990 - val_accuracy: 0.7057\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.7792 - val_loss: 0.5869 - val_accuracy: 0.7227\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4492 - accuracy: 0.7904 - val_loss: 0.5951 - val_accuracy: 0.7273\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7922 - val_loss: 0.6375 - val_accuracy: 0.7260\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4149 - accuracy: 0.8027 - val_loss: 0.5865 - val_accuracy: 0.7254\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4077 - accuracy: 0.8091 - val_loss: 0.6302 - val_accuracy: 0.7227\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3990 - accuracy: 0.8213 - val_loss: 0.6665 - val_accuracy: 0.7122\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8318 - val_loss: 0.7235 - val_accuracy: 0.7142\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.8262 - val_loss: 0.6847 - val_accuracy: 0.7234\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3536 - accuracy: 0.8430 - val_loss: 0.7887 - val_accuracy: 0.7168\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3453 - accuracy: 0.8354 - val_loss: 0.7139 - val_accuracy: 0.7129\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.8507 - val_loss: 0.8123 - val_accuracy: 0.7129\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8571 - val_loss: 0.7711 - val_accuracy: 0.7188\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3099 - accuracy: 0.8612 - val_loss: 0.7954 - val_accuracy: 0.7148\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3084 - accuracy: 0.8638 - val_loss: 0.8204 - val_accuracy: 0.7201\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8697 - val_loss: 0.9813 - val_accuracy: 0.6978\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2933 - accuracy: 0.8712 - val_loss: 0.8133 - val_accuracy: 0.7024\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.7349 - accuracy: 0.7217\n",
            "This is for learning rate with a value of 0.01\n",
            "Valid accuracy: 0.7217185497283936\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.8083 - accuracy: 0.6122 - val_loss: 0.6430 - val_accuracy: 0.6577\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6316 - accuracy: 0.6654 - val_loss: 0.6085 - val_accuracy: 0.6702\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6005 - accuracy: 0.6927 - val_loss: 0.6045 - val_accuracy: 0.6938\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5776 - accuracy: 0.7107 - val_loss: 0.6017 - val_accuracy: 0.7004\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.7191 - val_loss: 0.6644 - val_accuracy: 0.6735\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.7250 - val_loss: 0.6057 - val_accuracy: 0.6971\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.7395 - val_loss: 0.6084 - val_accuracy: 0.6978\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7398 - val_loss: 0.5771 - val_accuracy: 0.7109\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5141 - accuracy: 0.7502 - val_loss: 0.6021 - val_accuracy: 0.6984\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5042 - accuracy: 0.7564 - val_loss: 0.6152 - val_accuracy: 0.7116\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.7594 - val_loss: 0.5971 - val_accuracy: 0.7142\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4938 - accuracy: 0.7612 - val_loss: 0.6282 - val_accuracy: 0.7122\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4786 - accuracy: 0.7704 - val_loss: 0.6404 - val_accuracy: 0.7122\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.7774 - val_loss: 0.7278 - val_accuracy: 0.6984\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4549 - accuracy: 0.7792 - val_loss: 0.7028 - val_accuracy: 0.6991\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4534 - accuracy: 0.7832 - val_loss: 0.6624 - val_accuracy: 0.7043\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7868 - val_loss: 0.6437 - val_accuracy: 0.7083\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.7901 - val_loss: 0.7056 - val_accuracy: 0.7102\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4184 - accuracy: 0.7957 - val_loss: 0.7360 - val_accuracy: 0.7096\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4173 - accuracy: 0.8021 - val_loss: 0.9376 - val_accuracy: 0.6735\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4137 - accuracy: 0.8065 - val_loss: 0.7208 - val_accuracy: 0.6866\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4076 - accuracy: 0.8103 - val_loss: 0.7398 - val_accuracy: 0.7148\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.8131 - val_loss: 0.7463 - val_accuracy: 0.7162\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3957 - accuracy: 0.8146 - val_loss: 0.7093 - val_accuracy: 0.7122\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8185 - val_loss: 0.7919 - val_accuracy: 0.7089\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.7446 - accuracy: 0.7020\n",
            "This is for learning rate with a value of 0.01\n",
            "Valid accuracy: 0.7020102739334106\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 6ms/step - loss: 61.9270 - accuracy: 0.5325 - val_loss: 0.9057 - val_accuracy: 0.4277\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.5404 - val_loss: 0.6836 - val_accuracy: 0.5933\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.5473 - val_loss: 0.6782 - val_accuracy: 0.5940\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5683 - val_loss: 0.6783 - val_accuracy: 0.5940\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5664 - val_loss: 0.6883 - val_accuracy: 0.5940\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5624 - val_loss: 0.6810 - val_accuracy: 0.5940\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5690 - val_loss: 0.6755 - val_accuracy: 0.5940\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5634 - val_loss: 0.6786 - val_accuracy: 0.5940\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6853 - accuracy: 0.5654 - val_loss: 0.6774 - val_accuracy: 0.5940\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5680 - val_loss: 0.6754 - val_accuracy: 0.5940\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5664 - val_loss: 0.6889 - val_accuracy: 0.5940\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5690 - val_loss: 0.6790 - val_accuracy: 0.5940\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5664 - val_loss: 0.6754 - val_accuracy: 0.5940\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6859 - accuracy: 0.5601 - val_loss: 0.6838 - val_accuracy: 0.5940\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5660 - val_loss: 0.6754 - val_accuracy: 0.5940\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5667 - val_loss: 0.6900 - val_accuracy: 0.5940\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5552 - val_loss: 0.6875 - val_accuracy: 0.5940\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.5690 - val_loss: 0.6754 - val_accuracy: 0.5940\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5690 - val_loss: 0.7091 - val_accuracy: 0.4060\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5575 - val_loss: 0.6757 - val_accuracy: 0.5940\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.5545 - val_loss: 0.6808 - val_accuracy: 0.5940\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5562 - val_loss: 0.6782 - val_accuracy: 0.5940\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5690 - val_loss: 0.6763 - val_accuracy: 0.5940\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5618 - val_loss: 0.6922 - val_accuracy: 0.5940\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5670 - val_loss: 0.6883 - val_accuracy: 0.5940\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5581\n",
            "This is for learning rate with a value of 0.10\n",
            "Valid accuracy: 0.5581395626068115\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 5232.6309 - accuracy: 0.5317 - val_loss: 1.1561 - val_accuracy: 0.5815\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.9737 - accuracy: 0.5345 - val_loss: 0.7386 - val_accuracy: 0.4054\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7244 - accuracy: 0.5333 - val_loss: 0.6761 - val_accuracy: 0.5940\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 13.0334 - accuracy: 0.5212 - val_loss: 0.7489 - val_accuracy: 0.5940\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.9562 - accuracy: 0.5416 - val_loss: 0.6927 - val_accuracy: 0.5946\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7157 - accuracy: 0.5241 - val_loss: 0.6749 - val_accuracy: 0.5946\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7145 - accuracy: 0.5407 - val_loss: 0.8606 - val_accuracy: 0.4060\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7186 - accuracy: 0.5222 - val_loss: 0.6804 - val_accuracy: 0.5946\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7160 - accuracy: 0.5251 - val_loss: 0.7093 - val_accuracy: 0.4060\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7124 - accuracy: 0.5343 - val_loss: 0.7504 - val_accuracy: 0.4060\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7171 - accuracy: 0.5315 - val_loss: 0.6953 - val_accuracy: 0.4060\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7167 - accuracy: 0.5329 - val_loss: 0.6937 - val_accuracy: 0.5946\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7154 - accuracy: 0.5189 - val_loss: 0.6871 - val_accuracy: 0.5946\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7169 - accuracy: 0.5176 - val_loss: 0.7533 - val_accuracy: 0.5946\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.5227 - val_loss: 0.7469 - val_accuracy: 0.5946\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7201 - accuracy: 0.5220 - val_loss: 0.6940 - val_accuracy: 0.4060\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7139 - accuracy: 0.5240 - val_loss: 0.6832 - val_accuracy: 0.5946\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7128 - accuracy: 0.5294 - val_loss: 0.7002 - val_accuracy: 0.5946\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7223 - accuracy: 0.5194 - val_loss: 0.7253 - val_accuracy: 0.5946\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7099 - accuracy: 0.5465 - val_loss: 0.6877 - val_accuracy: 0.5946\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7121 - accuracy: 0.5151 - val_loss: 0.6889 - val_accuracy: 0.5946\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7212 - accuracy: 0.5200 - val_loss: 0.7374 - val_accuracy: 0.4060\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7172 - accuracy: 0.5199 - val_loss: 0.7299 - val_accuracy: 0.4060\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7135 - accuracy: 0.5251 - val_loss: 0.7185 - val_accuracy: 0.4060\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7166 - accuracy: 0.5163 - val_loss: 0.6749 - val_accuracy: 0.5946\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.7076 - accuracy: 0.5577\n",
            "This is for learning rate with a value of 0.50\n",
            "Valid accuracy: 0.5577453970909119\n"
          ]
        }
      ],
      "source": [
        "learning_rate = [0.001, 0.005, 0.01, 0.1, 0.5]\n",
        "\n",
        "valid_acc_learning_rate = {}\n",
        "for lrate in learning_rate:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hiddens, input_shape=(XTrain_mixture.shape[1],), \n",
        "                name='dense_layer1', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hiddens,\n",
        "    name='dense_layer_2', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=lrate)\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "    model.compile(optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=bp, epochs=epoch,\n",
        "    verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_learn_valid_loss, NN_learn_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for learning rate with a value of %.2f\" % lrate)\n",
        "    print('Valid accuracy:', NN_learn_valid_acc)\n",
        "\n",
        "    valid_acc_learning_rate[lrate] = NN_learn_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMoq52dRwH2I",
        "outputId": "4319d7f9-c5b5-4da7-a0f9-d63cc35dfe3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0.001: 0.7303902506828308, 0.005: 0.7217185497283936, 0.01: 0.7020102739334106, 0.1: 0.5581395626068115, 0.5: 0.5577453970909119}\n",
            "0.001\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_learning_rate)\n",
        "lrate = max(valid_acc_learning_rate, key=valid_acc_learning_rate.get)\n",
        "print(lrate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UFJ5UHNe92R"
      },
      "source": [
        "#### 4.2.7 - Number of hidden neurons\n",
        "Number of Hidden neurons: [128, 256, 512, 1024, 2048]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqCDJuxce92R",
        "outputId": "7e2ce1af-3351-4cf3-b445-1ce393734e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 128)               9216      \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,986\n",
            "Trainable params: 25,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6720 - accuracy: 0.5982 - val_loss: 0.6690 - val_accuracy: 0.6005\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6225 - accuracy: 0.6549 - val_loss: 0.6164 - val_accuracy: 0.6623\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6021 - accuracy: 0.6817 - val_loss: 0.6080 - val_accuracy: 0.6702\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5906 - accuracy: 0.6891 - val_loss: 0.6063 - val_accuracy: 0.6794\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5734 - accuracy: 0.7043 - val_loss: 0.6153 - val_accuracy: 0.6715\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5656 - accuracy: 0.7147 - val_loss: 0.6206 - val_accuracy: 0.6682\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5547 - accuracy: 0.7191 - val_loss: 0.5853 - val_accuracy: 0.6958\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5460 - accuracy: 0.7226 - val_loss: 0.5739 - val_accuracy: 0.7057\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5277 - accuracy: 0.7388 - val_loss: 0.5780 - val_accuracy: 0.7043\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5213 - accuracy: 0.7477 - val_loss: 0.5693 - val_accuracy: 0.7162\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5103 - accuracy: 0.7528 - val_loss: 0.5809 - val_accuracy: 0.7142\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5014 - accuracy: 0.7592 - val_loss: 0.5842 - val_accuracy: 0.7017\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.7559 - val_loss: 0.5707 - val_accuracy: 0.7175\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4884 - accuracy: 0.7695 - val_loss: 0.5816 - val_accuracy: 0.7122\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4802 - accuracy: 0.7648 - val_loss: 0.5838 - val_accuracy: 0.7057\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4712 - accuracy: 0.7718 - val_loss: 0.5797 - val_accuracy: 0.7221\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7820 - val_loss: 0.5688 - val_accuracy: 0.7306\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.7837 - val_loss: 0.5761 - val_accuracy: 0.7240\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4478 - accuracy: 0.7932 - val_loss: 0.5825 - val_accuracy: 0.7175\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4433 - accuracy: 0.7917 - val_loss: 0.6158 - val_accuracy: 0.6859\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4346 - accuracy: 0.7999 - val_loss: 0.5727 - val_accuracy: 0.7405\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8062 - val_loss: 0.5906 - val_accuracy: 0.7122\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.7952 - val_loss: 0.6034 - val_accuracy: 0.7109\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4112 - accuracy: 0.8144 - val_loss: 0.5887 - val_accuracy: 0.7208\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8146 - val_loss: 0.5892 - val_accuracy: 0.7332\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.7280\n",
            "This is for number of hidden layers with a value of 128.00\n",
            "Valid accuracy: 0.7280251979827881\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 256)               18432     \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 84,738\n",
            "Trainable params: 84,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.6770 - accuracy: 0.6046 - val_loss: 0.6166 - val_accuracy: 0.6721\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.6615 - val_loss: 0.6117 - val_accuracy: 0.6649\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6822 - val_loss: 0.6151 - val_accuracy: 0.6603\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5700 - accuracy: 0.7094 - val_loss: 0.6058 - val_accuracy: 0.6767\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7168 - val_loss: 0.6259 - val_accuracy: 0.6478\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5401 - accuracy: 0.7346 - val_loss: 0.6051 - val_accuracy: 0.6728\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.7441 - val_loss: 0.5967 - val_accuracy: 0.6833\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5075 - accuracy: 0.7562 - val_loss: 0.5705 - val_accuracy: 0.7109\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4921 - accuracy: 0.7645 - val_loss: 0.5849 - val_accuracy: 0.7011\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7746 - val_loss: 0.6164 - val_accuracy: 0.6748\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7855 - val_loss: 0.5712 - val_accuracy: 0.7201\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7873 - val_loss: 0.6556 - val_accuracy: 0.6787\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7985 - val_loss: 0.5807 - val_accuracy: 0.7122\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8045 - val_loss: 0.5929 - val_accuracy: 0.7221\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4158 - accuracy: 0.8072 - val_loss: 0.5992 - val_accuracy: 0.7148\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3946 - accuracy: 0.8195 - val_loss: 0.6235 - val_accuracy: 0.6991\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3850 - accuracy: 0.8259 - val_loss: 0.6013 - val_accuracy: 0.7234\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3765 - accuracy: 0.8302 - val_loss: 0.6302 - val_accuracy: 0.6965\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 0.8369 - val_loss: 0.6273 - val_accuracy: 0.7234\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3565 - accuracy: 0.8449 - val_loss: 0.6211 - val_accuracy: 0.7175\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3399 - accuracy: 0.8502 - val_loss: 0.6280 - val_accuracy: 0.7148\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3253 - accuracy: 0.8640 - val_loss: 0.6395 - val_accuracy: 0.7175\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3162 - accuracy: 0.8630 - val_loss: 0.6561 - val_accuracy: 0.7083\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3019 - accuracy: 0.8683 - val_loss: 0.6764 - val_accuracy: 0.7214\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3062 - accuracy: 0.8673 - val_loss: 0.6873 - val_accuracy: 0.7208\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.7237\n",
            "This is for number of hidden layers with a value of 256.00\n",
            "Valid accuracy: 0.7236893773078918\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.7364 - accuracy: 0.5997 - val_loss: 0.6190 - val_accuracy: 0.6689\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6185 - accuracy: 0.6592 - val_loss: 0.6152 - val_accuracy: 0.6570\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5850 - accuracy: 0.6956 - val_loss: 0.6247 - val_accuracy: 0.6551\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5574 - accuracy: 0.7173 - val_loss: 0.5783 - val_accuracy: 0.7083\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5303 - accuracy: 0.7380 - val_loss: 0.6034 - val_accuracy: 0.6702\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 0.7462 - val_loss: 0.5917 - val_accuracy: 0.6971\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.7666 - val_loss: 0.5710 - val_accuracy: 0.7221\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4659 - accuracy: 0.7804 - val_loss: 0.5685 - val_accuracy: 0.7286\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4433 - accuracy: 0.7899 - val_loss: 0.6650 - val_accuracy: 0.6623\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7958 - val_loss: 0.6628 - val_accuracy: 0.6616\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4040 - accuracy: 0.8159 - val_loss: 0.6198 - val_accuracy: 0.6932\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3902 - accuracy: 0.8221 - val_loss: 0.6010 - val_accuracy: 0.7063\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8357 - val_loss: 0.6238 - val_accuracy: 0.7293\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.8407 - val_loss: 0.6393 - val_accuracy: 0.7011\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3266 - accuracy: 0.8550 - val_loss: 0.6573 - val_accuracy: 0.7405\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.8607 - val_loss: 0.6476 - val_accuracy: 0.7181\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2920 - accuracy: 0.8771 - val_loss: 0.8542 - val_accuracy: 0.6537\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2770 - accuracy: 0.8824 - val_loss: 0.6883 - val_accuracy: 0.7254\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.8875 - val_loss: 0.7457 - val_accuracy: 0.7221\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.8876 - val_loss: 0.6944 - val_accuracy: 0.7234\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2271 - accuracy: 0.9034 - val_loss: 0.7428 - val_accuracy: 0.7260\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9067 - val_loss: 0.7361 - val_accuracy: 0.7162\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9146 - val_loss: 0.7618 - val_accuracy: 0.7162\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9197 - val_loss: 0.8041 - val_accuracy: 0.7096\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.9253 - val_loss: 0.9158 - val_accuracy: 0.7365\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.8556 - accuracy: 0.7304\n",
            "This is for number of hidden layers with a value of 512.00\n",
            "Valid accuracy: 0.7303902506828308\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 1024)              73728     \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,125,378\n",
            "Trainable params: 1,125,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 0.7754 - accuracy: 0.6151 - val_loss: 0.6674 - val_accuracy: 0.6104\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6167 - accuracy: 0.6726 - val_loss: 0.5973 - val_accuracy: 0.6820\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5761 - accuracy: 0.7073 - val_loss: 0.6876 - val_accuracy: 0.6143\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5431 - accuracy: 0.7311 - val_loss: 0.5908 - val_accuracy: 0.6919\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5134 - accuracy: 0.7516 - val_loss: 0.5826 - val_accuracy: 0.7050\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.7631 - val_loss: 0.5930 - val_accuracy: 0.7070\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4603 - accuracy: 0.7819 - val_loss: 0.5930 - val_accuracy: 0.7116\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7981 - val_loss: 0.6688 - val_accuracy: 0.7076\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4021 - accuracy: 0.8108 - val_loss: 0.6369 - val_accuracy: 0.6925\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3834 - accuracy: 0.8279 - val_loss: 0.6305 - val_accuracy: 0.7148\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3494 - accuracy: 0.8435 - val_loss: 0.6348 - val_accuracy: 0.7319\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3129 - accuracy: 0.8627 - val_loss: 0.6530 - val_accuracy: 0.7280\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2920 - accuracy: 0.8720 - val_loss: 0.7177 - val_accuracy: 0.7313\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2843 - accuracy: 0.8773 - val_loss: 0.7225 - val_accuracy: 0.7135\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2542 - accuracy: 0.8929 - val_loss: 0.9087 - val_accuracy: 0.6794\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2260 - accuracy: 0.9080 - val_loss: 0.8774 - val_accuracy: 0.7037\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2120 - accuracy: 0.9151 - val_loss: 1.1529 - val_accuracy: 0.6827\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9172 - val_loss: 0.8791 - val_accuracy: 0.7221\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1705 - accuracy: 0.9335 - val_loss: 1.0495 - val_accuracy: 0.7155\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9328 - val_loss: 1.0016 - val_accuracy: 0.6899\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9422 - val_loss: 0.9663 - val_accuracy: 0.6945\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1412 - accuracy: 0.9460 - val_loss: 1.0448 - val_accuracy: 0.7076\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9428 - val_loss: 0.9706 - val_accuracy: 0.7234\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9691 - val_loss: 1.2316 - val_accuracy: 0.7273\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9545 - val_loss: 1.1620 - val_accuracy: 0.7142\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.9841 - accuracy: 0.7288\n",
            "This is for number of hidden layers with a value of 1024.00\n",
            "Valid accuracy: 0.7288135886192322\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 2048)              147456    \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 2048)              4196352   \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,347,906\n",
            "Trainable params: 4,347,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 10ms/step - loss: 0.9336 - accuracy: 0.6038 - val_loss: 0.6111 - val_accuracy: 0.6708\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6020 - accuracy: 0.6794 - val_loss: 0.5956 - val_accuracy: 0.6833\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.5644 - accuracy: 0.7096 - val_loss: 0.6054 - val_accuracy: 0.7102\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.5294 - accuracy: 0.7447 - val_loss: 0.6309 - val_accuracy: 0.6715\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4894 - accuracy: 0.7658 - val_loss: 0.5776 - val_accuracy: 0.7208\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4603 - accuracy: 0.7787 - val_loss: 0.6164 - val_accuracy: 0.7208\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4334 - accuracy: 0.7991 - val_loss: 0.6164 - val_accuracy: 0.6997\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.4037 - accuracy: 0.8142 - val_loss: 0.6338 - val_accuracy: 0.7234\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3707 - accuracy: 0.8275 - val_loss: 0.6644 - val_accuracy: 0.7148\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 7ms/step - loss: 0.3403 - accuracy: 0.8492 - val_loss: 0.6500 - val_accuracy: 0.7254\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8637 - val_loss: 0.7274 - val_accuracy: 0.7424\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2905 - accuracy: 0.8709 - val_loss: 0.8274 - val_accuracy: 0.7175\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8809 - val_loss: 0.8169 - val_accuracy: 0.7221\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.2314 - accuracy: 0.9034 - val_loss: 0.8607 - val_accuracy: 0.7162\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9108 - val_loss: 0.9363 - val_accuracy: 0.7070\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1931 - accuracy: 0.9177 - val_loss: 1.0427 - val_accuracy: 0.6675\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9294 - val_loss: 1.0655 - val_accuracy: 0.7227\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9410 - val_loss: 1.2546 - val_accuracy: 0.7372\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1354 - accuracy: 0.9446 - val_loss: 1.4200 - val_accuracy: 0.7267\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1524 - accuracy: 0.9443 - val_loss: 1.2952 - val_accuracy: 0.7365\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1187 - accuracy: 0.9571 - val_loss: 1.4251 - val_accuracy: 0.6919\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9517 - val_loss: 1.0994 - val_accuracy: 0.7070\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.1069 - accuracy: 0.9630 - val_loss: 1.5090 - val_accuracy: 0.7260\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.0892 - accuracy: 0.9678 - val_loss: 1.5245 - val_accuracy: 0.7102\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.0879 - accuracy: 0.9670 - val_loss: 1.5420 - val_accuracy: 0.7175\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 1.3720 - accuracy: 0.7280\n",
            "This is for number of hidden layers with a value of 2048.00\n",
            "Valid accuracy: 0.7280251979827881\n"
          ]
        }
      ],
      "source": [
        "n_hiddens = [128, 256, 512, 1024, 2048]\n",
        "\n",
        "valid_acc_hidden_neurons = {}\n",
        "for n_hidden in n_hiddens:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hidden, input_shape=(XTrain_mixture.shape[1],), \n",
        "                name='dense_layer1', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hidden,\n",
        "    name='dense_layer_2', activation='relu'))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=lrate)\n",
        "    model.compile(optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=bp, epochs=epoch,\n",
        "    verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_hNeuron_valid_loss, NN_hNeuron_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for number of hidden layers with a value of %.2f\" % n_hidden)\n",
        "    print('Valid accuracy:', NN_hNeuron_valid_acc)\n",
        "\n",
        "    valid_acc_hidden_neurons[n_hidden] = NN_hNeuron_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHXVmUOdwH2J",
        "outputId": "aa6f7628-8684-4304-923f-e00495f54fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{128: 0.7280251979827881, 256: 0.7236893773078918, 512: 0.7303902506828308, 1024: 0.7288135886192322, 2048: 0.7280251979827881}\n",
            "512\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_hidden_neurons)\n",
        "n_hidden = max(valid_acc_hidden_neurons, key=valid_acc_hidden_neurons.get)\n",
        "print(n_hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRYiiDlGe92R"
      },
      "source": [
        "#### 4.2.8 - Regularization\n",
        "Regularization type: lasso and ridge "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xee3Tp9rL8Z",
        "outputId": "a1de2cad-a6e7-4f22-9d39-5ece18d9fde3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 51.8516 - accuracy: 0.5552 - val_loss: 13.1114 - val_accuracy: 0.5940\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 4.6730 - accuracy: 0.5690 - val_loss: 2.2337 - val_accuracy: 0.5940\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1807 - accuracy: 0.5690 - val_loss: 2.1554 - val_accuracy: 0.5940\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1596 - accuracy: 0.5690 - val_loss: 2.1523 - val_accuracy: 0.5940\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1582 - accuracy: 0.5690 - val_loss: 2.1500 - val_accuracy: 0.5940\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1576 - accuracy: 0.5690 - val_loss: 2.1496 - val_accuracy: 0.5940\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1574 - accuracy: 0.5690 - val_loss: 2.1487 - val_accuracy: 0.5940\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1573 - accuracy: 0.5690 - val_loss: 2.1483 - val_accuracy: 0.5940\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1483 - val_accuracy: 0.5940\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1484 - val_accuracy: 0.5940\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1481 - val_accuracy: 0.5940\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1481 - val_accuracy: 0.5940\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1482 - val_accuracy: 0.5940\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1482 - val_accuracy: 0.5940\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1480 - val_accuracy: 0.5940\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1478 - val_accuracy: 0.5940\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1481 - val_accuracy: 0.5940\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1479 - val_accuracy: 0.5940\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1478 - val_accuracy: 0.5940\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 2.1572 - accuracy: 0.5690 - val_loss: 2.1478 - val_accuracy: 0.5940\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 2.1578 - accuracy: 0.5581\n",
            "This is for regularization type l1\n",
            "Valid accuracy: 0.5581395626068115\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 3.5557 - accuracy: 0.5930 - val_loss: 1.7539 - val_accuracy: 0.5854\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 1.1927 - accuracy: 0.6416 - val_loss: 0.8821 - val_accuracy: 0.6590\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7983 - accuracy: 0.6562 - val_loss: 0.7363 - val_accuracy: 0.6610\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7119 - accuracy: 0.6629 - val_loss: 0.7371 - val_accuracy: 0.6104\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.6651 - val_loss: 0.6706 - val_accuracy: 0.6728\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.6661 - val_loss: 0.6879 - val_accuracy: 0.6367\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6746 - val_loss: 0.6639 - val_accuracy: 0.6583\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6552 - accuracy: 0.6726 - val_loss: 0.6770 - val_accuracy: 0.6472\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.6744 - val_loss: 0.6690 - val_accuracy: 0.6432\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.6776 - val_loss: 0.6770 - val_accuracy: 0.6524\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6477 - accuracy: 0.6759 - val_loss: 0.6553 - val_accuracy: 0.6590\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6800 - val_loss: 0.6510 - val_accuracy: 0.6682\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.6808 - val_loss: 0.6487 - val_accuracy: 0.6715\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6439 - accuracy: 0.6785 - val_loss: 0.6463 - val_accuracy: 0.6728\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.6846 - val_loss: 0.6518 - val_accuracy: 0.6623\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6406 - accuracy: 0.6863 - val_loss: 0.6521 - val_accuracy: 0.6675\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6407 - accuracy: 0.6831 - val_loss: 0.6476 - val_accuracy: 0.6767\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6856 - val_loss: 0.6517 - val_accuracy: 0.6649\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6376 - accuracy: 0.6932 - val_loss: 0.6512 - val_accuracy: 0.6715\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6373 - accuracy: 0.6904 - val_loss: 0.6554 - val_accuracy: 0.6728\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6382 - accuracy: 0.6927 - val_loss: 0.6674 - val_accuracy: 0.6603\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6359 - accuracy: 0.6946 - val_loss: 0.6462 - val_accuracy: 0.6833\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6353 - accuracy: 0.6935 - val_loss: 0.6427 - val_accuracy: 0.6813\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.6335 - accuracy: 0.6963 - val_loss: 0.6448 - val_accuracy: 0.6853\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6347 - accuracy: 0.6969 - val_loss: 0.6425 - val_accuracy: 0.6846\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.6855\n",
            "This is for regularization type l2\n",
            "Valid accuracy: 0.6854552626609802\n"
          ]
        }
      ],
      "source": [
        "regtype = ['l1', 'l2']\n",
        "    \n",
        "valid_acc_regularization = {}\n",
        "for l in regtype:\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_hidden, input_shape=(XTrain_mixture.shape[1],), \n",
        "                  name='dense_layer1', activation='relu', kernel_regularizer=l))\n",
        "\n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_hidden,\n",
        "    name='dense_layer_2', activation='relu', kernel_regularizer=l))\n",
        "  \n",
        "    model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "    model.add(keras.layers.Dense(n_classes,\n",
        "    name='dense_layer_3', activation='softmax',kernel_regularizer=l))\n",
        "\n",
        "    # Summary of the model.\n",
        "    model.summary()\n",
        "\n",
        "    # Compiling the model.\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=lrate)\n",
        "    model.compile(optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "    # Training the model.\n",
        "    model.fit(XTrain_mixture, yTrain, batch_size=bp, epochs=epoch,\n",
        "      verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "    # Evaluating the model.\n",
        "    NN_regular_valid_loss, NN_regular_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "    print(\"This is for regularization type %s\" % l)\n",
        "    print('Valid accuracy:', NN_regular_valid_acc)\n",
        "\n",
        "    valid_acc_regularization[l] = NN_regular_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVDezn26wH2K",
        "outputId": "cb7d6ee9-c52c-4300-c9d6-36c5abea2e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'l1': 0.5581395626068115, 'l2': 0.6854552626609802}\n",
            "l2\n"
          ]
        }
      ],
      "source": [
        "print(valid_acc_regularization)\n",
        "l = max(valid_acc_regularization, key=valid_acc_regularization.get)\n",
        "print(l) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIiZXUtF2rs5"
      },
      "source": [
        "#### 4.2.9 - Test Accuracy under Tuned Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omf22LCX7Vnv",
        "outputId": "cc42eb0e-815c-440a-c16a-257640235047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 6ms/step - loss: 0.7046 - accuracy: 0.6184 - val_loss: 0.6129 - val_accuracy: 0.6702\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.6112 - accuracy: 0.6738 - val_loss: 0.6022 - val_accuracy: 0.6761\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5762 - accuracy: 0.7027 - val_loss: 0.5937 - val_accuracy: 0.6912\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5546 - accuracy: 0.7249 - val_loss: 0.5927 - val_accuracy: 0.6997\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.5320 - accuracy: 0.7324 - val_loss: 0.6004 - val_accuracy: 0.6971\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.5066 - accuracy: 0.7516 - val_loss: 0.5724 - val_accuracy: 0.7043\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.7679 - val_loss: 0.5746 - val_accuracy: 0.7050\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4657 - accuracy: 0.7781 - val_loss: 0.6384 - val_accuracy: 0.6991\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.7875 - val_loss: 0.5820 - val_accuracy: 0.7286\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8058 - val_loss: 0.6793 - val_accuracy: 0.6886\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.4156 - accuracy: 0.8040 - val_loss: 0.5888 - val_accuracy: 0.7306\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.8193 - val_loss: 0.6073 - val_accuracy: 0.7267\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8333 - val_loss: 0.7157 - val_accuracy: 0.6656\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3510 - accuracy: 0.8435 - val_loss: 0.6153 - val_accuracy: 0.7319\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.3334 - accuracy: 0.8527 - val_loss: 0.6338 - val_accuracy: 0.7162\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8648 - val_loss: 0.6420 - val_accuracy: 0.7162\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2890 - accuracy: 0.8783 - val_loss: 0.7242 - val_accuracy: 0.7168\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2837 - accuracy: 0.8817 - val_loss: 0.6992 - val_accuracy: 0.7280\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8855 - val_loss: 0.7987 - val_accuracy: 0.6774\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2515 - accuracy: 0.8922 - val_loss: 0.7537 - val_accuracy: 0.7070\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.2405 - accuracy: 0.8996 - val_loss: 0.7207 - val_accuracy: 0.7313\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2243 - accuracy: 0.9082 - val_loss: 0.7307 - val_accuracy: 0.7240\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2151 - accuracy: 0.9139 - val_loss: 0.8387 - val_accuracy: 0.6984\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.2004 - accuracy: 0.9170 - val_loss: 0.7807 - val_accuracy: 0.7346\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9244 - val_loss: 0.9638 - val_accuracy: 0.7247\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.3835 - accuracy: 0.8506\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.9302 - accuracy: 0.7056\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.9404 - accuracy: 0.7036\n",
            "Test accuracy: 0.703587 for Neural network with hidden layers, when dropout = 0.100000,         optimizers: RMSProp, epoch:25, batch_size:128, learing rate 0.001000, n_hidden:512:\n"
          ]
        }
      ],
      "source": [
        "def neuronNetworks():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_hidden, input_shape=(XTrain_mixture.shape[1],), \n",
        "                    name='dense_layer1', activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "  model.add(keras.layers.Dense(n_hidden,\n",
        "      name='dense_layer_2', activation='relu'))\n",
        "    \n",
        "  model.add(keras.layers.Dropout(dp))\n",
        "\n",
        "  model.add(keras.layers.Dense(n_classes,\n",
        "      name='dense_layer_3', activation='softmax'))\n",
        "\n",
        "  # Summary of the model.\n",
        "  model.summary()\n",
        "\n",
        "  # Compiling the model.\n",
        "  opt = keras.optimizers.RMSprop(learning_rate=lrate)\n",
        "  model.compile(optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  # Training the model.\n",
        "  model.fit(XTrain_mixture, yTrain, batch_size=bp, epochs=epoch,\n",
        "        verbose=verbose, validation_split=validation_split)\n",
        "\n",
        "  # Evaluating the model.\n",
        "  NN_train_loss, NN_train_acc = model.evaluate(XTrain_mixture, yTrain)\n",
        "  NN_valid_loss, NN_valid_acc = model.evaluate(XValid_mixture, yValid)\n",
        "  NN_test_loss, NN_test_acc = model.evaluate(XTest_mixture, yTest)\n",
        "  print('Test accuracy: %f for Neural network with hidden layers, when dropout = %f, \\\n",
        "        optimizers: %s, epoch:%i, batch_size:%i, learing rate %f, n_hidden:%i:' \\\n",
        "        %(NN_test_acc, dp, optimizer, epoch, bp, lrate, n_hidden))\n",
        "  return NN_train_acc, NN_valid_acc, NN_test_acc\n",
        "NN_train_acc, NN_valid_acc, NN_test_acc = neuronNetworks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHi9h-3ZC0r-"
      },
      "source": [
        "#### Package Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrFCppjACSyu"
      },
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from keras.layers import Dense, Input, Dropout\n",
        "# from keras import Sequential\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def create_model(optimizer='adam',\n",
        "#                  dropout=0.1):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(20,activation='relu'))\n",
        "#     model.add(Dropout(dropout))\n",
        "#     model.add(Dense(1,activation='sigmoid'))\n",
        "#     model.compile(loss='categorical_crossentropy',optimizer=optimizer,\n",
        "#     metrics=['accuracy'])\n",
        "#     return model\n",
        "# estimators = []\n",
        "# estimators.append(('pf', PolynomialFeatures(interaction_only=True,\n",
        "#                                             include_bias=False)))\n",
        "# estimators.append(('ss', StandardScaler()))\n",
        "# estimators.append(('nn', KerasClassifier(build_fn=create_model, epochs=10, batch_size=5, verbose=0)))\n",
        "# nn_pipe = Pipeline(estimators)\n",
        "# nn_param_grid = {\n",
        "#     'nn__epochs': [50, 100, 150, 250],\n",
        "#     'nn__batch_size':[1000, 3000, 5000]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MryovylWCXPo"
      },
      "outputs": [],
      "source": [
        "# X_train = pd.read_csv(r\"D:\\Northeastern Semester 1\\Projects\\ml_project\\Data_Set\\x_train_encode.csv\")\n",
        "# X_test = pd.read_csv(r\"D:\\Northeastern Semester 1\\Projects\\ml_project\\Data_Set\\x_test_encode.csv\")\n",
        "# y_train = pd.read_csv(r\"D:\\Northeastern Semester 1\\Projects\\ml_project\\Data_Set\\y_train_encode.csv\")\n",
        "# y_test = pd.read_csv(r\"D:\\Northeastern Semester 1\\Projects\\ml_project\\Data_Set\\y_test_encode.csv\")\n",
        "# y_train.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# y_test.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# gs_nn = GridSearchCV(nn_pipe, nn_param_grid,\n",
        "# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â verbose=0, cv=3)\n",
        "# gs_nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yFRWnbE6Pd3"
      },
      "source": [
        "### 4.3 - Support Vector Machine (SVM)\n",
        "Support Vector Machine work as a powerful learning alogrithm that can capture the non-linearlity relationship. What is hard margin SVM and soft-margin SVM? What is the hyper-parameters here? How to tune them? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24stjgDkTOQD"
      },
      "source": [
        "#### 4.3.1 - Hard-Margin SVM - baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiJI-D4Xi1Xc",
        "outputId": "d0214139-15ac-48e2-b838-990b20ee8908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7610, 71) (7610,) (2537, 71) (2537,) (2537, 71) (2537,)\n"
          ]
        }
      ],
      "source": [
        "# Save dataframes as arrays for utilization by SVM models\n",
        "x_train_svm = np.asarray(XTrain_mixture)\n",
        "y_train_svm = np.asarray(np.squeeze(np.asarray(yTrain)))\n",
        "x_test_svm = np.asarray(XTest_mixture)\n",
        "y_test_svm = np.asarray(np.squeeze(np.asarray(yTest)))\n",
        "x_valid_svm = np.asarray(XValid_mixture)\n",
        "y_valid_svm = np.asarray(np.squeeze(np.asanyarray(yValid)))\n",
        "y_train_svm = np.where(y_train_svm==0, -1, 1)\n",
        "y_test_svm = np.where(y_test_svm==0, -1, 1)\n",
        "y_valid_svm = np.where(y_valid_svm == 0, -1, 1)\n",
        "\n",
        "print(x_train_svm.shape, y_train_svm.shape, x_valid_svm.shape, y_valid_svm.shape, x_test_svm.shape, y_test_svm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7610, 111) (7610,) (2537, 111) (2537,) (2537, 111) (2537,)\n"
          ]
        }
      ],
      "source": [
        "# one hot encoding \n",
        "x_train_ohe, x_valid_ohe, x_test_ohe = np.asarray(XTrain_only_one_hot), np.asarray(XValid_only_one_hot), np.asarray(XTest_only_one_hot)\n",
        "print(x_train_ohe.shape, y_train_svm.shape, x_valid_ohe.shape, y_valid_svm.shape, x_test_ohe.shape, y_test_svm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSBrFuEcZ65k"
      },
      "outputs": [],
      "source": [
        "class SVM:\n",
        "    def __init__(self, learningRate=0.0001, lambda_=0.001, nIterations=1000):\n",
        "        self.learningRate = learningRate\n",
        "        self.lambda_ = lambda_\n",
        "        self.nIterations = nIterations\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        nSamples, nFeatures = X.shape\n",
        "        self.w = np.zeros(nFeatures)\n",
        "        self.b = 0\n",
        "        \n",
        "        # Gradient Descent\n",
        "        # Cost function\n",
        "            # \n",
        "        # Gradient\n",
        "            # y_i*(wx_i + b) >= 1, then d_w = 2*lambda_*w \n",
        "            # otherwise, then d_w = 2*lambda_*w - y_i*x_i,  d_b = -y_i\n",
        "        for i in range(self.nIterations):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condtions = y[idx] * np.dot(self.w, x_i + self.b)\n",
        "                if condtions >= 1:\n",
        "                    # only for w\n",
        "                    self.w = self.w -self.learningRate * (2 * self.lambda_ * self.w)\n",
        "                else:\n",
        "                    # update both w and b\n",
        "                    self.w = self.w -self.learningRate * (2 * self.lambda_ * self.w - np.dot(y[idx], x_i))\n",
        "                    self.b = self.b - self.learningRate * (-y[idx])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        pred_ = np.dot(X, self.w) + self.b\n",
        "        return np.sign(pred_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrV0R7O_uFiW"
      },
      "outputs": [],
      "source": [
        "clf = SVM()\n",
        "clf.fit(x_train_svm, y_train_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Kx6KSJuv4M",
        "outputId": "5fdcffbd-230a-48f7-e772-d567138eeae8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.26371574, -0.19007065, -0.10467051, -0.10219876,  0.20836316,\n",
              "        0.13053066, -0.26772048, -0.08542704,  0.26876727, -0.21436565,\n",
              "        0.02471933,  0.0399149 , -0.09565966,  0.19284934,  0.19079616,\n",
              "       -0.15923757, -0.10467051, -0.15076284, -0.22009685, -0.45162533,\n",
              "        0.92531482,  0.58838061, -0.87299867,  0.39059186, -0.42161728,\n",
              "        0.05400747, -0.08503288, -0.03262966,  0.16072005, -0.06739663,\n",
              "        0.05925796,  0.0123879 , -0.1959947 , -0.05122219,  0.02019677,\n",
              "       -0.08608909,  0.06544953, -0.08294241, -0.11098343, -0.09088507,\n",
              "       -0.09020733, -0.26498976, -0.04076687, -0.11717594,  0.13896164,\n",
              "       -0.09102446, -0.27317684, -0.31669964, -0.02613449,  0.01325233,\n",
              "        0.06143763,  0.36750006,  0.59507087,  0.12560453,  0.27068322,\n",
              "       -0.18232518, -0.47361325, -0.00131406,  0.17641917,  0.20084356,\n",
              "        0.11599078, -0.00945511,  0.05110007,  0.22469946, -0.0043182 ,\n",
              "       -0.03289023,  0.14226997, -0.1538659 ,  0.12284048,  0.05876487,\n",
              "       -0.08979029])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQCiaSJSux_W",
        "outputId": "9b4e1855-2c53-4bc7-919b-84251bb524fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.07390000000000108"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZCzuV2k2I0E",
        "outputId": "72430df3-fe29-433d-e5c2-1616b9150da5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.,  1., -1., ...,  1.,  1.,  1.])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.predict(x_test_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwLrb8YkOBx0",
        "outputId": "e31ca9db-4d95-400b-905e-65edabf307ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6720536066219945"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hard_svm_train_base_accuracy = np.sum(clf.predict(x_train_svm) == y_train_svm) / np.size(y_train_svm)\n",
        "hard_svm_valid_base_accuracy = np.sum(clf.predict(x_valid_svm) == y_valid_svm) / np.size(y_valid_svm)\n",
        "hard_svm_test_base_accuracy = np.sum(clf.predict(x_test_svm) == y_test_svm) / np.size(y_test_svm)\n",
        "hard_svm_test_base_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf2 = SVM()\n",
        "clf2.fit(x_train_ohe, y_train_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6767836026803311"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hard_svm_train_ohe_accuracy = np.sum(clf2.predict(x_train_ohe) == y_train_svm) / np.size(y_train_svm)\n",
        "hard_svm_valid_ohe_accuracy = np.sum(clf2.predict(x_valid_ohe) == y_valid_svm) / np.size(y_valid_svm)\n",
        "hard_svm_test_ohe_accuracy = np.sum(clf2.predict(x_test_ohe) == y_test_svm) / np.size(y_test_svm)\n",
        "hard_svm_test_ohe_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.2 - Parameter Tuning\n",
        "- epochs  [25, 50, 100, 200, 300, 1000] \n",
        "- learning rate [0.00001, 0.0001, 0.001, 0.005, 0.01]\n",
        "- lambda [0.0001, 0.001, 0.01, 0.1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOUL13tArDNd"
      },
      "outputs": [],
      "source": [
        "'''Parameter Tuning for Hard Margin SVM''' \n",
        "\n",
        "epochs = [25, 50, 100, 200, 300, 1000] \n",
        "learning_rate = [0.00001, 0.0001, 0.001, 0.005, 0.01]\n",
        "lamda = [0.0001, 0.001, 0.01, 0.1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TEZZM3erElT",
        "outputId": "c8278292-f9e6-4446-bb77-02569088c80c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.0001, 0.001, 25, 0.6661],\n",
              " [0.0001, 0.001, 50, 0.6795],\n",
              " [0.0001, 0.001, 100, 0.6788],\n",
              " [0.0001, 0.001, 200, 0.6799],\n",
              " [0.0001, 0.001, 300, 0.6811],\n",
              " [0.0001, 0.001, 1000, 0.6862]]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Epochs'''\n",
        "ep_list = []\n",
        "\n",
        "for iter in epochs: \n",
        "  clf = SVM(learningRate= 0.0001, lambda_ = 0.001, nIterations = iter)\n",
        "  clf.fit(x_train_svm, y_train_svm)\n",
        "  y_hat_valid = round(accuracy_score(y_valid_svm, clf.predict(x_valid_svm)), 4)\n",
        "  pdata = [0.0001, 0.001, iter, y_hat_valid]\n",
        "  ep_list.append(pdata)\n",
        "\n",
        "# nIterations of 300 observes the highest test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "s7iUE693rHoZ",
        "outputId": "79b2cb37-c167-4629-fa78-270e02794023"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-161068a9-8987-40eb-9c64-d9da1ccd6a4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>lambda</th>\n",
              "      <th>maximum_iterations</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.6661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.6795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>0.6788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>200</td>\n",
              "      <td>0.6799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>300</td>\n",
              "      <td>0.6811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1000</td>\n",
              "      <td>0.6862</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-161068a9-8987-40eb-9c64-d9da1ccd6a4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-161068a9-8987-40eb-9c64-d9da1ccd6a4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-161068a9-8987-40eb-9c64-d9da1ccd6a4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   learning_rate  lambda  maximum_iterations  val_accuracy\n",
              "0         0.0001   0.001                  25        0.6661\n",
              "1         0.0001   0.001                  50        0.6795\n",
              "2         0.0001   0.001                 100        0.6788\n",
              "3         0.0001   0.001                 200        0.6799\n",
              "4         0.0001   0.001                 300        0.6811\n",
              "5         0.0001   0.001                1000        0.6862"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(ep_list, columns = ['learning_rate','lambda','maximum_iterations','val_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6HiE73VrKxq",
        "outputId": "83e8dcec-4e52-4b6d-e596-62a4b433aa3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1e-05, 0.001, 300, 0.6523],\n",
              " [0.0001, 0.001, 300, 0.6788],\n",
              " [0.001, 0.001, 300, 0.6776],\n",
              " [0.005, 0.001, 300, 0.4419],\n",
              " [0.01, 0.001, 300, 0.4419]]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Learning Rate'''\n",
        "lr_list = []\n",
        "\n",
        "for lr in learning_rate: \n",
        "  clf = SVM(learningRate= lr, lambda_ = 0.001, nIterations = 100)\n",
        "  clf.fit(x_train_svm, y_train_svm)\n",
        "  y_hat_valid = round(accuracy_score(y_valid_svm, clf.predict(x_valid_svm)), 4)\n",
        "  pdata = [lr, 0.001, 300, y_hat_valid]\n",
        "  lr_list.append(pdata)\n",
        "\n",
        "lr_list\n",
        "\n",
        "# learning rate of 0.0001 gives the best test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bI_3QCjrLh3",
        "outputId": "489ccc0a-69fb-4140-ecac-6edd33f5b0a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.0001, 0.0001, 300, 0.6799],\n",
              " [0.0001, 0.001, 300, 0.6788],\n",
              " [0.0001, 0.01, 300, 0.6717],\n",
              " [0.0001, 0.1, 300, 0.568],\n",
              " [0.0001, 1, 300, 0.5581]]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Lambda - Regularization Parameter'''\n",
        "lamda_list = []\n",
        "\n",
        "for l in lamda: \n",
        "  clf = SVM(learningRate= 0.0001, lambda_ = l, nIterations = 100)\n",
        "  clf.fit(x_train_svm, y_train_svm)\n",
        "  y_hat_valid = round(accuracy_score(y_valid_svm, clf.predict(x_valid_svm)), 4)\n",
        "  pdata = [0.0001, l, 300, y_hat_valid]\n",
        "  lamda_list.append(pdata)\n",
        "\n",
        "lamda_list\n",
        "\n",
        "# lambda of 0.001 is chosen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyrlYs10rO4W",
        "outputId": "8555216f-1523-4078-a1c9-40590f9b213b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6700827749310209\n"
          ]
        }
      ],
      "source": [
        "def HardSVM():\n",
        "  clf = SVM(learningRate=0.0001, lambda_=0.001, nIterations=300)\n",
        "  clf.fit(x_train_svm, y_train_svm)\n",
        "  hard_svm_train_accuracy = accuracy_score(y_train_svm, clf.predict(x_train_svm))\n",
        "  hard_svm_valid_accuracy = accuracy_score(y_valid_svm, clf.predict(x_valid_svm))\n",
        "  hard_svm_test_accuracy = accuracy_score(y_test_svm, clf.predict(x_test_svm))\n",
        "  print(hard_svm_test_accuracy)\n",
        "  return hard_svm_train_accuracy, hard_svm_valid_accuracy, hard_svm_test_accuracy\n",
        "hard_svm_train_accuracy, hard_svm_valid_accuracy, hard_svm_test_accuracy = HardSVM()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rJLZhYPTWNC"
      },
      "source": [
        "#### 4.3.3 - Soft-Margin SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4.3.2.1 - SMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TULnQYSvTZa8"
      },
      "outputs": [],
      "source": [
        "class SMO():\n",
        "    def __init__(self, max_iter=100, kernel_type='linear', C=1.0, epsilon=1e-4):\n",
        "        self.kernels = {\n",
        "            'linear' : self.kernel_linear,\n",
        "            'poly' : self.kernel_poly,\n",
        "            'rbf' : self.kernel_rbf\n",
        "        }\n",
        "        self.max_iter = max_iter\n",
        "        self.kernel_type = kernel_type\n",
        "        self.C = C\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n, d = X.shape[0], X.shape[1]\n",
        "        alpha = np.zeros((n))\n",
        "        kernel = self.kernels[self.kernel_type]\n",
        "        count = 0\n",
        "        while True:\n",
        "            count += 1\n",
        "            alpha_prev = np.copy(alpha)\n",
        "            for j in range(0, n):\n",
        "                i = self.get_rnd_int(0, n-1, j) # Get random int i~=j\n",
        "                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n",
        "                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
        "                if k_ij == 0:\n",
        "                    continue\n",
        "                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n",
        "                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n",
        "\n",
        "                # Compute model parameters\n",
        "                self.w = self.calc_w(alpha, y, X)\n",
        "                self.b = self.calc_b(X, y, self.w)\n",
        "\n",
        "                # Compute E_i, E_j\n",
        "                E_i = self.E(x_i, y_i, self.w, self.b)\n",
        "                E_j = self.E(x_j, y_j, self.w, self.b)\n",
        "\n",
        "                # Set new alpha values\n",
        "                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n",
        "                alpha[j] = max(alpha[j], L)\n",
        "                alpha[j] = min(alpha[j], H)\n",
        "\n",
        "                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n",
        "\n",
        "            # Check convergence\n",
        "            diff = np.linalg.norm(alpha - alpha_prev)\n",
        "            if diff < self.epsilon:\n",
        "                break\n",
        "            #print(count)\n",
        "            if count >= self.max_iter:\n",
        "                print(\"Iteration number exceeded the max of %d iterations\" % (self.max_iter))\n",
        "                return\n",
        "        self.b = self.calc_b(X, y, self.w)\n",
        "        if self.kernel_type == 'linear':\n",
        "            self.w = self.calc_w(alpha, y, X)\n",
        "        # Get support vectors\n",
        "        alpha_idx = np.where(alpha > 0)[0]\n",
        "        support_vectors = X[alpha_idx, :]\n",
        "\n",
        "        return support_vectors, count\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.h(X, self.w, self.b)\n",
        "\n",
        "    def calc_b(self, X, y, w):\n",
        "        b_tmp = y - np.dot(w.T, X.T)\n",
        "        return np.mean(b_tmp)\n",
        "\n",
        "    def calc_w(self, alpha, y, X):\n",
        "        return np.dot(X.T, np.multiply(alpha,y))\n",
        "\n",
        "    def h(self, X, w, b):\n",
        "        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n",
        "\n",
        "    def E(self, x_k, y_k, w, b):\n",
        "        return self.h(x_k, w, b) - y_k\n",
        "\n",
        "    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n",
        "        if(y_i != y_j):\n",
        "            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n",
        "        else:\n",
        "            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n",
        "\n",
        "    def get_rnd_int(self, a,b,z):\n",
        "        i = z\n",
        "        cnt=0\n",
        "        while i == z and cnt<1000:\n",
        "            i = random.randint(a,b)\n",
        "            cnt=cnt+1\n",
        "        return i\n",
        "\n",
        "    def kernel_linear(self, x, z):\n",
        "        return np.dot(x, z.T)\n",
        "\n",
        "    def kernel_poly(self, x, z):\n",
        "        return (np.dot(x, z.T) ** 2)\n",
        "\n",
        "    def kernel_rbf(self, x, z, sigma=1):\n",
        "        return np.exp(- (np.linalg.norm(x - z, 2)) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return np.dot(self.w.T, X.T) + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = SMO(C=5)\n",
        "# model.fit(x_train_svm, y_train_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4.3.2.2 - Minimize.minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoftSVM:\n",
        "    def __init__(self, C):\n",
        "        self.C = C\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.supportVectors = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        N = len(y)\n",
        "        \n",
        "        # Gram matrix of X, y\n",
        "        Xy = X * y[:, np.newaxis]\n",
        "        \n",
        "        # Same as np.dot(Xy, Xy.T)\n",
        "        # Same as Xy.dot(Xy.T)\n",
        "        GramXy = np.matmul(Xy, Xy.T)\n",
        "        \n",
        "        \n",
        "        # Define cost function\n",
        "        def Ld0(G, alpha):\n",
        "            obj_fn = alpha.sum() - 1/2 * alpha.dot(alpha.dot(G))\n",
        "            return obj_fn\n",
        "        \n",
        "        def particalDerivationLd0(G, alpha):\n",
        "            par_der = np.ones_like(alpha) - alpha.dot(G)\n",
        "            return par_der\n",
        "        \n",
        "        alpha = np.ones(N)\n",
        "        A = np.vstack((-np.eye(N), np.eye(N)))\n",
        "        b = np.concatenate((np.zeros(N), self.C * np.ones(N)))\n",
        "        \n",
        "        constraints = ({'type': 'eq', 'fun': lambda a: np.dot(a, y), 'jac': lambda a: y},\n",
        "                      {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A})\n",
        "        \n",
        "        optRes = optimize.minimize(fun = lambda a: -Ld0(GramXy, a),\n",
        "                                  x0 = alpha,\n",
        "                                  method = 'SLSQP', \n",
        "                                  jac = lambda a: -particalDerivationLd0(GramXy, a),\n",
        "                                  constraints = constraints)\n",
        "        \n",
        "        self.alpha = optRes.x\n",
        "        \n",
        "        # find w and b\n",
        "        self.w = np.sum((self.alpha[:, np.newaxis] * Xy), axis = 0)\n",
        "        epsilon = 1e-4\n",
        "        self.supportVectors = X[self.alpha > epsilon]\n",
        "        self.supportLabels = y[self.alpha > epsilon]\n",
        "        \n",
        "        b = []\n",
        "        for i in range(len(self.supportLabels)):\n",
        "            b_i = self.supportLabels[i] - np.matmul(self.supportVectors[i].T, self.w)\n",
        "            b.append(b_i)\n",
        "        self.b = sum(b) / len(b)    \n",
        "        \n",
        "    def predict(self, X):\n",
        "  #         pred = np.dot(X, w) + b\n",
        "  #         if pred > 0:\n",
        "  #             return 1\n",
        "  #         else:\n",
        "  #             return -1\n",
        "        return 2*(np.matmul(X, self.w) + b > 0) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = SoftSVM(C=5)\n",
        "# model.fit(x_train_svm, y_train_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.4 - Summary\n",
        "In our report, we try hard-margin SVM and soft-margin svm that is optimized by smo and optimize.minimize(). In hard-margin SVM, we turn the hyper-parameters such as epochs, learning rate and lambda. Compared to the hard-margin SVM, soft-margin one can tolerate more classification errors to capture non-linearlity reltionship underneath the data. We tried two ways, smo and minimize.minimize package to get the optimized cofficients. Both ways run slow and we just commet them for a normal run through for the whold notebook. In latter industial setting, we can play with them using GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adeCnm_1BgjB"
      },
      "source": [
        "### 4.4 - Naive Bayes\n",
        "Navie bayes are generally used for classification problem. We try to answer what is the difference between naive bayes and gaussian naive bayes? How to implement the gaussian naive bayes from scracth? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byEg_KoyBpff"
      },
      "source": [
        "#### 4.4.1 - Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "XtvrSy8aBtZT"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "  def __init__self(self, X, y) -> None:\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def splitData(self):\n",
        "    self.X_train, self.X_valid, self.X_test, = XTrain_mixture.apply(pd.to_numeric), XValid_mixture.apply(pd.to_numeric), XTest_mixture.apply(pd.to_numeric)\n",
        "    self.y_train, self.y_valid, self.y_test = yTrain, yValid, yTest\n",
        "    \n",
        "  # def min_max_scaling(col):\n",
        "  #   return (col - col.min())/(col.max() - col.min())\n",
        "    \n",
        "  # def normalize(self, X):\n",
        "  #   for i in range(X.shape[1]):\n",
        "  #     if len(X.iloc[:,i].unique())>2:\n",
        "  #       X.iloc[:,i] = self.min_max_scaling(X.iloc[:,i])\n",
        "\n",
        "        \n",
        "  def fitDistribution(self, data):\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    dist = norm(mean,std)\n",
        "    return dist\n",
        "  \n",
        "  def probability(self, x, y, prior):\n",
        "    prob = prior\n",
        "    for col in self.X_train.columns:\n",
        "      prob *= self.distributions[y][col].pdf(x[col])\n",
        "    return prob\n",
        "\n",
        "  def fit(self):\n",
        "    self.splitData()\n",
        "\n",
        "    self.distributions = {0:{},1:{}}\n",
        "\n",
        "    X0_train = self.X_train.iloc[np.where(self.y_train == 0)[0]]\n",
        "    X1_train = self.X_train.iloc[np.where(self.y_train == 1)[0]]\n",
        "\n",
        "    self.prior0 = len(X0_train) / len(self.X_train)\n",
        "    self.prior1 = len(X1_train) / len(self.X_train)\n",
        "\n",
        "    for col in self.X_train.columns:\n",
        "        self.distributions[0][col] = self.fitDistribution(X0_train[col])\n",
        "        self.distributions[1][col] = self.fitDistribution(X1_train[col])\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    y_hat_train = []\n",
        "    for idx in self.X_train.index:\n",
        "          # posterior is updated based on probabilites of test values\n",
        "      py_0 = self.probability(self.X_train.loc[idx, :], 0, self.prior0) \n",
        "      py_1 = self.probability(self.X_train.loc[idx, :], 1, self.prior1)\n",
        "      y_hat_train.append(np.argmax([py_0,py_1]))\n",
        "\n",
        "    return accuracy_score(self.y_train, y_hat_train)\n",
        "          \n",
        "\n",
        "  def validation(self):\n",
        "\n",
        "    y_hat_valid = []\n",
        "    for idx in self.X_valid.index:\n",
        "          # posterior is updated based on probabilites of test values\n",
        "      py_0 = self.probability(self.X_valid.loc[idx, :], 0, self.prior0) \n",
        "      py_1 = self.probability(self.X_valid.loc[idx, :], 1, self.prior1)\n",
        "      y_hat_valid.append(np.argmax([py_0,py_1]))\n",
        "\n",
        "    return accuracy_score(self.y_valid, y_hat_valid)\n",
        "\n",
        "  def test(self):\n",
        "\n",
        "      y_hat_test = []\n",
        "      for idx in self.X_test.index:\n",
        "            # posterior is updated based on probabilites of test values\n",
        "        py_0 = self.probability(self.X_test.loc[idx, :], 0, self.prior0) \n",
        "        py_1 = self.probability(self.X_test.loc[idx, :], 1, self.prior1)\n",
        "        y_hat_test.append(np.argmax([py_0,py_1]))\n",
        "\n",
        "      return accuracy_score(self.y_test, y_hat_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "8nwR2_RjB4oW",
        "outputId": "4ce25afa-fda9-4cab-e4cf-7dc53abadff8"
      },
      "outputs": [],
      "source": [
        "gnb = GaussianNaiveBayes()\n",
        "gnb.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GNB():\n",
        "  gnb = GaussianNaiveBayes()\n",
        "  gnb.fit()\n",
        "  gnb_train_accuracy = gnb.train()\n",
        "  gnb_valid_accuracy = gnb.validation()\n",
        "  gnb_test_accuracy = gnb.test()\n",
        "  \n",
        "  return gnb_train_accuracy, gnb_valid_accuracy, gnb_test_accuracy\n",
        "gnb_train_accuracy, gnb_valid_accuracy, gnb_test_accuracy = GNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.4.2 - Summary\n",
        "In this section, we implemeted the gaussian naive bayes and get an test accuracy on our data, which further deepen our understanding of gaussian naive bayes especially in high-dimensional features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKJOXKLv1IDj"
      },
      "source": [
        "### 4.5 - Model Performance Comparison\n",
        "We will measure the model performance by three metrics, like test accuracy, time consumption and space usage on previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_6g2YWL1gag"
      },
      "source": [
        "#### 4.5.1 - Test Accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4mm4BXKlTSp1",
        "outputId": "718fcd2e-ed5d-4d0c-bcf8-1e1c8ce3001b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'log_base_train_acc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_&_Models_(2).ipynb Cell 143'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=0'>1</a>\u001b[0m acc_list \u001b[39m=\u001b[39m [[\u001b[39m'\u001b[39m\u001b[39mLogistic Regression - Baseline\u001b[39m\u001b[39m'\u001b[39m, log_base_train_acc, log_base_valid_acc, log_base_test_acc],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=1'>2</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mLogistic Regression - Tuned\u001b[39m\u001b[39m'\u001b[39m, log_train_acc, log_valid_acc, log_test_acc],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=2'>3</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mNeural Network - Baseline\u001b[39m\u001b[39m'\u001b[39m, NN_base_train_acc, NN_base_valid_acc, NN_base_test_acc], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=3'>4</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mNeural Network - Tuned\u001b[39m\u001b[39m'\u001b[39m, NN_train_acc, NN_valid_acc, NN_test_acc],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=4'>5</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mHard SVM - Baseline\u001b[39m\u001b[39m'\u001b[39m, hard_svm_train_base_accuracy, hard_svm_valid_base_accuracy, hard_svm_test_base_accuracy], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=5'>6</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mHard SVM - Tuned\u001b[39m\u001b[39m'\u001b[39m, hard_svm_train_accuracy, hard_svm_valid_accuracy, hard_svm_test_accuracy],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=6'>7</a>\u001b[0m              [\u001b[39m'\u001b[39m\u001b[39mGaussian Naive Bayes\u001b[39m\u001b[39m'\u001b[39m, gnb_train_accuracy, gnb_valid_accuracy, gnb_test_accuracy]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=7'>8</a>\u001b[0m acc_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(acc_list, columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mModels\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTrain Accuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mValidation Accuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTest Accuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anushkahegde/Desktop/NEU/IE_7374_Machine_Learning/ml_project/Code/EDA_%26_Models_%282%29.ipynb#ch0000136?line=8'>9</a>\u001b[0m display(acc_df)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_base_train_acc' is not defined"
          ]
        }
      ],
      "source": [
        "acc_list = [['Logistic Regression - Baseline', log_base_train_acc, log_base_valid_acc, log_base_test_acc],\n",
        "             ['Logistic Regression - Tuned', log_train_acc, log_valid_acc, log_test_acc],\n",
        "             ['Neural Network - Baseline', NN_base_train_acc, NN_base_valid_acc, NN_base_test_acc], \n",
        "             ['Neural Network - Tuned', NN_train_acc, NN_valid_acc, NN_test_acc],\n",
        "             ['Hard SVM - Baseline', hard_svm_train_base_accuracy, hard_svm_valid_base_accuracy, hard_svm_test_base_accuracy], \n",
        "             ['Hard SVM - Tuned', hard_svm_train_accuracy, hard_svm_valid_accuracy, hard_svm_test_accuracy],\n",
        "             ['Gaussian Naive Bayes', gnb_train_accuracy, gnb_valid_accuracy, gnb_test_accuracy]]\n",
        "acc_df = pd.DataFrame(acc_list, columns = ['Models', 'Train Accuracy', 'Validation Accuracy', 'Test Accuracy'])\n",
        "display(acc_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XrpnX7F1gmr"
      },
      "source": [
        "#### 4.5.2 - Running time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oJwRDoB7Dt27",
        "outputId": "711284c3-8661-4903-db3d-4e8ff30137bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for best parameters, learning rate 0.500000, tolerance 0.001000, max iteration 1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.607227332457293"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5904611746156878"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5865195112337407"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_66 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 7ms/step - loss: 122099.9531 - accuracy: 0.5138 - val_loss: 9.7145 - val_accuracy: 0.5986\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 118.9921 - accuracy: 0.5179 - val_loss: 2.4896 - val_accuracy: 0.4120\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 75.1177 - accuracy: 0.5232 - val_loss: 0.7800 - val_accuracy: 0.5940\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 2.6807 - accuracy: 0.5381 - val_loss: 0.9110 - val_accuracy: 0.5940\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 128.5056 - accuracy: 0.5370 - val_loss: 0.7200 - val_accuracy: 0.4087\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 141.0248 - accuracy: 0.5389 - val_loss: 0.6866 - val_accuracy: 0.5940\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.5153 - val_loss: 0.7615 - val_accuracy: 0.4074\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 10.9534 - accuracy: 0.5215 - val_loss: 0.7602 - val_accuracy: 0.4067\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 33.2604 - accuracy: 0.5332 - val_loss: 0.7041 - val_accuracy: 0.5933\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 5.2379 - accuracy: 0.5317 - val_loss: 0.7501 - val_accuracy: 0.4054\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7199 - accuracy: 0.5113 - val_loss: 0.7028 - val_accuracy: 0.5933\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7195 - accuracy: 0.5192 - val_loss: 0.7314 - val_accuracy: 0.5933\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7158 - accuracy: 0.5338 - val_loss: 0.7551 - val_accuracy: 0.4054\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7056 - accuracy: 0.5358 - val_loss: 0.7027 - val_accuracy: 0.5933\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.5085 - val_loss: 0.7027 - val_accuracy: 0.5933\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7077 - accuracy: 0.5340 - val_loss: 0.7129 - val_accuracy: 0.5933\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7234 - accuracy: 0.5279 - val_loss: 0.7417 - val_accuracy: 0.4054\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7074 - accuracy: 0.5315 - val_loss: 0.7270 - val_accuracy: 0.5933\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7158 - accuracy: 0.5322 - val_loss: 0.7364 - val_accuracy: 0.4054\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7108 - accuracy: 0.5304 - val_loss: 0.7295 - val_accuracy: 0.5933\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7151 - accuracy: 0.5210 - val_loss: 0.7700 - val_accuracy: 0.4054\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7122 - accuracy: 0.5322 - val_loss: 0.8216 - val_accuracy: 0.5933\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7190 - accuracy: 0.5182 - val_loss: 0.7208 - val_accuracy: 0.4054\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7144 - accuracy: 0.5279 - val_loss: 0.7142 - val_accuracy: 0.5933\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7136 - accuracy: 0.5223 - val_loss: 0.7902 - val_accuracy: 0.4054\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.7585 - accuracy: 0.4265\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.8103 - accuracy: 0.4426\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.7804 - accuracy: 0.4387\n",
            "Test accuracy: 0.438707 for Neural network with hidden layers, when dropout = 0.100000,         optimizers: RMSProp, epoch:25, batch_size:128, learing rate 0.500000, n_hidden:512:\n",
            "0.6700827749310209\n"
          ]
        }
      ],
      "source": [
        "\n",
        "st = time.process_time()\n",
        "logRegression()\n",
        "end = time.process_time()\n",
        "time_log = end - st\n",
        "\n",
        "st = time.process_time()\n",
        "neuronNetworks()\n",
        "end = time.process_time()\n",
        "time_neuron = end-st\n",
        "\n",
        "st = time.process_time()\n",
        "HardSVM()\n",
        "end = time.process_time()\n",
        "time_hard = end - st\n",
        "\n",
        "st = time.process_time()\n",
        "GNB()\n",
        "end = time.process_time()\n",
        "time_gnb = end - st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpb5ula1FnaR",
        "outputId": "34d922f5-2cc9-4824-d452-97a3a36e5bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Model  Processing Time(s)\n",
            "0  Logistic Regression           18.790732\n",
            "1       Neural Network            8.543720\n",
            "2             Hard SVM           59.411566\n"
          ]
        }
      ],
      "source": [
        "time_data = [['Logistic Regression', time_log], ['Neural Network', time_neuron], ['Hard SVM', time_hard], ['Gaussian Naive Bayes', time_gnb]]\n",
        "time_df = pd.DataFrame(time_data, columns=['Model', 'Processing Time(s)'])\n",
        "display(time_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikj6yb3F14cM"
      },
      "source": [
        "#### 4.5.3 - Space Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1vqF3j_HHcj-",
        "outputId": "9ac414ca-36a8-48f9-99b0-bc2ca60ce14c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for best parameters, learning rate 0.500000, tolerance 0.001000, max iteration 1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.607227332457293"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5904611746156878"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.5865195112337407"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_layer1 (Dense)        (None, 512)               36864     \n",
            "                                                                 \n",
            " dropout_64 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_layer_3 (Dense)       (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,546\n",
            "Trainable params: 300,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "48/48 [==============================] - 1s 8ms/step - loss: 107602.2188 - accuracy: 0.5168 - val_loss: 7.5877 - val_accuracy: 0.6058\n",
            "Epoch 2/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 17.6600 - accuracy: 0.5284 - val_loss: 0.9888 - val_accuracy: 0.5848\n",
            "Epoch 3/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 1.4791 - accuracy: 0.5361 - val_loss: 0.7903 - val_accuracy: 0.5940\n",
            "Epoch 4/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 20.8385 - accuracy: 0.5268 - val_loss: 0.6874 - val_accuracy: 0.5940\n",
            "Epoch 5/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7356 - accuracy: 0.5243 - val_loss: 0.7154 - val_accuracy: 0.5940\n",
            "Epoch 6/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7135 - accuracy: 0.5266 - val_loss: 0.6756 - val_accuracy: 0.5940\n",
            "Epoch 7/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7184 - accuracy: 0.5250 - val_loss: 0.7199 - val_accuracy: 0.5940\n",
            "Epoch 8/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7145 - accuracy: 0.5248 - val_loss: 0.7157 - val_accuracy: 0.5940\n",
            "Epoch 9/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7192 - accuracy: 0.5260 - val_loss: 0.7728 - val_accuracy: 0.4060\n",
            "Epoch 10/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7191 - accuracy: 0.5250 - val_loss: 0.7157 - val_accuracy: 0.4060\n",
            "Epoch 11/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7142 - accuracy: 0.5256 - val_loss: 0.8672 - val_accuracy: 0.4060\n",
            "Epoch 12/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7160 - accuracy: 0.5342 - val_loss: 0.6799 - val_accuracy: 0.5940\n",
            "Epoch 13/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7141 - accuracy: 0.5276 - val_loss: 0.8024 - val_accuracy: 0.4060\n",
            "Epoch 14/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7117 - accuracy: 0.5223 - val_loss: 0.6914 - val_accuracy: 0.5940\n",
            "Epoch 15/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7183 - accuracy: 0.5250 - val_loss: 0.6818 - val_accuracy: 0.5940\n",
            "Epoch 16/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7096 - accuracy: 0.5371 - val_loss: 0.7143 - val_accuracy: 0.4060\n",
            "Epoch 17/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7176 - accuracy: 0.5118 - val_loss: 0.7184 - val_accuracy: 0.5940\n",
            "Epoch 18/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7102 - accuracy: 0.5299 - val_loss: 0.7232 - val_accuracy: 0.4060\n",
            "Epoch 19/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7216 - accuracy: 0.5261 - val_loss: 0.6881 - val_accuracy: 0.5940\n",
            "Epoch 20/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7143 - accuracy: 0.5297 - val_loss: 0.6941 - val_accuracy: 0.4060\n",
            "Epoch 21/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7100 - accuracy: 0.5384 - val_loss: 0.7853 - val_accuracy: 0.4060\n",
            "Epoch 22/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7140 - accuracy: 0.5227 - val_loss: 0.6771 - val_accuracy: 0.5940\n",
            "Epoch 23/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7210 - accuracy: 0.5184 - val_loss: 0.7029 - val_accuracy: 0.4060\n",
            "Epoch 24/25\n",
            "48/48 [==============================] - 0s 4ms/step - loss: 0.7137 - accuracy: 0.5302 - val_loss: 0.6873 - val_accuracy: 0.5940\n",
            "Epoch 25/25\n",
            "48/48 [==============================] - 0s 5ms/step - loss: 0.7117 - accuracy: 0.5355 - val_loss: 0.7091 - val_accuracy: 0.5940\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.7274 - accuracy: 0.5740\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7423 - accuracy: 0.5581\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.7383 - accuracy: 0.5621\n",
            "Test accuracy: 0.562081 for Neural network with hidden layers, when dropout = 0.100000,         optimizers: RMSProp, epoch:25, batch_size:128, learing rate 0.500000, n_hidden:512:\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-6efcbcad0474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mHardSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_space_hard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_traced_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-ffdbb5fa20a3>\u001b[0m in \u001b[0;36mHardSVM\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mHardSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mhard_svm_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mhard_svm_valid_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-d3b0d1a81963>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;31m# update both w and b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearningRate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearningRate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# starting the monitoring\n",
        "tracemalloc.start()\n",
        "logRegression()\n",
        "_, max_space_log = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "max_space_log\n",
        " \n",
        "\n",
        "tracemalloc.start()\n",
        "neuronNetworks()\n",
        "_, max_space_neuron = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "\n",
        "\n",
        "tracemalloc.start()\n",
        "HardSVM()\n",
        "_, max_space_hard = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "\n",
        "tracemalloc.start()\n",
        "GNB()\n",
        "_, max_space_gnb = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2S_yt5v9qaW"
      },
      "outputs": [],
      "source": [
        "space_data = [['Logistic Regression', max_space_log], ['Neural Network', max_space_neuron], ['Hard SVM', max_space_hard], ['Gaussian Naive Bayes', max_space_gnb]]\n",
        "space_df = pd.DataFrame(space_data, columns=['Model', 'Peak Memory(B)'])\n",
        "print(space_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx7RQaFY1sBv"
      },
      "outputs": [],
      "source": [
        "# !pip install memory_profiler\n",
        "# import time\n",
        "# from memory_profiler import memory_usage\n",
        "# from time import sleep\n",
        "\n",
        "# func_list = [logRegression(), neuronNetworks(), HardSVM()]\n",
        "# time_dic = {}\n",
        "# memory_dic = {}\n",
        "# cnt = 0 \n",
        "# for fun in func_list:\n",
        "  \n",
        "#   # time comsumption\n",
        "\n",
        "#   time_dic[cnt] = end - st\n",
        "  \n",
        "#   # memory usage\n",
        "#   # m = memory_usage(fun)\n",
        "#   # memory_dic[str(fun)] = m\n",
        "\n",
        "#   cnt += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.4 - Summary"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EDA & Models (2).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "1143f320a06349356e2325743eda34a6c9064036a78c665fba996f767620c39f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('projectVenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "af8259ad5c1c9c7a69bd6ea085234cf8fd3a6a37a71ca551828b314c4d89b0ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
