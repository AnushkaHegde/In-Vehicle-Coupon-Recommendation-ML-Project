{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning'''\n",
    "df = pd.read_csv(r\"D:\\Northeastern Semester 1\\Projects\\ml_project\\Data_Set\\Vehicle_Coupon.csv\")\n",
    "df.drop(['car', 'direction_same', 'toCoupon_GEQ5min'], axis=1, inplace=True)\n",
    "df['temperature'] = df['temperature'].astype(str)\n",
    "df = df.apply(lambda x: x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create dummies and split data'''\n",
    "df_ohe = pd.get_dummies(df)\n",
    "X, y = df_ohe.drop(['Y'], axis=1), df_ohe['Y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self,X,y, learningRate = 0.00001, tolerance = 0.00005, maxIteration = 5000):\n",
    "        self.X = X\n",
    "        self.y =y\n",
    "        self.tolerance = tolerance\n",
    "        self.maxIteration = maxIteration\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "    def splitData(self):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "        return X_train, y_train, X_valid, y_valid, X_test, y_test \n",
    "\n",
    "    def add_x0(self, X):\n",
    "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        sig = 1/(1+np.exp(-z))\n",
    "        return sig\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        pred_ =np.log(np.ones(X.shape[0])+np.exp(X.dot(self.w))) - X.dot(self.w).dot(y)\n",
    "        cost = pred_.sum( )\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self,X,y):\n",
    "        sigmoid = self.sigmoid(X.dot(self.w))\n",
    "        grad = (sigmoid -y ).dot(X)\n",
    "        return grad\n",
    "    \n",
    "    def gradientDescent(self, X, y):\n",
    "        errors = []\n",
    "        last = float('inf')\n",
    "        \n",
    "        for i in range(self.maxIteration):\n",
    "            self.w = self.w - self.learningRate*self.gradient(X,y)\n",
    "            curr = self.costFunction(X,y)\n",
    "            \n",
    "            diff = last - curr\n",
    "            last - curr\n",
    "            \n",
    "            errors.append(curr)\n",
    "            \n",
    "            if diff < self.tolerance:\n",
    "                print(\"The model stopped Learning\")\n",
    "                break\n",
    "        # self.plot_cost(errors)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        pred = self.sigmoid(X.dot(self.w))\n",
    "        return np.around(pred)\n",
    "        \n",
    "    def evaluate(self, y, y_hat):\n",
    "        \n",
    "        y = (y == 1)\n",
    "        y_hat = (y_hat == 1)\n",
    "        \n",
    "        accuracy = (y == y_hat).sum() / y.size\n",
    "        precision = (y & y_hat).sum() / y_hat.sum()\n",
    "        recall = (y & y_hat).sum() / y.sum()\n",
    "\n",
    "        print(\"Accuracy is\", accuracy)\n",
    "        print('Recall is', recall)\n",
    "        print('precision is ', precision)\n",
    "        \n",
    "        return recall, precision, accuracy\n",
    "    \n",
    "    def fit(self):\n",
    "\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
    "        self.w = np.ones(X_train.shape[1], dtype = np.float64)*0\n",
    "        self.gradientDescent(X_train, y_train)\n",
    "        \n",
    "        #print(self.w)\n",
    "        \n",
    "        y_hat_train = self.predict(X_train)\n",
    "        recall, precision, accuracy = self.evaluate(y_train,y_hat_train)\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
    "        y_hat_valid = self.predict(X_valid)\n",
    "        recall, precision, accuracy  = self.evaluate(y_valid, y_hat_valid)\n",
    "\n",
    "    def test(self):\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = self.splitData()\n",
    "        y_hat_test = self.predict(X_test)\n",
    "        recall, precision, accuracy  = self.evaluate(y_test, y_hat_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(X, y, learningRate = 0.0001, tolerance = 0.00005, maxIteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6910729372007886\n",
      "Recall is 0.7747836835599505\n",
      "precision is  0.7095313561240661\n"
     ]
    }
   ],
   "source": [
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6981981981981982\n",
      "Recall is 0.7896825396825397\n",
      "precision is  0.7107142857142857\n"
     ]
    }
   ],
   "source": [
    "lr.validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6918024172359433\n",
      "Recall is 0.7816411682892906\n",
      "precision is  0.7060301507537688\n"
     ]
    }
   ],
   "source": [
    "lr.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class LogisticRegressionn:\n",
    "\n",
    "\n",
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (dim,1) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w = np.zeros_like(X_train[0])\n",
    "    b = 0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    return 1/(1+np.exp(-z))\n",
    "  \n",
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    sum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        sum += (y_true[i] * np.log10(y_pred[i])) + ((1 - y_true[i]) * np.log10(1 - y_pred[i]))\n",
    "    loss = -1 * (1 / len(y_true)) * sum\n",
    "    return loss\n",
    "  \n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw = x * (y - sigmoid(np.dot(w,x) + b) - (alpha / N) * w)\n",
    "    return dw\n",
    "\n",
    "def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    db = y - sigmoid(np.dot(w,x) + b)\n",
    "    return db\n",
    "\n",
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    \n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    for i in range(epochs):\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "        for j in range(N):\n",
    "            dw = gradient_dw(X_train[j],y_train[j],w,b,alpha,N)\n",
    "            db = gradient_db(X_train[j],y_train[j],w,b)\n",
    "            w = w + (eta0 * dw)\n",
    "            b = b + (eta0 * db)\n",
    "        for val in range(N):\n",
    "            train_pred.append(sigmoid(np.dot(w, X_train[val]) + b))\n",
    "            \n",
    "        loss1 = logloss(y_train, train_pred)\n",
    "        train_loss.append(loss1)\n",
    "            \n",
    "        for val in range(len(X_test)):\n",
    "            test_pred.append(sigmoid(np.dot(w, X_test[val]) + b))\n",
    "            \n",
    "        loss2 = logloss(y_test, test_pred)\n",
    "        test_loss.append(loss2)\n",
    "        \n",
    "    return w,b,train_loss,test_loss     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,train_log_loss,test_log_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b8e6af7e1c800dc5745a28ff6ca8872762767d15c22a299591d50db6a7acdf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
